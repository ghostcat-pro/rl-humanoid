
=== Merged Config ===
 exp_name: rl-humanoid
seed: 42
resume_from: null
paths:
  log_root: null
vecnorm:
  enabled: true
  clip_obs: 10.0
  gamma: 0.99
  epsilon: 1.0e-08
env:
  name: HumanoidCircuit-v0
  make_kwargs:
    render_mode: null
    waypoints:
    - - 5.0
      - 0.0
    - - 5.0
      - 5.0
    - - 2.0
      - 5.0
    - - 2.0
      - 0.0
    waypoint_reach_threshold: 1.5
    stairs: []
    terrain_width: 15.0
    progress_reward_weight: 200.0
    waypoint_bonus: 150.0
    circuit_completion_bonus: 500.0
    height_reward_weight: 0.0
    forward_reward_weight: 1.0
    heading_reward_weight: 2.0
    balance_reward_weight: 0.5
    optimal_speed: 1.2
    speed_regulation_weight: 0.2
    ctrl_cost_weight: 0.1
    contact_cost_weight: 5.0e-07
    healthy_reward: 5.0
    terminate_when_unhealthy: true
    healthy_z_range:
    - 0.8
    - 3.0
  vec_env:
    n_envs: 8
    start_method: spawn
    monitor: true
algo:
  policy: MlpPolicy
  device: cuda
  policy_kwargs:
    net_arch:
    - 256
    - 256
  hyperparams:
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 16384
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.005
    vf_coef: 0.25
    max_grad_norm: 0.5
training:
  total_timesteps: 80000000
  log_interval: 10
  checkpoint_every_steps: 250000

Logging to /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/14-37-50
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Using cuda device

=== Starting Training ===
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.4        |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 7177        |
|    iterations           | 10          |
|    time_elapsed         | 45          |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.010297303 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0841     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0132     |
|    std                  | 1.01        |
|    value_loss           | 0.261       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 54.2         |
|    ep_rew_mean          | 317          |
| time/                   |              |
|    fps                  | 7284         |
|    iterations           | 20           |
|    time_elapsed         | 89           |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0111259725 |
|    clip_fraction        | 0.0805       |
|    clip_range           | 0.2          |
|    entropy_loss         | -24.1        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.118       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.013       |
|    std                  | 1            |
|    value_loss           | 0.114        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 65.5        |
|    ep_rew_mean          | 387         |
| time/                   |             |
|    fps                  | 7303        |
|    iterations           | 30          |
|    time_elapsed         | 134         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.009535918 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.127      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0128     |
|    std                  | 0.997       |
|    value_loss           | 0.0737      |
-----------------------------------------
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000000, episode_reward=757.33 +/- 176.54
Episode length: 90.60 +/- 11.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.6        |
|    mean_reward          | 757         |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.010435816 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0125     |
|    std                  | 0.995       |
|    value_loss           | 0.0701      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 76.7        |
|    ep_rew_mean          | 442         |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 40          |
|    time_elapsed         | 179         |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.009916108 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0131     |
|    std                  | 0.991       |
|    value_loss           | 0.0651      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 79.5        |
|    ep_rew_mean          | 450         |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 50          |
|    time_elapsed         | 224         |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.010745604 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0132     |
|    std                  | 0.989       |
|    value_loss           | 0.076       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 87.6      |
|    ep_rew_mean          | 536       |
| time/                   |           |
|    fps                  | 7294      |
|    iterations           | 60        |
|    time_elapsed         | 269       |
|    total_timesteps      | 1966080   |
| train/                  |           |
|    approx_kl            | 0.0111319 |
|    clip_fraction        | 0.0887    |
|    clip_range           | 0.2       |
|    entropy_loss         | -24       |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.126    |
|    n_updates            | 590       |
|    policy_gradient_loss | -0.0133   |
|    std                  | 0.994     |
|    value_loss           | 0.0819    |
---------------------------------------
Eval num_timesteps=2000000, episode_reward=752.04 +/- 150.41
Episode length: 92.40 +/- 13.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.4        |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.010979189 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.124      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0126     |
|    std                  | 0.993       |
|    value_loss           | 0.0842      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.9         |
|    ep_rew_mean          | 584          |
| time/                   |              |
|    fps                  | 7281         |
|    iterations           | 70           |
|    time_elapsed         | 315          |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0110485945 |
|    clip_fraction        | 0.0926       |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.9        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.125       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.0137      |
|    std                  | 0.992        |
|    value_loss           | 0.0857       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.1        |
|    ep_rew_mean          | 631         |
| time/                   |             |
|    fps                  | 7275        |
|    iterations           | 80          |
|    time_elapsed         | 360         |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.011202518 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.122      |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0137     |
|    std                  | 0.992       |
|    value_loss           | 0.0967      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 7271        |
|    iterations           | 90          |
|    time_elapsed         | 405         |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.011429442 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.124      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0133     |
|    std                  | 0.992       |
|    value_loss           | 0.0832      |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=759.09 +/- 300.63
Episode length: 87.80 +/- 27.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.8         |
|    mean_reward          | 759          |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0116781145 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.9        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.123       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.0136      |
|    std                  | 0.992        |
|    value_loss           | 0.0912       |
------------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 732         |
| time/                   |             |
|    fps                  | 7262        |
|    iterations           | 100         |
|    time_elapsed         | 451         |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.012336237 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.99        |
|    value_loss           | 0.0761      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 7256        |
|    iterations           | 110         |
|    time_elapsed         | 496         |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.012769887 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0143     |
|    std                  | 0.992       |
|    value_loss           | 0.0816      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 110          |
|    ep_rew_mean          | 810          |
| time/                   |              |
|    fps                  | 7252         |
|    iterations           | 120          |
|    time_elapsed         | 542          |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0125018675 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.127       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0142      |
|    std                  | 0.995        |
|    value_loss           | 0.0738       |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=1035.87 +/- 293.50
Episode length: 111.00 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 1.04e+03    |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.013221728 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 0.996       |
|    value_loss           | 0.0711      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 920         |
| time/                   |             |
|    fps                  | 7243        |
|    iterations           | 130         |
|    time_elapsed         | 588         |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.012887841 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0147     |
|    std                  | 0.998       |
|    value_loss           | 0.0721      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 954         |
| time/                   |             |
|    fps                  | 7240        |
|    iterations           | 140         |
|    time_elapsed         | 633         |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.013577942 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 0.998       |
|    value_loss           | 0.0665      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 150         |
|    time_elapsed         | 678         |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.014322734 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 1           |
|    value_loss           | 0.0733      |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=1214.06 +/- 372.58
Episode length: 133.80 +/- 27.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 1.21e+03    |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.013276277 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0155     |
|    std                  | 1           |
|    value_loss           | 0.0713      |
-----------------------------------------
New best mean reward!
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 126          |
|    ep_rew_mean          | 1.02e+03     |
| time/                   |              |
|    fps                  | 7233         |
|    iterations           | 160          |
|    time_elapsed         | 724          |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0141287185 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.132       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.0164      |
|    std                  | 1            |
|    value_loss           | 0.0712       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 144         |
|    ep_rew_mean          | 1.26e+03    |
| time/                   |             |
|    fps                  | 7232        |
|    iterations           | 170         |
|    time_elapsed         | 770         |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.013903452 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1           |
|    value_loss           | 0.0671      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 7231        |
|    iterations           | 180         |
|    time_elapsed         | 815         |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.014012106 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0173     |
|    std                  | 1           |
|    value_loss           | 0.0641      |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=2258.72 +/- 363.51
Episode length: 198.60 +/- 19.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 2.26e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.014255364 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.134      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1           |
|    value_loss           | 0.0696      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 7226        |
|    iterations           | 190         |
|    time_elapsed         | 861         |
|    total_timesteps      | 6225920     |
| train/                  |             |
|    approx_kl            | 0.015678775 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.01        |
|    value_loss           | 0.0715      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | 1.56e+03     |
| time/                   |              |
|    fps                  | 7223         |
|    iterations           | 200          |
|    time_elapsed         | 907          |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0151181575 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.134       |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.0175      |
|    std                  | 1            |
|    value_loss           | 0.0714       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 1.75e+03    |
| time/                   |             |
|    fps                  | 7228        |
|    iterations           | 210         |
|    time_elapsed         | 952         |
|    total_timesteps      | 6881280     |
| train/                  |             |
|    approx_kl            | 0.015222359 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.7       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 1           |
|    value_loss           | 0.0722      |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=2035.33 +/- 350.01
Episode length: 185.80 +/- 26.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 2.04e+03   |
| time/                   |            |
|    total_timesteps      | 7000000    |
| train/                  |            |
|    approx_kl            | 0.01533648 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.134     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0177    |
|    std                  | 1          |
|    value_loss           | 0.0679     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 1.81e+03    |
| time/                   |             |
|    fps                  | 7227        |
|    iterations           | 220         |
|    time_elapsed         | 997         |
|    total_timesteps      | 7208960     |
| train/                  |             |
|    approx_kl            | 0.015649045 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1           |
|    value_loss           | 0.0653      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 7233        |
|    iterations           | 230         |
|    time_elapsed         | 1041        |
|    total_timesteps      | 7536640     |
| train/                  |             |
|    approx_kl            | 0.015140012 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.01        |
|    value_loss           | 0.0565      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 7237        |
|    iterations           | 240         |
|    time_elapsed         | 1086        |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.015559951 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.02        |
|    value_loss           | 0.0564      |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2768.87 +/- 429.91
Episode length: 249.60 +/- 10.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.016414374 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.02        |
|    value_loss           | 0.0475      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 250         |
|    time_elapsed         | 1132        |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.015371178 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.143      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.018      |
|    std                  | 1.03        |
|    value_loss           | 0.0459      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 220         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 260         |
|    time_elapsed         | 1176        |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.016211044 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.143      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.03        |
|    value_loss           | 0.0425      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 228         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 7243        |
|    iterations           | 270         |
|    time_elapsed         | 1221        |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.016020544 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.147      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.04        |
|    value_loss           | 0.0348      |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2597.78 +/- 1233.31
Episode length: 248.20 +/- 81.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.015647864 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.2       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.04        |
|    value_loss           | 0.035       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 2.54e+03   |
| time/                   |            |
|    fps                  | 7243       |
|    iterations           | 280        |
|    time_elapsed         | 1266       |
|    total_timesteps      | 9175040    |
| train/                  |            |
|    approx_kl            | 0.01670146 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.146     |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 1.04       |
|    value_loss           | 0.0391     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 245         |
|    ep_rew_mean          | 2.7e+03     |
| time/                   |             |
|    fps                  | 7247        |
|    iterations           | 290         |
|    time_elapsed         | 1311        |
|    total_timesteps      | 9502720     |
| train/                  |             |
|    approx_kl            | 0.015250227 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.146      |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 1.05        |
|    value_loss           | 0.0354      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 269       |
|    ep_rew_mean          | 2.82e+03  |
| time/                   |           |
|    fps                  | 7250      |
|    iterations           | 300       |
|    time_elapsed         | 1355      |
|    total_timesteps      | 9830400   |
| train/                  |           |
|    approx_kl            | 0.0162506 |
|    clip_fraction        | 0.133     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.4     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.148    |
|    n_updates            | 2990      |
|    policy_gradient_loss | -0.0183   |
|    std                  | 1.05      |
|    value_loss           | 0.0309    |
---------------------------------------
Eval num_timesteps=10000000, episode_reward=3638.33 +/- 95.20
Episode length: 318.80 +/- 17.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 319         |
|    mean_reward          | 3.64e+03    |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.016459256 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.151      |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.06        |
|    value_loss           | 0.0265      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 256        |
|    ep_rew_mean          | 2.79e+03   |
| time/                   |            |
|    fps                  | 7249       |
|    iterations           | 310        |
|    time_elapsed         | 1401       |
|    total_timesteps      | 10158080   |
| train/                  |            |
|    approx_kl            | 0.01727093 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.151     |
|    n_updates            | 3090       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 1.06       |
|    value_loss           | 0.0262     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 260         |
|    ep_rew_mean          | 2.89e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 320         |
|    time_elapsed         | 1445        |
|    total_timesteps      | 10485760    |
| train/                  |             |
|    approx_kl            | 0.016089983 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.149      |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.07        |
|    value_loss           | 0.0278      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 270         |
|    ep_rew_mean          | 3.01e+03    |
| time/                   |             |
|    fps                  | 7254        |
|    iterations           | 330         |
|    time_elapsed         | 1490        |
|    total_timesteps      | 10813440    |
| train/                  |             |
|    approx_kl            | 0.015714442 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.07        |
|    value_loss           | 0.0221      |
-----------------------------------------
Eval num_timesteps=11000000, episode_reward=3653.84 +/- 359.51
Episode length: 309.80 +/- 33.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 310         |
|    mean_reward          | 3.65e+03    |
| time/                   |             |
|    total_timesteps      | 11000000    |
| train/                  |             |
|    approx_kl            | 0.016496548 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.08        |
|    value_loss           | 0.0252      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 276         |
|    ep_rew_mean          | 3.05e+03    |
| time/                   |             |
|    fps                  | 7253        |
|    iterations           | 340         |
|    time_elapsed         | 1536        |
|    total_timesteps      | 11141120    |
| train/                  |             |
|    approx_kl            | 0.016789854 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.08        |
|    value_loss           | 0.0216      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 290         |
|    ep_rew_mean          | 3.2e+03     |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 350         |
|    time_elapsed         | 1580        |
|    total_timesteps      | 11468800    |
| train/                  |             |
|    approx_kl            | 0.015796563 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.09        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 295         |
|    ep_rew_mean          | 3.32e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 360         |
|    time_elapsed         | 1625        |
|    total_timesteps      | 11796480    |
| train/                  |             |
|    approx_kl            | 0.016010638 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.1         |
|    value_loss           | 0.0185      |
-----------------------------------------
Eval num_timesteps=12000000, episode_reward=3778.39 +/- 170.00
Episode length: 309.40 +/- 24.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 3.78e+03    |
| time/                   |             |
|    total_timesteps      | 12000000    |
| train/                  |             |
|    approx_kl            | 0.015807036 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.1         |
|    value_loss           | 0.0183      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 292         |
|    ep_rew_mean          | 3.27e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 370         |
|    time_elapsed         | 1670        |
|    total_timesteps      | 12124160    |
| train/                  |             |
|    approx_kl            | 0.015767384 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.11        |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 298         |
|    ep_rew_mean          | 3.29e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 380         |
|    time_elapsed         | 1715        |
|    total_timesteps      | 12451840    |
| train/                  |             |
|    approx_kl            | 0.016612794 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.156      |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.12        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 301         |
|    ep_rew_mean          | 3.4e+03     |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 390         |
|    time_elapsed         | 1760        |
|    total_timesteps      | 12779520    |
| train/                  |             |
|    approx_kl            | 0.016783983 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.157      |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.13        |
|    value_loss           | 0.0191      |
-----------------------------------------
Eval num_timesteps=13000000, episode_reward=3754.17 +/- 275.60
Episode length: 314.20 +/- 26.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 314         |
|    mean_reward          | 3.75e+03    |
| time/                   |             |
|    total_timesteps      | 13000000    |
| train/                  |             |
|    approx_kl            | 0.015915174 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.155      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 1.13        |
|    value_loss           | 0.0188      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 303        |
|    ep_rew_mean          | 3.41e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 400        |
|    time_elapsed         | 1805       |
|    total_timesteps      | 13107200   |
| train/                  |            |
|    approx_kl            | 0.01602345 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.156     |
|    n_updates            | 3990       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 1.14       |
|    value_loss           | 0.0173     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 3.42e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 410         |
|    time_elapsed         | 1850        |
|    total_timesteps      | 13434880    |
| train/                  |             |
|    approx_kl            | 0.017406266 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.158      |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.15        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 323         |
|    ep_rew_mean          | 3.6e+03     |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 420         |
|    time_elapsed         | 1896        |
|    total_timesteps      | 13762560    |
| train/                  |             |
|    approx_kl            | 0.017087549 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.16        |
|    value_loss           | 0.0166      |
-----------------------------------------
Eval num_timesteps=14000000, episode_reward=3364.71 +/- 1458.91
Episode length: 279.60 +/- 111.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 3.36e+03    |
| time/                   |             |
|    total_timesteps      | 14000000    |
| train/                  |             |
|    approx_kl            | 0.016577803 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.17        |
|    value_loss           | 0.0213      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 320         |
|    ep_rew_mean          | 3.45e+03    |
| time/                   |             |
|    fps                  | 7246        |
|    iterations           | 430         |
|    time_elapsed         | 1944        |
|    total_timesteps      | 14090240    |
| train/                  |             |
|    approx_kl            | 0.016664261 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.162      |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.18        |
|    value_loss           | 0.0149      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 321         |
|    ep_rew_mean          | 3.58e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 440         |
|    time_elapsed         | 1991        |
|    total_timesteps      | 14417920    |
| train/                  |             |
|    approx_kl            | 0.017029481 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.19        |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 312         |
|    ep_rew_mean          | 3.61e+03    |
| time/                   |             |
|    fps                  | 7234        |
|    iterations           | 450         |
|    time_elapsed         | 2038        |
|    total_timesteps      | 14745600    |
| train/                  |             |
|    approx_kl            | 0.016093351 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.2         |
|    value_loss           | 0.0143      |
-----------------------------------------
Eval num_timesteps=15000000, episode_reward=2914.93 +/- 1298.84
Episode length: 232.00 +/- 91.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 2.91e+03   |
| time/                   |            |
|    total_timesteps      | 15000000   |
| train/                  |            |
|    approx_kl            | 0.01701479 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.164     |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.21       |
|    value_loss           | 0.0181     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 322         |
|    ep_rew_mean          | 3.74e+03    |
| time/                   |             |
|    fps                  | 7228        |
|    iterations           | 460         |
|    time_elapsed         | 2085        |
|    total_timesteps      | 15073280    |
| train/                  |             |
|    approx_kl            | 0.017158924 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 1.21        |
|    value_loss           | 0.0168      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.84e+03    |
| time/                   |             |
|    fps                  | 7223        |
|    iterations           | 470         |
|    time_elapsed         | 2131        |
|    total_timesteps      | 15400960    |
| train/                  |             |
|    approx_kl            | 0.016675778 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.165      |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.22        |
|    value_loss           | 0.013       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 3.75e+03    |
| time/                   |             |
|    fps                  | 7223        |
|    iterations           | 480         |
|    time_elapsed         | 2177        |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.017265387 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.166      |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 1.24        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=16000000, episode_reward=3522.63 +/- 347.14
Episode length: 273.80 +/- 34.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 3.52e+03    |
| time/                   |             |
|    total_timesteps      | 16000000    |
| train/                  |             |
|    approx_kl            | 0.016347557 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.166      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.25        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.93e+03    |
| time/                   |             |
|    fps                  | 7221        |
|    iterations           | 490         |
|    time_elapsed         | 2223        |
|    total_timesteps      | 16056320    |
| train/                  |             |
|    approx_kl            | 0.016210757 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.25        |
|    value_loss           | 0.015       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 326         |
|    ep_rew_mean          | 3.8e+03     |
| time/                   |             |
|    fps                  | 7222        |
|    iterations           | 500         |
|    time_elapsed         | 2268        |
|    total_timesteps      | 16384000    |
| train/                  |             |
|    approx_kl            | 0.017417781 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.164      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.27        |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 341         |
|    ep_rew_mean          | 3.87e+03    |
| time/                   |             |
|    fps                  | 7219        |
|    iterations           | 510         |
|    time_elapsed         | 2314        |
|    total_timesteps      | 16711680    |
| train/                  |             |
|    approx_kl            | 0.017543036 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.28        |
|    value_loss           | 0.0148      |
-----------------------------------------
Eval num_timesteps=17000000, episode_reward=4136.39 +/- 353.14
Episode length: 332.20 +/- 39.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 332         |
|    mean_reward          | 4.14e+03    |
| time/                   |             |
|    total_timesteps      | 17000000    |
| train/                  |             |
|    approx_kl            | 0.017526733 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.29        |
|    value_loss           | 0.014       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 4.27e+03    |
| time/                   |             |
|    fps                  | 7213        |
|    iterations           | 520         |
|    time_elapsed         | 2362        |
|    total_timesteps      | 17039360    |
| train/                  |             |
|    approx_kl            | 0.018134179 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.29        |
|    value_loss           | 0.015       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 334         |
|    ep_rew_mean          | 3.9e+03     |
| time/                   |             |
|    fps                  | 7212        |
|    iterations           | 530         |
|    time_elapsed         | 2407        |
|    total_timesteps      | 17367040    |
| train/                  |             |
|    approx_kl            | 0.017296806 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.167      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.3         |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.95e+03    |
| time/                   |             |
|    fps                  | 7210        |
|    iterations           | 540         |
|    time_elapsed         | 2454        |
|    total_timesteps      | 17694720    |
| train/                  |             |
|    approx_kl            | 0.018097993 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.17       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.32        |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=18000000, episode_reward=3926.36 +/- 1749.42
Episode length: 323.20 +/- 126.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 3.93e+03    |
| time/                   |             |
|    total_timesteps      | 18000000    |
| train/                  |             |
|    approx_kl            | 0.016725343 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.171      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.34        |
|    value_loss           | 0.0153      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | 4.14e+03 |
| time/              |          |
|    fps             | 7205     |
|    iterations      | 550      |
|    time_elapsed    | 2501     |
|    total_timesteps | 18022400 |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 334        |
|    ep_rew_mean          | 4.05e+03   |
| time/                   |            |
|    fps                  | 7203       |
|    iterations           | 560        |
|    time_elapsed         | 2547       |
|    total_timesteps      | 18350080   |
| train/                  |            |
|    approx_kl            | 0.01740751 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.172     |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.35       |
|    value_loss           | 0.0154     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 342        |
|    ep_rew_mean          | 4.17e+03   |
| time/                   |            |
|    fps                  | 7205       |
|    iterations           | 570        |
|    time_elapsed         | 2592       |
|    total_timesteps      | 18677760   |
| train/                  |            |
|    approx_kl            | 0.01653136 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.9      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.174     |
|    n_updates            | 5690       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.37       |
|    value_loss           | 0.0114     |
----------------------------------------
Eval num_timesteps=19000000, episode_reward=4540.70 +/- 507.54
Episode length: 361.20 +/- 27.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 361         |
|    mean_reward          | 4.54e+03    |
| time/                   |             |
|    total_timesteps      | 19000000    |
| train/                  |             |
|    approx_kl            | 0.017512705 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.39        |
|    value_loss           | 0.0138      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 4.2e+03  |
| time/              |          |
|    fps             | 7205     |
|    iterations      | 580      |
|    time_elapsed    | 2637     |
|    total_timesteps | 19005440 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.24e+03    |
| time/                   |             |
|    fps                  | 7207        |
|    iterations           | 590         |
|    time_elapsed         | 2682        |
|    total_timesteps      | 19333120    |
| train/                  |             |
|    approx_kl            | 0.017574277 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.4         |
|    value_loss           | 0.0148      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 342         |
|    ep_rew_mean          | 4.26e+03    |
| time/                   |             |
|    fps                  | 7209        |
|    iterations           | 600         |
|    time_elapsed         | 2727        |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.016366895 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.42        |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 331         |
|    ep_rew_mean          | 4.12e+03    |
| time/                   |             |
|    fps                  | 7211        |
|    iterations           | 610         |
|    time_elapsed         | 2771        |
|    total_timesteps      | 19988480    |
| train/                  |             |
|    approx_kl            | 0.018768266 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.177      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.44        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=20000000, episode_reward=5171.40 +/- 50.91
Episode length: 408.20 +/- 12.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 408        |
|    mean_reward          | 5.17e+03   |
| time/                   |            |
|    total_timesteps      | 20000000   |
| train/                  |            |
|    approx_kl            | 0.01691229 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.175     |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.0185    |
|    std                  | 1.44       |
|    value_loss           | 0.0137     |
----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 341        |
|    ep_rew_mean          | 4.17e+03   |
| time/                   |            |
|    fps                  | 7211       |
|    iterations           | 620        |
|    time_elapsed         | 2817       |
|    total_timesteps      | 20316160   |
| train/                  |            |
|    approx_kl            | 0.01759119 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.177     |
|    n_updates            | 6190       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 1.46       |
|    value_loss           | 0.0112     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 351         |
|    ep_rew_mean          | 4.39e+03    |
| time/                   |             |
|    fps                  | 7213        |
|    iterations           | 630         |
|    time_elapsed         | 2861        |
|    total_timesteps      | 20643840    |
| train/                  |             |
|    approx_kl            | 0.017498557 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.18       |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0217     |
|    std                  | 1.48        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 341         |
|    ep_rew_mean          | 4.28e+03    |
| time/                   |             |
|    fps                  | 7215        |
|    iterations           | 640         |
|    time_elapsed         | 2906        |
|    total_timesteps      | 20971520    |
| train/                  |             |
|    approx_kl            | 0.016870512 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.5         |
|    value_loss           | 0.0143      |
-----------------------------------------
Eval num_timesteps=21000000, episode_reward=5221.29 +/- 221.45
Episode length: 394.80 +/- 19.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 395        |
|    mean_reward          | 5.22e+03   |
| time/                   |            |
|    total_timesteps      | 21000000   |
| train/                  |            |
|    approx_kl            | 0.01738807 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.179     |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.5        |
|    value_loss           | 0.0136     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 336         |
|    ep_rew_mean          | 4.23e+03    |
| time/                   |             |
|    fps                  | 7215        |
|    iterations           | 650         |
|    time_elapsed         | 2952        |
|    total_timesteps      | 21299200    |
| train/                  |             |
|    approx_kl            | 0.018084588 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.179      |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.52        |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 355         |
|    ep_rew_mean          | 4.57e+03    |
| time/                   |             |
|    fps                  | 7216        |
|    iterations           | 660         |
|    time_elapsed         | 2996        |
|    total_timesteps      | 21626880    |
| train/                  |             |
|    approx_kl            | 0.018397799 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.54        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 354         |
|    ep_rew_mean          | 4.44e+03    |
| time/                   |             |
|    fps                  | 7218        |
|    iterations           | 670         |
|    time_elapsed         | 3041        |
|    total_timesteps      | 21954560    |
| train/                  |             |
|    approx_kl            | 0.018538786 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.183      |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.56        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=22000000, episode_reward=5182.98 +/- 150.62
Episode length: 387.80 +/- 21.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 388        |
|    mean_reward          | 5.18e+03   |
| time/                   |            |
|    total_timesteps      | 22000000   |
| train/                  |            |
|    approx_kl            | 0.02647432 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.181     |
|    n_updates            | 6710       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 1.56       |
|    value_loss           | 0.011      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | 4.5e+03     |
| time/                   |             |
|    fps                  | 7216        |
|    iterations           | 680         |
|    time_elapsed         | 3087        |
|    total_timesteps      | 22282240    |
| train/                  |             |
|    approx_kl            | 0.016482454 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.179      |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.58        |
|    value_loss           | 0.0128      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 351         |
|    ep_rew_mean          | 4.44e+03    |
| time/                   |             |
|    fps                  | 7218        |
|    iterations           | 690         |
|    time_elapsed         | 3132        |
|    total_timesteps      | 22609920    |
| train/                  |             |
|    approx_kl            | 0.016682032 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.6         |
|    value_loss           | 0.0124      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 342        |
|    ep_rew_mean          | 4.37e+03   |
| time/                   |            |
|    fps                  | 7220       |
|    iterations           | 700        |
|    time_elapsed         | 3176       |
|    total_timesteps      | 22937600   |
| train/                  |            |
|    approx_kl            | 0.01667092 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.179     |
|    n_updates            | 6990       |
|    policy_gradient_loss | -0.0179    |
|    std                  | 1.62       |
|    value_loss           | 0.0116     |
----------------------------------------
Eval num_timesteps=23000000, episode_reward=5101.62 +/- 331.57
Episode length: 396.00 +/- 22.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 396       |
|    mean_reward          | 5.1e+03   |
| time/                   |           |
|    total_timesteps      | 23000000  |
| train/                  |           |
|    approx_kl            | 0.0187371 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.2       |
|    entropy_loss         | -30.2     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.18     |
|    n_updates            | 7010      |
|    policy_gradient_loss | -0.0188   |
|    std                  | 1.63      |
|    value_loss           | 0.0152    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 354         |
|    ep_rew_mean          | 4.47e+03    |
| time/                   |             |
|    fps                  | 7219        |
|    iterations           | 710         |
|    time_elapsed         | 3222        |
|    total_timesteps      | 23265280    |
| train/                  |             |
|    approx_kl            | 0.018485758 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.64        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 344         |
|    ep_rew_mean          | 4.38e+03    |
| time/                   |             |
|    fps                  | 7220        |
|    iterations           | 720         |
|    time_elapsed         | 3267        |
|    total_timesteps      | 23592960    |
| train/                  |             |
|    approx_kl            | 0.016560469 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.184      |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.66        |
|    value_loss           | 0.0118      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 367         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7222        |
|    iterations           | 730         |
|    time_elapsed         | 3312        |
|    total_timesteps      | 23920640    |
| train/                  |             |
|    approx_kl            | 0.016537063 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.186      |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.68        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=24000000, episode_reward=5118.38 +/- 341.23
Episode length: 378.20 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 5.12e+03    |
| time/                   |             |
|    total_timesteps      | 24000000    |
| train/                  |             |
|    approx_kl            | 0.018093333 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.68        |
|    value_loss           | 0.0144      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 346        |
|    ep_rew_mean          | 4.37e+03   |
| time/                   |            |
|    fps                  | 7221       |
|    iterations           | 740        |
|    time_elapsed         | 3357       |
|    total_timesteps      | 24248320   |
| train/                  |            |
|    approx_kl            | 0.01740398 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.188     |
|    n_updates            | 7390       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.7        |
|    value_loss           | 0.0118     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | 4.56e+03    |
| time/                   |             |
|    fps                  | 7222        |
|    iterations           | 750         |
|    time_elapsed         | 3402        |
|    total_timesteps      | 24576000    |
| train/                  |             |
|    approx_kl            | 0.019105121 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.189      |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.73        |
|    value_loss           | 0.0118      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 351        |
|    ep_rew_mean          | 4.36e+03   |
| time/                   |            |
|    fps                  | 7224       |
|    iterations           | 760        |
|    time_elapsed         | 3447       |
|    total_timesteps      | 24903680   |
| train/                  |            |
|    approx_kl            | 0.01786679 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.189     |
|    n_updates            | 7590       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 1.75       |
|    value_loss           | 0.0108     |
----------------------------------------
Eval num_timesteps=25000000, episode_reward=4701.50 +/- 386.89
Episode length: 399.40 +/- 43.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 4.7e+03     |
| time/                   |             |
|    total_timesteps      | 25000000    |
| train/                  |             |
|    approx_kl            | 0.016979128 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.76        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 349         |
|    ep_rew_mean          | 4.38e+03    |
| time/                   |             |
|    fps                  | 7222        |
|    iterations           | 770         |
|    time_elapsed         | 3493        |
|    total_timesteps      | 25231360    |
| train/                  |             |
|    approx_kl            | 0.016883038 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.188      |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.77        |
|    value_loss           | 0.013       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.3e+03     |
| time/                   |             |
|    fps                  | 7224        |
|    iterations           | 780         |
|    time_elapsed         | 3537        |
|    total_timesteps      | 25559040    |
| train/                  |             |
|    approx_kl            | 0.017381586 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.8         |
|    value_loss           | 0.0175      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 360         |
|    ep_rew_mean          | 4.58e+03    |
| time/                   |             |
|    fps                  | 7225        |
|    iterations           | 790         |
|    time_elapsed         | 3582        |
|    total_timesteps      | 25886720    |
| train/                  |             |
|    approx_kl            | 0.017933274 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.194      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.83        |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=26000000, episode_reward=5248.21 +/- 295.28
Episode length: 410.20 +/- 19.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 410        |
|    mean_reward          | 5.25e+03   |
| time/                   |            |
|    total_timesteps      | 26000000   |
| train/                  |            |
|    approx_kl            | 0.01710907 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.192     |
|    n_updates            | 7930       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 1.84       |
|    value_loss           | 0.0129     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 365         |
|    ep_rew_mean          | 4.66e+03    |
| time/                   |             |
|    fps                  | 7224        |
|    iterations           | 800         |
|    time_elapsed         | 3628        |
|    total_timesteps      | 26214400    |
| train/                  |             |
|    approx_kl            | 0.018508002 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.193      |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.85        |
|    value_loss           | 0.011       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.76e+03    |
| time/                   |             |
|    fps                  | 7226        |
|    iterations           | 810         |
|    time_elapsed         | 3673        |
|    total_timesteps      | 26542080    |
| train/                  |             |
|    approx_kl            | 0.016981024 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.88        |
|    value_loss           | 0.0097      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | 4.67e+03    |
| time/                   |             |
|    fps                  | 7227        |
|    iterations           | 820         |
|    time_elapsed         | 3717        |
|    total_timesteps      | 26869760    |
| train/                  |             |
|    approx_kl            | 0.018075708 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.193      |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.91        |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=27000000, episode_reward=5281.48 +/- 240.72
Episode length: 416.00 +/- 23.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | 5.28e+03    |
| time/                   |             |
|    total_timesteps      | 27000000    |
| train/                  |             |
|    approx_kl            | 0.018653538 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8230        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 1.92        |
|    value_loss           | 0.0149      |
-----------------------------------------
New best mean reward!
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 370       |
|    ep_rew_mean          | 4.73e+03  |
| time/                   |           |
|    fps                  | 7227      |
|    iterations           | 830       |
|    time_elapsed         | 3763      |
|    total_timesteps      | 27197440  |
| train/                  |           |
|    approx_kl            | 0.0187455 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.2       |
|    entropy_loss         | -32.7     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.195    |
|    n_updates            | 8290      |
|    policy_gradient_loss | -0.0198   |
|    std                  | 1.94      |
|    value_loss           | 0.0125    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | 4.74e+03    |
| time/                   |             |
|    fps                  | 7228        |
|    iterations           | 840         |
|    time_elapsed         | 3807        |
|    total_timesteps      | 27525120    |
| train/                  |             |
|    approx_kl            | 0.018117836 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.96        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 4.47e+03    |
| time/                   |             |
|    fps                  | 7230        |
|    iterations           | 850         |
|    time_elapsed         | 3852        |
|    total_timesteps      | 27852800    |
| train/                  |             |
|    approx_kl            | 0.017775416 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.198      |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2           |
|    value_loss           | 0.0124      |
-----------------------------------------
Eval num_timesteps=28000000, episode_reward=4450.61 +/- 1480.44
Episode length: 336.60 +/- 93.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 337         |
|    mean_reward          | 4.45e+03    |
| time/                   |             |
|    total_timesteps      | 28000000    |
| train/                  |             |
|    approx_kl            | 0.016826686 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.199      |
|    n_updates            | 8540        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.01        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 4.71e+03    |
| time/                   |             |
|    fps                  | 7229        |
|    iterations           | 860         |
|    time_elapsed         | 3897        |
|    total_timesteps      | 28180480    |
| train/                  |             |
|    approx_kl            | 0.017207533 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.3       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.197      |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 2.02        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 362         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7230        |
|    iterations           | 870         |
|    time_elapsed         | 3942        |
|    total_timesteps      | 28508160    |
| train/                  |             |
|    approx_kl            | 0.019004185 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.05        |
|    value_loss           | 0.0103      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 382        |
|    ep_rew_mean          | 4.93e+03   |
| time/                   |            |
|    fps                  | 7232       |
|    iterations           | 880        |
|    time_elapsed         | 3987       |
|    total_timesteps      | 28835840   |
| train/                  |            |
|    approx_kl            | 0.01691171 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.201     |
|    n_updates            | 8790       |
|    policy_gradient_loss | -0.02      |
|    std                  | 2.07       |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=29000000, episode_reward=4603.59 +/- 1212.76
Episode length: 387.40 +/- 31.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 4.6e+03     |
| time/                   |             |
|    total_timesteps      | 29000000    |
| train/                  |             |
|    approx_kl            | 0.017333295 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8850        |
|    policy_gradient_loss | -0.02       |
|    std                  | 2.1         |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 360         |
|    ep_rew_mean          | 4.66e+03    |
| time/                   |             |
|    fps                  | 7231        |
|    iterations           | 890         |
|    time_elapsed         | 4032        |
|    total_timesteps      | 29163520    |
| train/                  |             |
|    approx_kl            | 0.015304524 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 2.1         |
|    value_loss           | 0.0104      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 366        |
|    ep_rew_mean          | 4.64e+03   |
| time/                   |            |
|    fps                  | 7232       |
|    iterations           | 900        |
|    time_elapsed         | 4077       |
|    total_timesteps      | 29491200   |
| train/                  |            |
|    approx_kl            | 0.01898979 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.204     |
|    n_updates            | 8990       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 2.13       |
|    value_loss           | 0.0131     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 344         |
|    ep_rew_mean          | 4.33e+03    |
| time/                   |             |
|    fps                  | 7234        |
|    iterations           | 910         |
|    time_elapsed         | 4121        |
|    total_timesteps      | 29818880    |
| train/                  |             |
|    approx_kl            | 0.017770976 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.203      |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.17        |
|    value_loss           | 0.0134      |
-----------------------------------------
Eval num_timesteps=30000000, episode_reward=5423.54 +/- 296.65
Episode length: 406.20 +/- 46.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | 5.42e+03    |
| time/                   |             |
|    total_timesteps      | 30000000    |
| train/                  |             |
|    approx_kl            | 0.017347744 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.205      |
|    n_updates            | 9150        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 2.19        |
|    value_loss           | 0.013       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 362         |
|    ep_rew_mean          | 4.7e+03     |
| time/                   |             |
|    fps                  | 7233        |
|    iterations           | 920         |
|    time_elapsed         | 4167        |
|    total_timesteps      | 30146560    |
| train/                  |             |
|    approx_kl            | 0.018128488 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.202      |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 2.2         |
|    value_loss           | 0.0169      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 367        |
|    ep_rew_mean          | 4.62e+03   |
| time/                   |            |
|    fps                  | 7234       |
|    iterations           | 930        |
|    time_elapsed         | 4212       |
|    total_timesteps      | 30474240   |
| train/                  |            |
|    approx_kl            | 0.01797387 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.205     |
|    n_updates            | 9290       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 2.23       |
|    value_loss           | 0.0136     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7235        |
|    iterations           | 940         |
|    time_elapsed         | 4256        |
|    total_timesteps      | 30801920    |
| train/                  |             |
|    approx_kl            | 0.018678594 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.207      |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.26        |
|    value_loss           | 0.0125      |
-----------------------------------------
Eval num_timesteps=31000000, episode_reward=5087.22 +/- 286.76
Episode length: 377.60 +/- 16.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 5.09e+03    |
| time/                   |             |
|    total_timesteps      | 31000000    |
| train/                  |             |
|    approx_kl            | 0.018121213 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.207      |
|    n_updates            | 9460        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 2.28        |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 4.58e+03    |
| time/                   |             |
|    fps                  | 7235        |
|    iterations           | 950         |
|    time_elapsed         | 4302        |
|    total_timesteps      | 31129600    |
| train/                  |             |
|    approx_kl            | 0.017271493 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9490        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 2.29        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | 4.61e+03    |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 960         |
|    time_elapsed         | 4347        |
|    total_timesteps      | 31457280    |
| train/                  |             |
|    approx_kl            | 0.018320246 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.33        |
|    value_loss           | 0.0164      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 364         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 970         |
|    time_elapsed         | 4392        |
|    total_timesteps      | 31784960    |
| train/                  |             |
|    approx_kl            | 0.018801183 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.36        |
|    value_loss           | 0.013       |
-----------------------------------------
Eval num_timesteps=32000000, episode_reward=4834.53 +/- 1282.49
Episode length: 381.00 +/- 50.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 4.83e+03    |
| time/                   |             |
|    total_timesteps      | 32000000    |
| train/                  |             |
|    approx_kl            | 0.017243747 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 2.38        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.78e+03    |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 980         |
|    time_elapsed         | 4437        |
|    total_timesteps      | 32112640    |
| train/                  |             |
|    approx_kl            | 0.018085113 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 2.39        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.36e+03    |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 990         |
|    time_elapsed         | 4482        |
|    total_timesteps      | 32440320    |
| train/                  |             |
|    approx_kl            | 0.018949928 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.213      |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.43        |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 372         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7237        |
|    iterations           | 1000        |
|    time_elapsed         | 4527        |
|    total_timesteps      | 32768000    |
| train/                  |             |
|    approx_kl            | 0.017659022 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36         |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.213      |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 2.46        |
|    value_loss           | 0.0132      |
-----------------------------------------
Eval num_timesteps=33000000, episode_reward=5378.75 +/- 173.01
Episode length: 404.20 +/- 10.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 5.38e+03    |
| time/                   |             |
|    total_timesteps      | 33000000    |
| train/                  |             |
|    approx_kl            | 0.016885985 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.48        |
|    value_loss           | 0.00991     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 380         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7237        |
|    iterations           | 1010        |
|    time_elapsed         | 4573        |
|    total_timesteps      | 33095680    |
| train/                  |             |
|    approx_kl            | 0.017528385 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.1       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 2.49        |
|    value_loss           | 0.00823     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 4.63e+03    |
| time/                   |             |
|    fps                  | 7237        |
|    iterations           | 1020        |
|    time_elapsed         | 4617        |
|    total_timesteps      | 33423360    |
| train/                  |             |
|    approx_kl            | 0.017909208 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.214      |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 2.53        |
|    value_loss           | 0.0131      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 371         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7238        |
|    iterations           | 1030        |
|    time_elapsed         | 4662        |
|    total_timesteps      | 33751040    |
| train/                  |             |
|    approx_kl            | 0.018018413 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.5       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.56        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=34000000, episode_reward=5476.07 +/- 187.16
Episode length: 407.40 +/- 29.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 407        |
|    mean_reward          | 5.48e+03   |
| time/                   |            |
|    total_timesteps      | 34000000   |
| train/                  |            |
|    approx_kl            | 0.01725314 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 10370      |
|    policy_gradient_loss | -0.0193    |
|    std                  | 2.59       |
|    value_loss           | 0.0115     |
----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 371        |
|    ep_rew_mean          | 4.72e+03   |
| time/                   |            |
|    fps                  | 7238       |
|    iterations           | 1040       |
|    time_elapsed         | 4708       |
|    total_timesteps      | 34078720   |
| train/                  |            |
|    approx_kl            | 0.01804078 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 10390      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 2.6        |
|    value_loss           | 0.0134     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 1050        |
|    time_elapsed         | 4752        |
|    total_timesteps      | 34406400    |
| train/                  |             |
|    approx_kl            | 0.018689679 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0213     |
|    std                  | 2.64        |
|    value_loss           | 0.00877     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 1060        |
|    time_elapsed         | 4797        |
|    total_timesteps      | 34734080    |
| train/                  |             |
|    approx_kl            | 0.017338932 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.67        |
|    value_loss           | 0.00907     |
-----------------------------------------
Eval num_timesteps=35000000, episode_reward=5341.75 +/- 139.59
Episode length: 399.60 +/- 21.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 5.34e+03    |
| time/                   |             |
|    total_timesteps      | 35000000    |
| train/                  |             |
|    approx_kl            | 0.018085858 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.71        |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 1070        |
|    time_elapsed         | 4843        |
|    total_timesteps      | 35061760    |
| train/                  |             |
|    approx_kl            | 0.018646136 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.71        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.78e+03    |
| time/                   |             |
|    fps                  | 7240        |
|    iterations           | 1080        |
|    time_elapsed         | 4888        |
|    total_timesteps      | 35389440    |
| train/                  |             |
|    approx_kl            | 0.018206667 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.5       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.221      |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.75        |
|    value_loss           | 0.0158      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7241        |
|    iterations           | 1090        |
|    time_elapsed         | 4932        |
|    total_timesteps      | 35717120    |
| train/                  |             |
|    approx_kl            | 0.018956661 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.222      |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 2.79        |
|    value_loss           | 0.0123      |
-----------------------------------------
Eval num_timesteps=36000000, episode_reward=5528.24 +/- 299.52
Episode length: 402.00 +/- 17.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 5.53e+03    |
| time/                   |             |
|    total_timesteps      | 36000000    |
| train/                  |             |
|    approx_kl            | 0.017256018 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.224      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.83        |
|    value_loss           | 0.00983     |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 371         |
|    ep_rew_mean          | 4.7e+03     |
| time/                   |             |
|    fps                  | 7240        |
|    iterations           | 1100        |
|    time_elapsed         | 4978        |
|    total_timesteps      | 36044800    |
| train/                  |             |
|    approx_kl            | 0.017237555 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.221      |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 2.83        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 385         |
|    ep_rew_mean          | 4.93e+03    |
| time/                   |             |
|    fps                  | 7241        |
|    iterations           | 1110        |
|    time_elapsed         | 5023        |
|    total_timesteps      | 36372480    |
| train/                  |             |
|    approx_kl            | 0.018586453 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.223      |
|    n_updates            | 11090       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.87        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 4.94e+03    |
| time/                   |             |
|    fps                  | 7241        |
|    iterations           | 1120        |
|    time_elapsed         | 5067        |
|    total_timesteps      | 36700160    |
| train/                  |             |
|    approx_kl            | 0.018611297 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.91        |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=37000000, episode_reward=5560.07 +/- 84.52
Episode length: 408.00 +/- 7.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 5.56e+03    |
| time/                   |             |
|    total_timesteps      | 37000000    |
| train/                  |             |
|    approx_kl            | 0.018078716 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.226      |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.96        |
|    value_loss           | 0.0105      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 4.93e+03 |
| time/              |          |
|    fps             | 7241     |
|    iterations      | 1130     |
|    time_elapsed    | 5113     |
|    total_timesteps | 37027840 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7242        |
|    iterations           | 1140        |
|    time_elapsed         | 5158        |
|    total_timesteps      | 37355520    |
| train/                  |             |
|    approx_kl            | 0.016109794 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 3.01        |
|    value_loss           | 0.0106      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 378         |
|    ep_rew_mean          | 4.8e+03     |
| time/                   |             |
|    fps                  | 7242        |
|    iterations           | 1150        |
|    time_elapsed         | 5202        |
|    total_timesteps      | 37683200    |
| train/                  |             |
|    approx_kl            | 0.017058615 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 3.06        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=38000000, episode_reward=5538.03 +/- 139.79
Episode length: 413.20 +/- 17.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 5.54e+03    |
| time/                   |             |
|    total_timesteps      | 38000000    |
| train/                  |             |
|    approx_kl            | 0.018478842 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.228      |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 3.11        |
|    value_loss           | 0.0128      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 381      |
|    ep_rew_mean     | 4.87e+03 |
| time/              |          |
|    fps             | 7242     |
|    iterations      | 1160     |
|    time_elapsed    | 5248     |
|    total_timesteps | 38010880 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.8e+03     |
| time/                   |             |
|    fps                  | 7242        |
|    iterations           | 1170        |
|    time_elapsed         | 5293        |
|    total_timesteps      | 38338560    |
| train/                  |             |
|    approx_kl            | 0.016657647 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 3.16        |
|    value_loss           | 0.00937     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 355         |
|    ep_rew_mean          | 4.55e+03    |
| time/                   |             |
|    fps                  | 7243        |
|    iterations           | 1180        |
|    time_elapsed         | 5337        |
|    total_timesteps      | 38666240    |
| train/                  |             |
|    approx_kl            | 0.018213741 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.232      |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 3.21        |
|    value_loss           | 0.0111      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.75e+03    |
| time/                   |             |
|    fps                  | 7244        |
|    iterations           | 1190        |
|    time_elapsed         | 5382        |
|    total_timesteps      | 38993920    |
| train/                  |             |
|    approx_kl            | 0.016899161 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 3.26        |
|    value_loss           | 0.0138      |
-----------------------------------------
Eval num_timesteps=39000000, episode_reward=5289.10 +/- 780.35
Episode length: 391.40 +/- 63.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 5.29e+03    |
| time/                   |             |
|    total_timesteps      | 39000000    |
| train/                  |             |
|    approx_kl            | 0.017097887 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.23       |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 3.27        |
|    value_loss           | 0.0138      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 374         |
|    ep_rew_mean          | 4.73e+03    |
| time/                   |             |
|    fps                  | 7243        |
|    iterations           | 1200        |
|    time_elapsed         | 5428        |
|    total_timesteps      | 39321600    |
| train/                  |             |
|    approx_kl            | 0.017440893 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.231      |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 3.32        |
|    value_loss           | 0.0114      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7244        |
|    iterations           | 1210        |
|    time_elapsed         | 5473        |
|    total_timesteps      | 39649280    |
| train/                  |             |
|    approx_kl            | 0.017910779 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.37        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 374         |
|    ep_rew_mean          | 4.79e+03    |
| time/                   |             |
|    fps                  | 7244        |
|    iterations           | 1220        |
|    time_elapsed         | 5518        |
|    total_timesteps      | 39976960    |
| train/                  |             |
|    approx_kl            | 0.017491309 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 3.41        |
|    value_loss           | 0.00968     |
-----------------------------------------
Eval num_timesteps=40000000, episode_reward=5630.62 +/- 47.55
Episode length: 403.80 +/- 7.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 5.63e+03    |
| time/                   |             |
|    total_timesteps      | 40000000    |
| train/                  |             |
|    approx_kl            | 0.018493395 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.235      |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 3.42        |
|    value_loss           | 0.0116      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.03e+03    |
| time/                   |             |
|    fps                  | 7244        |
|    iterations           | 1230        |
|    time_elapsed         | 5563        |
|    total_timesteps      | 40304640    |
| train/                  |             |
|    approx_kl            | 0.017220616 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 3.46        |
|    value_loss           | 0.0104      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.99e+03   |
| time/                   |            |
|    fps                  | 7244       |
|    iterations           | 1240       |
|    time_elapsed         | 5608       |
|    total_timesteps      | 40632320   |
| train/                  |            |
|    approx_kl            | 0.01731256 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.239     |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.0206    |
|    std                  | 3.53       |
|    value_loss           | 0.00928    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 393        |
|    ep_rew_mean          | 5.08e+03   |
| time/                   |            |
|    fps                  | 7245       |
|    iterations           | 1250       |
|    time_elapsed         | 5652       |
|    total_timesteps      | 40960000   |
| train/                  |            |
|    approx_kl            | 0.01681754 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -41.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.239     |
|    n_updates            | 12490      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 3.58       |
|    value_loss           | 0.0098     |
----------------------------------------
Eval num_timesteps=41000000, episode_reward=5581.22 +/- 244.68
Episode length: 418.40 +/- 17.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 5.58e+03    |
| time/                   |             |
|    total_timesteps      | 41000000    |
| train/                  |             |
|    approx_kl            | 0.015942145 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.236      |
|    n_updates            | 12510       |
|    policy_gradient_loss | -0.018      |
|    std                  | 3.59        |
|    value_loss           | 0.00904     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7245        |
|    iterations           | 1260        |
|    time_elapsed         | 5698        |
|    total_timesteps      | 41287680    |
| train/                  |             |
|    approx_kl            | 0.017004274 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.242      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 3.63        |
|    value_loss           | 0.0108      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 389        |
|    ep_rew_mean          | 4.96e+03   |
| time/                   |            |
|    fps                  | 7245       |
|    iterations           | 1270       |
|    time_elapsed         | 5743       |
|    total_timesteps      | 41615360   |
| train/                  |            |
|    approx_kl            | 0.01736335 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.24      |
|    n_updates            | 12690      |
|    policy_gradient_loss | -0.0192    |
|    std                  | 3.67       |
|    value_loss           | 0.0117     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.76e+03    |
| time/                   |             |
|    fps                  | 7246        |
|    iterations           | 1280        |
|    time_elapsed         | 5788        |
|    total_timesteps      | 41943040    |
| train/                  |             |
|    approx_kl            | 0.017719254 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.24       |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 3.72        |
|    value_loss           | 0.0126      |
-----------------------------------------
Eval num_timesteps=42000000, episode_reward=5592.67 +/- 264.78
Episode length: 408.40 +/- 14.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 5.59e+03    |
| time/                   |             |
|    total_timesteps      | 42000000    |
| train/                  |             |
|    approx_kl            | 0.017680012 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.239      |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 3.74        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | 5.02e+03    |
| time/                   |             |
|    fps                  | 7245        |
|    iterations           | 1290        |
|    time_elapsed         | 5833        |
|    total_timesteps      | 42270720    |
| train/                  |             |
|    approx_kl            | 0.017689805 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 3.79        |
|    value_loss           | 0.00899     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.79e+03    |
| time/                   |             |
|    fps                  | 7246        |
|    iterations           | 1300        |
|    time_elapsed         | 5878        |
|    total_timesteps      | 42598400    |
| train/                  |             |
|    approx_kl            | 0.017659217 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.244      |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.02       |
|    std                  | 3.84        |
|    value_loss           | 0.00927     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 5.01e+03    |
| time/                   |             |
|    fps                  | 7247        |
|    iterations           | 1310        |
|    time_elapsed         | 5923        |
|    total_timesteps      | 42926080    |
| train/                  |             |
|    approx_kl            | 0.018011194 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13090       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.91        |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=43000000, episode_reward=5572.71 +/- 229.80
Episode length: 424.80 +/- 19.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 425        |
|    mean_reward          | 5.57e+03   |
| time/                   |            |
|    total_timesteps      | 43000000   |
| train/                  |            |
|    approx_kl            | 0.01777501 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.246     |
|    n_updates            | 13120      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 3.93       |
|    value_loss           | 0.0122     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 5e+03      |
| time/                   |            |
|    fps                  | 7246       |
|    iterations           | 1320       |
|    time_elapsed         | 5968       |
|    total_timesteps      | 43253760   |
| train/                  |            |
|    approx_kl            | 0.01682643 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.245     |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.0192    |
|    std                  | 3.97       |
|    value_loss           | 0.00955    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 5e+03       |
| time/                   |             |
|    fps                  | 7247        |
|    iterations           | 1330        |
|    time_elapsed         | 6013        |
|    total_timesteps      | 43581440    |
| train/                  |             |
|    approx_kl            | 0.017707694 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.03        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 7248        |
|    iterations           | 1340        |
|    time_elapsed         | 6058        |
|    total_timesteps      | 43909120    |
| train/                  |             |
|    approx_kl            | 0.017714273 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.247      |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.1         |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=44000000, episode_reward=5444.22 +/- 137.16
Episode length: 399.80 +/- 17.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 5.44e+03    |
| time/                   |             |
|    total_timesteps      | 44000000    |
| train/                  |             |
|    approx_kl            | 0.017493758 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.251      |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 4.12        |
|    value_loss           | 0.00774     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.31e+03    |
| time/                   |             |
|    fps                  | 7247        |
|    iterations           | 1350        |
|    time_elapsed         | 6103        |
|    total_timesteps      | 44236800    |
| train/                  |             |
|    approx_kl            | 0.017894806 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.2       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.25       |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 4.18        |
|    value_loss           | 0.00802     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 379        |
|    ep_rew_mean          | 4.88e+03   |
| time/                   |            |
|    fps                  | 7248       |
|    iterations           | 1360       |
|    time_elapsed         | 6148       |
|    total_timesteps      | 44564480   |
| train/                  |            |
|    approx_kl            | 0.01755324 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.248     |
|    n_updates            | 13590      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 4.25       |
|    value_loss           | 0.0118     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 370        |
|    ep_rew_mean          | 4.84e+03   |
| time/                   |            |
|    fps                  | 7248       |
|    iterations           | 1370       |
|    time_elapsed         | 6192       |
|    total_timesteps      | 44892160   |
| train/                  |            |
|    approx_kl            | 0.01805049 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.252     |
|    n_updates            | 13690      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 4.3        |
|    value_loss           | 0.0129     |
----------------------------------------
Eval num_timesteps=45000000, episode_reward=4615.85 +/- 2134.32
Episode length: 353.40 +/- 150.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | 4.62e+03    |
| time/                   |             |
|    total_timesteps      | 45000000    |
| train/                  |             |
|    approx_kl            | 0.017828993 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.252      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.34        |
|    value_loss           | 0.00832     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.91e+03    |
| time/                   |             |
|    fps                  | 7248        |
|    iterations           | 1380        |
|    time_elapsed         | 6238        |
|    total_timesteps      | 45219840    |
| train/                  |             |
|    approx_kl            | 0.016782328 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.249      |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 4.37        |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 5.3e+03     |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 1390        |
|    time_elapsed         | 6283        |
|    total_timesteps      | 45547520    |
| train/                  |             |
|    approx_kl            | 0.017340746 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.256      |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 4.44        |
|    value_loss           | 0.00838     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 1400        |
|    time_elapsed         | 6327        |
|    total_timesteps      | 45875200    |
| train/                  |             |
|    approx_kl            | 0.017470203 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.253      |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.52        |
|    value_loss           | 0.00876     |
-----------------------------------------
Eval num_timesteps=46000000, episode_reward=5396.74 +/- 234.91
Episode length: 405.40 +/- 12.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 5.4e+03     |
| time/                   |             |
|    total_timesteps      | 46000000    |
| train/                  |             |
|    approx_kl            | 0.017440865 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 14030       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 4.55        |
|    value_loss           | 0.0128      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.09e+03    |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 1410        |
|    time_elapsed         | 6373        |
|    total_timesteps      | 46202880    |
| train/                  |             |
|    approx_kl            | 0.018322617 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.254      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 4.59        |
|    value_loss           | 0.00963     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.89e+03    |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 1420        |
|    time_elapsed         | 6418        |
|    total_timesteps      | 46530560    |
| train/                  |             |
|    approx_kl            | 0.018709254 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 4.68        |
|    value_loss           | 0.00952     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7250        |
|    iterations           | 1430        |
|    time_elapsed         | 6462        |
|    total_timesteps      | 46858240    |
| train/                  |             |
|    approx_kl            | 0.018500438 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.256      |
|    n_updates            | 14290       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.76        |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=47000000, episode_reward=5567.11 +/- 127.24
Episode length: 442.20 +/- 55.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | 5.57e+03    |
| time/                   |             |
|    total_timesteps      | 47000000    |
| train/                  |             |
|    approx_kl            | 0.017972529 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.257      |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 4.79        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 1440        |
|    time_elapsed         | 6508        |
|    total_timesteps      | 47185920    |
| train/                  |             |
|    approx_kl            | 0.018135302 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 4.83        |
|    value_loss           | 0.0111      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 377        |
|    ep_rew_mean          | 4.78e+03   |
| time/                   |            |
|    fps                  | 7250       |
|    iterations           | 1450       |
|    time_elapsed         | 6553       |
|    total_timesteps      | 47513600   |
| train/                  |            |
|    approx_kl            | 0.01905592 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.26      |
|    n_updates            | 14490      |
|    policy_gradient_loss | -0.02      |
|    std                  | 4.92       |
|    value_loss           | 0.0141     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.87e+03    |
| time/                   |             |
|    fps                  | 7250        |
|    iterations           | 1460        |
|    time_elapsed         | 6597        |
|    total_timesteps      | 47841280    |
| train/                  |             |
|    approx_kl            | 0.018095752 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 14590       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.99        |
|    value_loss           | 0.0142      |
-----------------------------------------
Eval num_timesteps=48000000, episode_reward=5079.11 +/- 1345.23
Episode length: 395.80 +/- 58.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 5.08e+03    |
| time/                   |             |
|    total_timesteps      | 48000000    |
| train/                  |             |
|    approx_kl            | 0.018362785 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.262      |
|    n_updates            | 14640       |
|    policy_gradient_loss | -0.02       |
|    std                  | 5.03        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 384         |
|    ep_rew_mean          | 4.89e+03    |
| time/                   |             |
|    fps                  | 7250        |
|    iterations           | 1470        |
|    time_elapsed         | 6643        |
|    total_timesteps      | 48168960    |
| train/                  |             |
|    approx_kl            | 0.018041953 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 14690       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 5.07        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 7250        |
|    iterations           | 1480        |
|    time_elapsed         | 6688        |
|    total_timesteps      | 48496640    |
| train/                  |             |
|    approx_kl            | 0.018324332 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -46         |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 5.17        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.85e+03    |
| time/                   |             |
|    fps                  | 7251        |
|    iterations           | 1490        |
|    time_elapsed         | 6733        |
|    total_timesteps      | 48824320    |
| train/                  |             |
|    approx_kl            | 0.017617572 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.264      |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 5.24        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=49000000, episode_reward=5687.60 +/- 90.06
Episode length: 432.40 +/- 9.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 5.69e+03    |
| time/                   |             |
|    total_timesteps      | 49000000    |
| train/                  |             |
|    approx_kl            | 0.017861772 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.263      |
|    n_updates            | 14950       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 5.31        |
|    value_loss           | 0.0147      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.9e+03    |
| time/                   |            |
|    fps                  | 7250       |
|    iterations           | 1500       |
|    time_elapsed         | 6779       |
|    total_timesteps      | 49152000   |
| train/                  |            |
|    approx_kl            | 0.01769726 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -46.4      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.264     |
|    n_updates            | 14990      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 5.35       |
|    value_loss           | 0.0163     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.93e+03   |
| time/                   |            |
|    fps                  | 7251       |
|    iterations           | 1510       |
|    time_elapsed         | 6823       |
|    total_timesteps      | 49479680   |
| train/                  |            |
|    approx_kl            | 0.01776395 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -46.7      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.265     |
|    n_updates            | 15090      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 5.45       |
|    value_loss           | 0.0164     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 403         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7251        |
|    iterations           | 1520        |
|    time_elapsed         | 6868        |
|    total_timesteps      | 49807360    |
| train/                  |             |
|    approx_kl            | 0.017881848 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.266      |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 5.53        |
|    value_loss           | 0.0117      |
-----------------------------------------
Eval num_timesteps=50000000, episode_reward=5845.80 +/- 95.41
Episode length: 447.40 +/- 14.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 447         |
|    mean_reward          | 5.85e+03    |
| time/                   |             |
|    total_timesteps      | 50000000    |
| train/                  |             |
|    approx_kl            | 0.017441723 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15250       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.58        |
|    value_loss           | 0.013       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.87e+03    |
| time/                   |             |
|    fps                  | 7251        |
|    iterations           | 1530        |
|    time_elapsed         | 6913        |
|    total_timesteps      | 50135040    |
| train/                  |             |
|    approx_kl            | 0.018137576 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.266      |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.019      |
|    std                  | 5.61        |
|    value_loss           | 0.0112      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 1540        |
|    time_elapsed         | 6958        |
|    total_timesteps      | 50462720    |
| train/                  |             |
|    approx_kl            | 0.018313557 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 5.7         |
|    value_loss           | 0.0138      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 380         |
|    ep_rew_mean          | 4.92e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 1550        |
|    time_elapsed         | 7002        |
|    total_timesteps      | 50790400    |
| train/                  |             |
|    approx_kl            | 0.019468626 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.269      |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 5.79        |
|    value_loss           | 0.0131      |
-----------------------------------------
Eval num_timesteps=51000000, episode_reward=5617.98 +/- 47.78
Episode length: 412.60 +/- 12.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 5.62e+03    |
| time/                   |             |
|    total_timesteps      | 51000000    |
| train/                  |             |
|    approx_kl            | 0.017993655 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15560       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 5.84        |
|    value_loss           | 0.0153      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 386         |
|    ep_rew_mean          | 4.99e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 1560        |
|    time_elapsed         | 7048        |
|    total_timesteps      | 51118080    |
| train/                  |             |
|    approx_kl            | 0.017883794 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.269      |
|    n_updates            | 15590       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.86        |
|    value_loss           | 0.00916     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 1570        |
|    time_elapsed         | 7093        |
|    total_timesteps      | 51445760    |
| train/                  |             |
|    approx_kl            | 0.017446391 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.7       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.271      |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 5.96        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7253        |
|    iterations           | 1580        |
|    time_elapsed         | 7137        |
|    total_timesteps      | 51773440    |
| train/                  |             |
|    approx_kl            | 0.017643353 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 6.06        |
|    value_loss           | 0.012       |
-----------------------------------------
Eval num_timesteps=52000000, episode_reward=5660.47 +/- 105.45
Episode length: 422.20 +/- 17.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 5.66e+03    |
| time/                   |             |
|    total_timesteps      | 52000000    |
| train/                  |             |
|    approx_kl            | 0.018578522 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15860       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 6.12        |
|    value_loss           | 0.0129      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7252        |
|    iterations           | 1590        |
|    time_elapsed         | 7183        |
|    total_timesteps      | 52101120    |
| train/                  |             |
|    approx_kl            | 0.017974555 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 6.16        |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 389         |
|    ep_rew_mean          | 4.97e+03    |
| time/                   |             |
|    fps                  | 7253        |
|    iterations           | 1600        |
|    time_elapsed         | 7228        |
|    total_timesteps      | 52428800    |
| train/                  |             |
|    approx_kl            | 0.018748138 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 15990       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 6.27        |
|    value_loss           | 0.0117      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 388        |
|    ep_rew_mean          | 5.08e+03   |
| time/                   |            |
|    fps                  | 7253       |
|    iterations           | 1610       |
|    time_elapsed         | 7272       |
|    total_timesteps      | 52756480   |
| train/                  |            |
|    approx_kl            | 0.01735156 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.4      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.274     |
|    n_updates            | 16090      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 6.33       |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=53000000, episode_reward=4965.02 +/- 1404.86
Episode length: 410.60 +/- 50.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 411         |
|    mean_reward          | 4.97e+03    |
| time/                   |             |
|    total_timesteps      | 53000000    |
| train/                  |             |
|    approx_kl            | 0.018463604 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 16170       |
|    policy_gradient_loss | -0.02       |
|    std                  | 6.4         |
|    value_loss           | 0.0132      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.18e+03    |
| time/                   |             |
|    fps                  | 7253        |
|    iterations           | 1620        |
|    time_elapsed         | 7318        |
|    total_timesteps      | 53084160    |
| train/                  |             |
|    approx_kl            | 0.016733533 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.276      |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 6.43        |
|    value_loss           | 0.00969     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 396        |
|    ep_rew_mean          | 5.16e+03   |
| time/                   |            |
|    fps                  | 7253       |
|    iterations           | 1630       |
|    time_elapsed         | 7363       |
|    total_timesteps      | 53411840   |
| train/                  |            |
|    approx_kl            | 0.01919121 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.275     |
|    n_updates            | 16290      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 6.53       |
|    value_loss           | 0.0121     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7254        |
|    iterations           | 1640        |
|    time_elapsed         | 7407        |
|    total_timesteps      | 53739520    |
| train/                  |             |
|    approx_kl            | 0.016685616 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -49         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 16390       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 6.62        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=54000000, episode_reward=3374.84 +/- 2167.86
Episode length: 312.80 +/- 143.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | 3.37e+03    |
| time/                   |             |
|    total_timesteps      | 54000000    |
| train/                  |             |
|    approx_kl            | 0.018962035 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.277      |
|    n_updates            | 16470       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 6.71        |
|    value_loss           | 0.0139      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 392        |
|    ep_rew_mean          | 5.04e+03   |
| time/                   |            |
|    fps                  | 7254       |
|    iterations           | 1650       |
|    time_elapsed         | 7453       |
|    total_timesteps      | 54067200   |
| train/                  |            |
|    approx_kl            | 0.01685045 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -49.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.278     |
|    n_updates            | 16490      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 6.75       |
|    value_loss           | 0.0116     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 7254        |
|    iterations           | 1660        |
|    time_elapsed         | 7497        |
|    total_timesteps      | 54394880    |
| train/                  |             |
|    approx_kl            | 0.017557856 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 16590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.81        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 1670        |
|    time_elapsed         | 7542        |
|    total_timesteps      | 54722560    |
| train/                  |             |
|    approx_kl            | 0.017943181 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.5       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 16690       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 6.91        |
|    value_loss           | 0.00998     |
-----------------------------------------
Eval num_timesteps=55000000, episode_reward=5133.60 +/- 1271.56
Episode length: 407.20 +/- 37.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 407         |
|    mean_reward          | 5.13e+03    |
| time/                   |             |
|    total_timesteps      | 55000000    |
| train/                  |             |
|    approx_kl            | 0.017853606 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.7       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.28       |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 7.01        |
|    value_loss           | 0.0141      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 4.96e+03    |
| time/                   |             |
|    fps                  | 7254        |
|    iterations           | 1680        |
|    time_elapsed         | 7588        |
|    total_timesteps      | 55050240    |
| train/                  |             |
|    approx_kl            | 0.017672388 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 16790       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.02        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 409         |
|    ep_rew_mean          | 5.34e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 1690        |
|    time_elapsed         | 7632        |
|    total_timesteps      | 55377920    |
| train/                  |             |
|    approx_kl            | 0.018253844 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.284      |
|    n_updates            | 16890       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.13        |
|    value_loss           | 0.01        |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 391       |
|    ep_rew_mean          | 5.15e+03  |
| time/                   |           |
|    fps                  | 7255      |
|    iterations           | 1700      |
|    time_elapsed         | 7677      |
|    total_timesteps      | 55705600  |
| train/                  |           |
|    approx_kl            | 0.0173188 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -50.1     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.281    |
|    n_updates            | 16990     |
|    policy_gradient_loss | -0.0187   |
|    std                  | 7.28      |
|    value_loss           | 0.0117    |
---------------------------------------
Eval num_timesteps=56000000, episode_reward=5597.75 +/- 275.99
Episode length: 427.20 +/- 8.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | 5.6e+03     |
| time/                   |             |
|    total_timesteps      | 56000000    |
| train/                  |             |
|    approx_kl            | 0.017918652 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 17080       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 7.4         |
|    value_loss           | 0.0142      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 405        |
|    ep_rew_mean          | 5.24e+03   |
| time/                   |            |
|    fps                  | 7255       |
|    iterations           | 1710       |
|    time_elapsed         | 7723       |
|    total_timesteps      | 56033280   |
| train/                  |            |
|    approx_kl            | 0.01761654 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.284     |
|    n_updates            | 17090      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 7.41       |
|    value_loss           | 0.0113     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 1720        |
|    time_elapsed         | 7767        |
|    total_timesteps      | 56360960    |
| train/                  |             |
|    approx_kl            | 0.018609986 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.6       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 17190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.53        |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 384         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 7256        |
|    iterations           | 1730        |
|    time_elapsed         | 7812        |
|    total_timesteps      | 56688640    |
| train/                  |             |
|    approx_kl            | 0.016934318 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.284      |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 7.68        |
|    value_loss           | 0.0102      |
-----------------------------------------
Eval num_timesteps=57000000, episode_reward=5596.90 +/- 390.05
Episode length: 429.00 +/- 23.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | 5.6e+03     |
| time/                   |             |
|    total_timesteps      | 57000000    |
| train/                  |             |
|    approx_kl            | 0.018105108 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 17390       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 7.82        |
|    value_loss           | 0.0121      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 7255     |
|    iterations      | 1740     |
|    time_elapsed    | 7857     |
|    total_timesteps | 57016320 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 382         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7256        |
|    iterations           | 1750        |
|    time_elapsed         | 7902        |
|    total_timesteps      | 57344000    |
| train/                  |             |
|    approx_kl            | 0.018229883 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.289      |
|    n_updates            | 17490       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.97        |
|    value_loss           | 0.0141      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7256        |
|    iterations           | 1760        |
|    time_elapsed         | 7947        |
|    total_timesteps      | 57671680    |
| train/                  |             |
|    approx_kl            | 0.018015455 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.292      |
|    n_updates            | 17590       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 8.12        |
|    value_loss           | 0.00955     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 1770        |
|    time_elapsed         | 7991        |
|    total_timesteps      | 57999360    |
| train/                  |             |
|    approx_kl            | 0.019724282 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.29       |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 8.23        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=58000000, episode_reward=5862.12 +/- 248.00
Episode length: 452.60 +/- 34.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | 5.86e+03    |
| time/                   |             |
|    total_timesteps      | 58000000    |
| train/                  |             |
|    approx_kl            | 0.017835245 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 17700       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 8.23        |
|    value_loss           | 0.0115      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | 5.24e+03    |
| time/                   |             |
|    fps                  | 7256        |
|    iterations           | 1780        |
|    time_elapsed         | 8037        |
|    total_timesteps      | 58327040    |
| train/                  |             |
|    approx_kl            | 0.016612656 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.292      |
|    n_updates            | 17790       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 8.35        |
|    value_loss           | 0.0099      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 1790        |
|    time_elapsed         | 8082        |
|    total_timesteps      | 58654720    |
| train/                  |             |
|    approx_kl            | 0.018090686 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 17890       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 8.48        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.17e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 1800        |
|    time_elapsed         | 8126        |
|    total_timesteps      | 58982400    |
| train/                  |             |
|    approx_kl            | 0.017477863 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 17990       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 8.55        |
|    value_loss           | 0.00846     |
-----------------------------------------
Eval num_timesteps=59000000, episode_reward=5106.09 +/- 849.26
Episode length: 393.00 +/- 84.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 393        |
|    mean_reward          | 5.11e+03   |
| time/                   |            |
|    total_timesteps      | 59000000   |
| train/                  |            |
|    approx_kl            | 0.01763868 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.292     |
|    n_updates            | 18000      |
|    policy_gradient_loss | -0.019     |
|    std                  | 8.57       |
|    value_loss           | 0.0101     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.18e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 1810        |
|    time_elapsed         | 8172        |
|    total_timesteps      | 59310080    |
| train/                  |             |
|    approx_kl            | 0.018013297 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 18090       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 8.7         |
|    value_loss           | 0.00978     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 1820        |
|    time_elapsed         | 8217        |
|    total_timesteps      | 59637760    |
| train/                  |             |
|    approx_kl            | 0.017770989 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 8.89        |
|    value_loss           | 0.00936     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1830        |
|    time_elapsed         | 8261        |
|    total_timesteps      | 59965440    |
| train/                  |             |
|    approx_kl            | 0.017772157 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 9.02        |
|    value_loss           | 0.0106      |
-----------------------------------------
Eval num_timesteps=60000000, episode_reward=5854.82 +/- 227.84
Episode length: 432.40 +/- 32.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 5.85e+03    |
| time/                   |             |
|    total_timesteps      | 60000000    |
| train/                  |             |
|    approx_kl            | 0.017018843 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18310       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 9.05        |
|    value_loss           | 0.0113      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 5.38e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 1840       |
|    time_elapsed         | 8307       |
|    total_timesteps      | 60293120   |
| train/                  |            |
|    approx_kl            | 0.01808159 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.9      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 18390      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 9.14       |
|    value_loss           | 0.0106     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 391        |
|    ep_rew_mean          | 5.02e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 1850       |
|    time_elapsed         | 8351       |
|    total_timesteps      | 60620800   |
| train/                  |            |
|    approx_kl            | 0.01669118 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 18490      |
|    policy_gradient_loss | -0.019     |
|    std                  | 9.26       |
|    value_loss           | 0.0131     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1860        |
|    time_elapsed         | 8396        |
|    total_timesteps      | 60948480    |
| train/                  |             |
|    approx_kl            | 0.017434653 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 9.37        |
|    value_loss           | 0.014       |
-----------------------------------------
Eval num_timesteps=61000000, episode_reward=6083.20 +/- 67.89
Episode length: 456.20 +/- 18.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 6.08e+03    |
| time/                   |             |
|    total_timesteps      | 61000000    |
| train/                  |             |
|    approx_kl            | 0.019613704 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.3        |
|    n_updates            | 18610       |
|    policy_gradient_loss | -0.0211     |
|    std                  | 9.39        |
|    value_loss           | 0.0151      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1870        |
|    time_elapsed         | 8442        |
|    total_timesteps      | 61276160    |
| train/                  |             |
|    approx_kl            | 0.017199576 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18690       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 9.51        |
|    value_loss           | 0.0129      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | 5.07e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 1880       |
|    time_elapsed         | 8487       |
|    total_timesteps      | 61603840   |
| train/                  |            |
|    approx_kl            | 0.01882249 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.303     |
|    n_updates            | 18790      |
|    policy_gradient_loss | -0.0205    |
|    std                  | 9.66       |
|    value_loss           | 0.0121     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 407        |
|    ep_rew_mean          | 5.24e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 1890       |
|    time_elapsed         | 8531       |
|    total_timesteps      | 61931520   |
| train/                  |            |
|    approx_kl            | 0.01830792 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.303     |
|    n_updates            | 18890      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 9.77       |
|    value_loss           | 0.0103     |
----------------------------------------
Eval num_timesteps=62000000, episode_reward=5361.54 +/- 1388.00
Episode length: 415.40 +/- 50.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 415         |
|    mean_reward          | 5.36e+03    |
| time/                   |             |
|    total_timesteps      | 62000000    |
| train/                  |             |
|    approx_kl            | 0.016916446 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18920       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 9.83        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | 5.34e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1900        |
|    time_elapsed         | 8577        |
|    total_timesteps      | 62259200    |
| train/                  |             |
|    approx_kl            | 0.016939014 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 18990       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 9.96        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 435         |
|    ep_rew_mean          | 5.58e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1910        |
|    time_elapsed         | 8622        |
|    total_timesteps      | 62586880    |
| train/                  |             |
|    approx_kl            | 0.017122135 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.302      |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 10.1        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 410         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 1920        |
|    time_elapsed         | 8666        |
|    total_timesteps      | 62914560    |
| train/                  |             |
|    approx_kl            | 0.016799834 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.3       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 10.3        |
|    value_loss           | 0.00928     |
-----------------------------------------
Eval num_timesteps=63000000, episode_reward=6014.45 +/- 178.49
Episode length: 447.40 +/- 23.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 447         |
|    mean_reward          | 6.01e+03    |
| time/                   |             |
|    total_timesteps      | 63000000    |
| train/                  |             |
|    approx_kl            | 0.017309876 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.4       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 19220       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 10.3        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 407         |
|    ep_rew_mean          | 5.27e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1930        |
|    time_elapsed         | 8712        |
|    total_timesteps      | 63242240    |
| train/                  |             |
|    approx_kl            | 0.016878678 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 19290       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 10.5        |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | 5.51e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 1940        |
|    time_elapsed         | 8757        |
|    total_timesteps      | 63569920    |
| train/                  |             |
|    approx_kl            | 0.016572013 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19390       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 10.6        |
|    value_loss           | 0.0091      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 1950        |
|    time_elapsed         | 8801        |
|    total_timesteps      | 63897600    |
| train/                  |             |
|    approx_kl            | 0.017150791 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 19490       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 10.8        |
|    value_loss           | 0.011       |
-----------------------------------------
Eval num_timesteps=64000000, episode_reward=6095.46 +/- 343.32
Episode length: 461.40 +/- 20.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 6.1e+03     |
| time/                   |             |
|    total_timesteps      | 64000000    |
| train/                  |             |
|    approx_kl            | 0.017232552 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19530       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 10.9        |
|    value_loss           | 0.0142      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 5.39e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1960        |
|    time_elapsed         | 8847        |
|    total_timesteps      | 64225280    |
| train/                  |             |
|    approx_kl            | 0.019352729 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.305      |
|    n_updates            | 19590       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 11          |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.16e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 1970        |
|    time_elapsed         | 8892        |
|    total_timesteps      | 64552960    |
| train/                  |             |
|    approx_kl            | 0.019251324 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19690       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 11.1        |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 5.39e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 1980        |
|    time_elapsed         | 8937        |
|    total_timesteps      | 64880640    |
| train/                  |             |
|    approx_kl            | 0.017061004 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19790       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 11.3        |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=65000000, episode_reward=5247.23 +/- 1420.60
Episode length: 428.00 +/- 57.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 5.25e+03    |
| time/                   |             |
|    total_timesteps      | 65000000    |
| train/                  |             |
|    approx_kl            | 0.016918452 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19830       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 11.4        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 1990        |
|    time_elapsed         | 8983        |
|    total_timesteps      | 65208320    |
| train/                  |             |
|    approx_kl            | 0.016969634 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.311      |
|    n_updates            | 19890       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 11.5        |
|    value_loss           | 0.00981     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 2000        |
|    time_elapsed         | 9028        |
|    total_timesteps      | 65536000    |
| train/                  |             |
|    approx_kl            | 0.017929006 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19990       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 11.6        |
|    value_loss           | 0.0159      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 419         |
|    ep_rew_mean          | 5.44e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 2010        |
|    time_elapsed         | 9074        |
|    total_timesteps      | 65863680    |
| train/                  |             |
|    approx_kl            | 0.017809657 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.313      |
|    n_updates            | 20090       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 11.9        |
|    value_loss           | 0.00959     |
-----------------------------------------
Eval num_timesteps=66000000, episode_reward=6048.83 +/- 272.01
Episode length: 471.20 +/- 23.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 471        |
|    mean_reward          | 6.05e+03   |
| time/                   |            |
|    total_timesteps      | 66000000   |
| train/                  |            |
|    approx_kl            | 0.01788121 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.312     |
|    n_updates            | 20140      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 12         |
|    value_loss           | 0.0105     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.03e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 2020        |
|    time_elapsed         | 9120        |
|    total_timesteps      | 66191360    |
| train/                  |             |
|    approx_kl            | 0.018064544 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.312      |
|    n_updates            | 20190       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 12.2        |
|    value_loss           | 0.0179      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 412        |
|    ep_rew_mean          | 5.33e+03   |
| time/                   |            |
|    fps                  | 7257       |
|    iterations           | 2030       |
|    time_elapsed         | 9165       |
|    total_timesteps      | 66519040   |
| train/                  |            |
|    approx_kl            | 0.01650403 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56.3      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.313     |
|    n_updates            | 20290      |
|    policy_gradient_loss | -0.0184    |
|    std                  | 12.4       |
|    value_loss           | 0.0106     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 2040        |
|    time_elapsed         | 9209        |
|    total_timesteps      | 66846720    |
| train/                  |             |
|    approx_kl            | 0.016428001 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.313      |
|    n_updates            | 20390       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 12.6        |
|    value_loss           | 0.0124      |
-----------------------------------------
Eval num_timesteps=67000000, episode_reward=5787.94 +/- 1146.70
Episode length: 428.00 +/- 81.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 5.79e+03    |
| time/                   |             |
|    total_timesteps      | 67000000    |
| train/                  |             |
|    approx_kl            | 0.017478492 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.6       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 20440       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 12.7        |
|    value_loss           | 0.0146      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 5.42e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 2050        |
|    time_elapsed         | 9255        |
|    total_timesteps      | 67174400    |
| train/                  |             |
|    approx_kl            | 0.017585857 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 20490       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 12.8        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 395         |
|    ep_rew_mean          | 5.15e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 2060        |
|    time_elapsed         | 9300        |
|    total_timesteps      | 67502080    |
| train/                  |             |
|    approx_kl            | 0.017527051 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 20590       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 13          |
|    value_loss           | 0.0172      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 413        |
|    ep_rew_mean          | 5.23e+03   |
| time/                   |            |
|    fps                  | 7258       |
|    iterations           | 2070       |
|    time_elapsed         | 9344       |
|    total_timesteps      | 67829760   |
| train/                  |            |
|    approx_kl            | 0.01814153 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20690      |
|    policy_gradient_loss | -0.0196    |
|    std                  | 13.3       |
|    value_loss           | 0.0158     |
----------------------------------------
Eval num_timesteps=68000000, episode_reward=6381.57 +/- 204.95
Episode length: 466.00 +/- 17.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 6.38e+03    |
| time/                   |             |
|    total_timesteps      | 68000000    |
| train/                  |             |
|    approx_kl            | 0.017667893 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 20750       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 13.4        |
|    value_loss           | 0.0118      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 399        |
|    ep_rew_mean          | 5.14e+03   |
| time/                   |            |
|    fps                  | 7257       |
|    iterations           | 2080       |
|    time_elapsed         | 9390       |
|    total_timesteps      | 68157440   |
| train/                  |            |
|    approx_kl            | 0.01748603 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20790      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 13.5       |
|    value_loss           | 0.0135     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 395         |
|    ep_rew_mean          | 5.09e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 2090        |
|    time_elapsed         | 9435        |
|    total_timesteps      | 68485120    |
| train/                  |             |
|    approx_kl            | 0.016964283 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.5       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.318      |
|    n_updates            | 20890       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 13.8        |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 389         |
|    ep_rew_mean          | 4.93e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2100        |
|    time_elapsed         | 9479        |
|    total_timesteps      | 68812800    |
| train/                  |             |
|    approx_kl            | 0.017287176 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.7       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.319      |
|    n_updates            | 20990       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 14          |
|    value_loss           | 0.016       |
-----------------------------------------
Eval num_timesteps=69000000, episode_reward=6469.11 +/- 446.52
Episode length: 483.40 +/- 28.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 483         |
|    mean_reward          | 6.47e+03    |
| time/                   |             |
|    total_timesteps      | 69000000    |
| train/                  |             |
|    approx_kl            | 0.017137457 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.321      |
|    n_updates            | 21050       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 14.1        |
|    value_loss           | 0.0114      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 428         |
|    ep_rew_mean          | 5.4e+03     |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 2110        |
|    time_elapsed         | 9525        |
|    total_timesteps      | 69140480    |
| train/                  |             |
|    approx_kl            | 0.017461319 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.8       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21090       |
|    policy_gradient_loss | -0.02       |
|    std                  | 14.2        |
|    value_loss           | 0.014       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 406         |
|    ep_rew_mean          | 5.27e+03    |
| time/                   |             |
|    fps                  | 7258        |
|    iterations           | 2120        |
|    time_elapsed         | 9570        |
|    total_timesteps      | 69468160    |
| train/                  |             |
|    approx_kl            | 0.016738253 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58         |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21190       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 14.4        |
|    value_loss           | 0.0101      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 400         |
|    ep_rew_mean          | 5.14e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2130        |
|    time_elapsed         | 9614        |
|    total_timesteps      | 69795840    |
| train/                  |             |
|    approx_kl            | 0.018531244 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21290       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 14.8        |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=70000000, episode_reward=6325.22 +/- 229.74
Episode length: 470.60 +/- 10.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 471         |
|    mean_reward          | 6.33e+03    |
| time/                   |             |
|    total_timesteps      | 70000000    |
| train/                  |             |
|    approx_kl            | 0.018012483 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.323      |
|    n_updates            | 21360       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 15          |
|    value_loss           | 0.0153      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 428       |
|    ep_rew_mean          | 5.44e+03  |
| time/                   |           |
|    fps                  | 7258      |
|    iterations           | 2140      |
|    time_elapsed         | 9660      |
|    total_timesteps      | 70123520  |
| train/                  |           |
|    approx_kl            | 0.0180853 |
|    clip_fraction        | 0.135     |
|    clip_range           | 0.2       |
|    entropy_loss         | -58.3     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.326    |
|    n_updates            | 21390     |
|    policy_gradient_loss | -0.0202   |
|    std                  | 15        |
|    value_loss           | 0.0125    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2150        |
|    time_elapsed         | 9705        |
|    total_timesteps      | 70451200    |
| train/                  |             |
|    approx_kl            | 0.018545825 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21490       |
|    policy_gradient_loss | -0.02       |
|    std                  | 15.3        |
|    value_loss           | 0.0147      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 425         |
|    ep_rew_mean          | 5.57e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2160        |
|    time_elapsed         | 9749        |
|    total_timesteps      | 70778880    |
| train/                  |             |
|    approx_kl            | 0.016428076 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.7       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21590       |
|    policy_gradient_loss | -0.019      |
|    std                  | 15.6        |
|    value_loss           | 0.0129      |
-----------------------------------------
Eval num_timesteps=71000000, episode_reward=6672.60 +/- 353.55
Episode length: 487.40 +/- 8.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 487        |
|    mean_reward          | 6.67e+03   |
| time/                   |            |
|    total_timesteps      | 71000000   |
| train/                  |            |
|    approx_kl            | 0.01652599 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -58.8      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.325     |
|    n_updates            | 21660      |
|    policy_gradient_loss | -0.0198    |
|    std                  | 15.8       |
|    value_loss           | 0.0148     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 417         |
|    ep_rew_mean          | 5.38e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2170        |
|    time_elapsed         | 9795        |
|    total_timesteps      | 71106560    |
| train/                  |             |
|    approx_kl            | 0.018294686 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21690       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 15.9        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 5.5e+03     |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 2180        |
|    time_elapsed         | 9839        |
|    total_timesteps      | 71434240    |
| train/                  |             |
|    approx_kl            | 0.019414939 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59         |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21790       |
|    policy_gradient_loss | -0.0204     |
|    std                  | 16.2        |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | 5.53e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2190        |
|    time_elapsed         | 9884        |
|    total_timesteps      | 71761920    |
| train/                  |             |
|    approx_kl            | 0.016194552 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.2       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21890       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 16.6        |
|    value_loss           | 0.0148      |
-----------------------------------------
Eval num_timesteps=72000000, episode_reward=5653.06 +/- 1922.72
Episode length: 435.00 +/- 116.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 435        |
|    mean_reward          | 5.65e+03   |
| time/                   |            |
|    total_timesteps      | 72000000   |
| train/                  |            |
|    approx_kl            | 0.01771386 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.331     |
|    n_updates            | 21970      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 16.8       |
|    value_loss           | 0.012      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 409        |
|    ep_rew_mean          | 5.35e+03   |
| time/                   |            |
|    fps                  | 7259       |
|    iterations           | 2200       |
|    time_elapsed         | 9930       |
|    total_timesteps      | 72089600   |
| train/                  |            |
|    approx_kl            | 0.01846053 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.4      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.328     |
|    n_updates            | 21990      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 16.8       |
|    value_loss           | 0.0143     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 5.38e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2210        |
|    time_elapsed         | 9974        |
|    total_timesteps      | 72417280    |
| train/                  |             |
|    approx_kl            | 0.017427765 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.5       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 22090       |
|    policy_gradient_loss | -0.019      |
|    std                  | 17.1        |
|    value_loss           | 0.0147      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 409        |
|    ep_rew_mean          | 5.33e+03   |
| time/                   |            |
|    fps                  | 7260       |
|    iterations           | 2220       |
|    time_elapsed         | 10019      |
|    total_timesteps      | 72744960   |
| train/                  |            |
|    approx_kl            | 0.01754508 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.7      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.331     |
|    n_updates            | 22190      |
|    policy_gradient_loss | -0.0195    |
|    std                  | 17.4       |
|    value_loss           | 0.0117     |
----------------------------------------
Eval num_timesteps=73000000, episode_reward=5538.62 +/- 2182.84
Episode length: 410.80 +/- 120.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 411         |
|    mean_reward          | 5.54e+03    |
| time/                   |             |
|    total_timesteps      | 73000000    |
| train/                  |             |
|    approx_kl            | 0.017594643 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.33       |
|    n_updates            | 22270       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 17.7        |
|    value_loss           | 0.0181      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 429         |
|    ep_rew_mean          | 5.62e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2230        |
|    time_elapsed         | 10064       |
|    total_timesteps      | 73072640    |
| train/                  |             |
|    approx_kl            | 0.018000089 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.332      |
|    n_updates            | 22290       |
|    policy_gradient_loss | -0.02       |
|    std                  | 17.7        |
|    value_loss           | 0.0177      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2240        |
|    time_elapsed         | 10109       |
|    total_timesteps      | 73400320    |
| train/                  |             |
|    approx_kl            | 0.017558657 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.02       |
|    std                  | 18          |
|    value_loss           | 0.0134      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 383         |
|    ep_rew_mean          | 4.95e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2250        |
|    time_elapsed         | 10153       |
|    total_timesteps      | 73728000    |
| train/                  |             |
|    approx_kl            | 0.018159604 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.3       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22490       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 18.4        |
|    value_loss           | 0.0158      |
-----------------------------------------
Eval num_timesteps=74000000, episode_reward=6555.80 +/- 592.19
Episode length: 487.40 +/- 23.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 487         |
|    mean_reward          | 6.56e+03    |
| time/                   |             |
|    total_timesteps      | 74000000    |
| train/                  |             |
|    approx_kl            | 0.017717436 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.3       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22580       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 18.6        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 418         |
|    ep_rew_mean          | 5.45e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2260        |
|    time_elapsed         | 10199       |
|    total_timesteps      | 74055680    |
| train/                  |             |
|    approx_kl            | 0.018116694 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.334      |
|    n_updates            | 22590       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 18.6        |
|    value_loss           | 0.0145      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | 5.61e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2270        |
|    time_elapsed         | 10244       |
|    total_timesteps      | 74383360    |
| train/                  |             |
|    approx_kl            | 0.017068455 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.336      |
|    n_updates            | 22690       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 18.9        |
|    value_loss           | 0.0148      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 412         |
|    ep_rew_mean          | 5.4e+03     |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2280        |
|    time_elapsed         | 10288       |
|    total_timesteps      | 74711040    |
| train/                  |             |
|    approx_kl            | 0.018497646 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.7       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.336      |
|    n_updates            | 22790       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 19.2        |
|    value_loss           | 0.0146      |
-----------------------------------------
Eval num_timesteps=75000000, episode_reward=6939.96 +/- 631.76
Episode length: 501.60 +/- 32.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 502         |
|    mean_reward          | 6.94e+03    |
| time/                   |             |
|    total_timesteps      | 75000000    |
| train/                  |             |
|    approx_kl            | 0.017485213 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.338      |
|    n_updates            | 22880       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 19.5        |
|    value_loss           | 0.0126      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 418         |
|    ep_rew_mean          | 5.41e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 2290        |
|    time_elapsed         | 10334       |
|    total_timesteps      | 75038720    |
| train/                  |             |
|    approx_kl            | 0.018048396 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.337      |
|    n_updates            | 22890       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 19.5        |
|    value_loss           | 0.0139      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 419         |
|    ep_rew_mean          | 5.47e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2300        |
|    time_elapsed         | 10379       |
|    total_timesteps      | 75366400    |
| train/                  |             |
|    approx_kl            | 0.018700326 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.1       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 22990       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 19.8        |
|    value_loss           | 0.0153      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 431         |
|    ep_rew_mean          | 5.66e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2310        |
|    time_elapsed         | 10423       |
|    total_timesteps      | 75694080    |
| train/                  |             |
|    approx_kl            | 0.018343596 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 20.2        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=76000000, episode_reward=6367.91 +/- 689.33
Episode length: 479.80 +/- 24.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 480        |
|    mean_reward          | 6.37e+03   |
| time/                   |            |
|    total_timesteps      | 76000000   |
| train/                  |            |
|    approx_kl            | 0.01683681 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -61.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.338     |
|    n_updates            | 23190      |
|    policy_gradient_loss | -0.0194    |
|    std                  | 20.4       |
|    value_loss           | 0.0146     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | 5.56e+03 |
| time/              |          |
|    fps             | 7261     |
|    iterations      | 2320     |
|    time_elapsed    | 10469    |
|    total_timesteps | 76021760 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 435         |
|    ep_rew_mean          | 5.65e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2330        |
|    time_elapsed         | 10514       |
|    total_timesteps      | 76349440    |
| train/                  |             |
|    approx_kl            | 0.016960472 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.5       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.339      |
|    n_updates            | 23290       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 20.8        |
|    value_loss           | 0.0177      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 432         |
|    ep_rew_mean          | 5.68e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2340        |
|    time_elapsed         | 10559       |
|    total_timesteps      | 76677120    |
| train/                  |             |
|    approx_kl            | 0.018084388 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.343      |
|    n_updates            | 23390       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 21          |
|    value_loss           | 0.0145      |
-----------------------------------------
Eval num_timesteps=77000000, episode_reward=7335.73 +/- 534.62
Episode length: 506.20 +/- 21.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 506         |
|    mean_reward          | 7.34e+03    |
| time/                   |             |
|    total_timesteps      | 77000000    |
| train/                  |             |
|    approx_kl            | 0.019147094 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.9       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 23490       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 21.4        |
|    value_loss           | 0.0189      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 417      |
|    ep_rew_mean     | 5.26e+03 |
| time/              |          |
|    fps             | 7260     |
|    iterations      | 2350     |
|    time_elapsed    | 10605    |
|    total_timesteps | 77004800 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 443         |
|    ep_rew_mean          | 5.81e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2360        |
|    time_elapsed         | 10649       |
|    total_timesteps      | 77332480    |
| train/                  |             |
|    approx_kl            | 0.017580638 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62         |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.342      |
|    n_updates            | 23590       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 21.7        |
|    value_loss           | 0.0165      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 424        |
|    ep_rew_mean          | 5.51e+03   |
| time/                   |            |
|    fps                  | 7261       |
|    iterations           | 2370       |
|    time_elapsed         | 10694      |
|    total_timesteps      | 77660160   |
| train/                  |            |
|    approx_kl            | 0.01910019 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -62.2      |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.342     |
|    n_updates            | 23690      |
|    policy_gradient_loss | -0.0204    |
|    std                  | 22.1       |
|    value_loss           | 0.0177     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | 5.47e+03   |
| time/                   |            |
|    fps                  | 7262       |
|    iterations           | 2380       |
|    time_elapsed         | 10738      |
|    total_timesteps      | 77987840   |
| train/                  |            |
|    approx_kl            | 0.01676868 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -62.4      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.341     |
|    n_updates            | 23790      |
|    policy_gradient_loss | -0.0189    |
|    std                  | 22.4       |
|    value_loss           | 0.0226     |
----------------------------------------
Eval num_timesteps=78000000, episode_reward=7296.15 +/- 625.21
Episode length: 503.60 +/- 23.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | 7.3e+03     |
| time/                   |             |
|    total_timesteps      | 78000000    |
| train/                  |             |
|    approx_kl            | 0.017032815 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.4       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.345      |
|    n_updates            | 23800       |
|    policy_gradient_loss | -0.02       |
|    std                  | 22.4        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 413         |
|    ep_rew_mean          | 5.32e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 2390        |
|    time_elapsed         | 10784       |
|    total_timesteps      | 78315520    |
| train/                  |             |
|    approx_kl            | 0.017889485 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.6       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.345      |
|    n_updates            | 23890       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 22.8        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 412         |
|    ep_rew_mean          | 5.28e+03    |
| time/                   |             |
|    fps                  | 7262        |
|    iterations           | 2400        |
|    time_elapsed         | 10828       |
|    total_timesteps      | 78643200    |
| train/                  |             |
|    approx_kl            | 0.017447669 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.8       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.346      |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 23.1        |
|    value_loss           | 0.0205      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | 5.52e+03    |
| time/                   |             |
|    fps                  | 7262        |
|    iterations           | 2410        |
|    time_elapsed         | 10873       |
|    total_timesteps      | 78970880    |
| train/                  |             |
|    approx_kl            | 0.016651895 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63         |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.346      |
|    n_updates            | 24090       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 23.5        |
|    value_loss           | 0.0219      |
-----------------------------------------
Eval num_timesteps=79000000, episode_reward=6995.14 +/- 1000.44
Episode length: 526.20 +/- 44.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 526        |
|    mean_reward          | 7e+03      |
| time/                   |            |
|    total_timesteps      | 79000000   |
| train/                  |            |
|    approx_kl            | 0.01828268 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -63        |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.347     |
|    n_updates            | 24100      |
|    policy_gradient_loss | -0.0204    |
|    std                  | 23.6       |
|    value_loss           | 0.019      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 400        |
|    ep_rew_mean          | 5.2e+03    |
| time/                   |            |
|    fps                  | 7262       |
|    iterations           | 2420       |
|    time_elapsed         | 10919      |
|    total_timesteps      | 79298560   |
| train/                  |            |
|    approx_kl            | 0.01756845 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -63.2      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.348     |
|    n_updates            | 24190      |
|    policy_gradient_loss | -0.0204    |
|    std                  | 24         |
|    value_loss           | 0.0246     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 450         |
|    ep_rew_mean          | 5.96e+03    |
| time/                   |             |
|    fps                  | 7262        |
|    iterations           | 2430        |
|    time_elapsed         | 10963       |
|    total_timesteps      | 79626240    |
| train/                  |             |
|    approx_kl            | 0.017627133 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.3       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.348      |
|    n_updates            | 24290       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 24.5        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | 5.72e+03    |
| time/                   |             |
|    fps                  | 7263        |
|    iterations           | 2440        |
|    time_elapsed         | 11008       |
|    total_timesteps      | 79953920    |
| train/                  |             |
|    approx_kl            | 0.017933572 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.5       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.349      |
|    n_updates            | 24390       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 24.9        |
|    value_loss           | 0.0208      |
-----------------------------------------
Eval num_timesteps=80000000, episode_reward=7841.35 +/- 110.74
Episode length: 544.00 +/- 30.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 544         |
|    mean_reward          | 7.84e+03    |
| time/                   |             |
|    total_timesteps      | 80000000    |
| train/                  |             |
|    approx_kl            | 0.018552389 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.5       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.349      |
|    n_updates            | 24410       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 25          |
|    value_loss           | 0.0248      |
-----------------------------------------
New best mean reward!

============================================================
Training Complete!
============================================================
Total training time: 3:03:38
Total timesteps: 80,000,000
Timesteps per second: 7260.37
Run directory: /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/14-37-50
============================================================

