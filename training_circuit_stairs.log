
=== Merged Config ===
 exp_name: rl-humanoid
seed: 42
resume_from: null
paths:
  log_root: null
vecnorm:
  enabled: true
  clip_obs: 10.0
  gamma: 0.99
  epsilon: 1.0e-08
env:
  name: HumanoidCircuit-v0
  make_kwargs:
    render_mode: null
    waypoints:
    - - 8.0
      - 0.0
    - - 15.0
      - 0.0
    - - 20.0
      - 5.0
    waypoint_reach_threshold: 1.5
    stairs:
    - - 9.0
      - 5
      - 0.15
      - 0.6
    terrain_width: 8.0
    progress_reward_weight: 200.0
    waypoint_bonus: 150.0
    circuit_completion_bonus: 500.0
    height_reward_weight: 2.0
    forward_reward_weight: 1.0
    heading_reward_weight: 2.0
    balance_reward_weight: 0.5
    optimal_speed: 1.0
    speed_regulation_weight: 0.2
    ctrl_cost_weight: 0.1
    contact_cost_weight: 5.0e-07
    healthy_reward: 5.0
    terminate_when_unhealthy: true
    healthy_z_range:
    - 0.8
    - 3.0
  vec_env:
    n_envs: 8
    start_method: spawn
    monitor: true
algo:
  policy: MlpPolicy
  device: cuda
  policy_kwargs:
    net_arch:
    - 256
    - 256
  hyperparams:
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 16384
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.005
    vf_coef: 0.25
    max_grad_norm: 0.5
training:
  total_timesteps: 80000000
  log_interval: 10
  checkpoint_every_steps: 250000

Logging to /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/17-59-51
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Using cuda device

=== Starting Training ===
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37          |
|    ep_rew_mean          | 214         |
| time/                   |             |
|    fps                  | 5582        |
|    iterations           | 10          |
|    time_elapsed         | 58          |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.010995706 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.2       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0848     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0137     |
|    std                  | 1.01        |
|    value_loss           | 0.262       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 53.8        |
|    ep_rew_mean          | 325         |
| time/                   |             |
|    fps                  | 5662        |
|    iterations           | 20          |
|    time_elapsed         | 115         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.010284725 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.919       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.118      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0126     |
|    std                  | 0.999       |
|    value_loss           | 0.107       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 65.4        |
|    ep_rew_mean          | 381         |
| time/                   |             |
|    fps                  | 5677        |
|    iterations           | 30          |
|    time_elapsed         | 173         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.010270448 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.127      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0127     |
|    std                  | 0.995       |
|    value_loss           | 0.0765      |
-----------------------------------------
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000000, episode_reward=554.82 +/- 46.22
Episode length: 75.00 +/- 6.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75          |
|    mean_reward          | 555         |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.009670297 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.013      |
|    std                  | 0.996       |
|    value_loss           | 0.0763      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 77.5        |
|    ep_rew_mean          | 448         |
| time/                   |             |
|    fps                  | 5677        |
|    iterations           | 40          |
|    time_elapsed         | 230         |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.009654568 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0129     |
|    std                  | 0.995       |
|    value_loss           | 0.0597      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 81.9        |
|    ep_rew_mean          | 472         |
| time/                   |             |
|    fps                  | 5682        |
|    iterations           | 50          |
|    time_elapsed         | 288         |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.010991527 |
|    clip_fraction        | 0.0818      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0139     |
|    std                  | 0.993       |
|    value_loss           | 0.0522      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 84.3        |
|    ep_rew_mean          | 494         |
| time/                   |             |
|    fps                  | 5685        |
|    iterations           | 60          |
|    time_elapsed         | 345         |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.011325661 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0139     |
|    std                  | 0.992       |
|    value_loss           | 0.0702      |
-----------------------------------------
Eval num_timesteps=2000000, episode_reward=677.52 +/- 64.00
Episode length: 98.60 +/- 9.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.6        |
|    mean_reward          | 678         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.011465096 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0139     |
|    std                  | 0.992       |
|    value_loss           | 0.0582      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 86.7        |
|    ep_rew_mean          | 513         |
| time/                   |             |
|    fps                  | 5684        |
|    iterations           | 70          |
|    time_elapsed         | 403         |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.011530351 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.014      |
|    std                  | 0.988       |
|    value_loss           | 0.059       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.8        |
|    ep_rew_mean          | 551         |
| time/                   |             |
|    fps                  | 5684        |
|    iterations           | 80          |
|    time_elapsed         | 461         |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.011121206 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.7       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0142     |
|    std                  | 0.985       |
|    value_loss           | 0.0631      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.1        |
|    ep_rew_mean          | 566         |
| time/                   |             |
|    fps                  | 5682        |
|    iterations           | 90          |
|    time_elapsed         | 518         |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.012348896 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0138     |
|    std                  | 0.994       |
|    value_loss           | 0.0706      |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=587.01 +/- 258.27
Episode length: 79.40 +/- 26.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.4        |
|    mean_reward          | 587         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.011986579 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0139     |
|    std                  | 0.997       |
|    value_loss           | 0.0677      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 94.8       |
|    ep_rew_mean          | 615        |
| time/                   |            |
|    fps                  | 5676       |
|    iterations           | 100        |
|    time_elapsed         | 577        |
|    total_timesteps      | 3276800    |
| train/                  |            |
|    approx_kl            | 0.01163401 |
|    clip_fraction        | 0.0997     |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.9      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.129     |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0142    |
|    std                  | 0.997      |
|    value_loss           | 0.0729     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 722         |
| time/                   |             |
|    fps                  | 5675        |
|    iterations           | 110         |
|    time_elapsed         | 635         |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.012604063 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 1           |
|    value_loss           | 0.082       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 742         |
| time/                   |             |
|    fps                  | 5676        |
|    iterations           | 120         |
|    time_elapsed         | 692         |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.013933794 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0149     |
|    std                  | 0.998       |
|    value_loss           | 0.067       |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=783.26 +/- 123.62
Episode length: 95.00 +/- 9.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95          |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.012430242 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 0.998       |
|    value_loss           | 0.0719      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | 776        |
| time/                   |            |
|    fps                  | 5672       |
|    iterations           | 130        |
|    time_elapsed         | 750        |
|    total_timesteps      | 4259840    |
| train/                  |            |
|    approx_kl            | 0.01215384 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.9      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.127     |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0147    |
|    std                  | 0.998      |
|    value_loss           | 0.0747     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 861         |
| time/                   |             |
|    fps                  | 5671        |
|    iterations           | 140         |
|    time_elapsed         | 808         |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.012536411 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0145     |
|    std                  | 1           |
|    value_loss           | 0.0703      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 914         |
| time/                   |             |
|    fps                  | 5671        |
|    iterations           | 150         |
|    time_elapsed         | 866         |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.013281777 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0159     |
|    std                  | 1.01        |
|    value_loss           | 0.0632      |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=895.85 +/- 82.32
Episode length: 97.60 +/- 7.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 896         |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.013408698 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0154     |
|    std                  | 1.01        |
|    value_loss           | 0.0634      |
-----------------------------------------
New best mean reward!
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | 989          |
| time/                   |              |
|    fps                  | 5669         |
|    iterations           | 160          |
|    time_elapsed         | 924          |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0143553335 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    entropy_loss         | -24          |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.133       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.0157      |
|    std                  | 1.01         |
|    value_loss           | 0.0613       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 1.03e+03    |
| time/                   |             |
|    fps                  | 5670        |
|    iterations           | 170         |
|    time_elapsed         | 982         |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.014109924 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.136      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0162     |
|    std                  | 1.02        |
|    value_loss           | 0.0621      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 1.11e+03    |
| time/                   |             |
|    fps                  | 5668        |
|    iterations           | 180         |
|    time_elapsed         | 1040        |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.013998983 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0169     |
|    std                  | 1.02        |
|    value_loss           | 0.0567      |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=1392.00 +/- 404.89
Episode length: 133.80 +/- 29.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 1.39e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.014660545 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.136      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0161     |
|    std                  | 1.02        |
|    value_loss           | 0.0601      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | 1.19e+03   |
| time/                   |            |
|    fps                  | 5670       |
|    iterations           | 190        |
|    time_elapsed         | 1097       |
|    total_timesteps      | 6225920    |
| train/                  |            |
|    approx_kl            | 0.01455198 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.136     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0162    |
|    std                  | 1.03       |
|    value_loss           | 0.0613     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 1.2e+03     |
| time/                   |             |
|    fps                  | 5673        |
|    iterations           | 200         |
|    time_elapsed         | 1155        |
|    total_timesteps      | 6553600     |
| train/                  |             |
|    approx_kl            | 0.014588848 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.2       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.017      |
|    std                  | 1.03        |
|    value_loss           | 0.0585      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 139         |
|    ep_rew_mean          | 1.39e+03    |
| time/                   |             |
|    fps                  | 5675        |
|    iterations           | 210         |
|    time_elapsed         | 1212        |
|    total_timesteps      | 6881280     |
| train/                  |             |
|    approx_kl            | 0.013901589 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1.04        |
|    value_loss           | 0.0552      |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=1402.43 +/- 301.03
Episode length: 131.40 +/- 15.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 1.4e+03     |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.015297353 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 1.04        |
|    value_loss           | 0.0593      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 1.47e+03    |
| time/                   |             |
|    fps                  | 5675        |
|    iterations           | 220         |
|    time_elapsed         | 1270        |
|    total_timesteps      | 7208960     |
| train/                  |             |
|    approx_kl            | 0.015459402 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 1.04        |
|    value_loss           | 0.056       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 139         |
|    ep_rew_mean          | 1.44e+03    |
| time/                   |             |
|    fps                  | 5679        |
|    iterations           | 230         |
|    time_elapsed         | 1326        |
|    total_timesteps      | 7536640     |
| train/                  |             |
|    approx_kl            | 0.015633075 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.142      |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0178     |
|    std                  | 1.05        |
|    value_loss           | 0.0525      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 5682        |
|    iterations           | 240         |
|    time_elapsed         | 1383        |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.015049118 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.141      |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.05        |
|    value_loss           | 0.0592      |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2192.37 +/- 461.57
Episode length: 191.00 +/- 48.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 2.19e+03    |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.015863262 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.142      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 1.05        |
|    value_loss           | 0.0564      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 1.65e+03    |
| time/                   |             |
|    fps                  | 5682        |
|    iterations           | 250         |
|    time_elapsed         | 1441        |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.015980009 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.06        |
|    value_loss           | 0.0572      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 163         |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 5685        |
|    iterations           | 260         |
|    time_elapsed         | 1498        |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.015467759 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.07        |
|    value_loss           | 0.0554      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 1.94e+03    |
| time/                   |             |
|    fps                  | 5688        |
|    iterations           | 270         |
|    time_elapsed         | 1555        |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.015643455 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.141      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 1.08        |
|    value_loss           | 0.0603      |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2233.96 +/- 498.77
Episode length: 223.20 +/- 67.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 2.23e+03    |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.014685765 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.8       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.08        |
|    value_loss           | 0.0589      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 185         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 5688        |
|    iterations           | 280         |
|    time_elapsed         | 1612        |
|    total_timesteps      | 9175040     |
| train/                  |             |
|    approx_kl            | 0.015806317 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.146      |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.09        |
|    value_loss           | 0.0532      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 2.17e+03   |
| time/                   |            |
|    fps                  | 5691       |
|    iterations           | 290        |
|    time_elapsed         | 1669       |
|    total_timesteps      | 9502720    |
| train/                  |            |
|    approx_kl            | 0.01668309 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.9      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.146     |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.0195    |
|    std                  | 1.09       |
|    value_loss           | 0.0583     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 2.4e+03     |
| time/                   |             |
|    fps                  | 5693        |
|    iterations           | 300         |
|    time_elapsed         | 1726        |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.016023891 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.1         |
|    value_loss           | 0.057       |
-----------------------------------------
Eval num_timesteps=10000000, episode_reward=4591.08 +/- 878.37
Episode length: 425.20 +/- 93.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 425         |
|    mean_reward          | 4.59e+03    |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.017999765 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.147      |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.11        |
|    value_loss           | 0.0603      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 234         |
|    ep_rew_mean          | 2.66e+03    |
| time/                   |             |
|    fps                  | 5689        |
|    iterations           | 310         |
|    time_elapsed         | 1785        |
|    total_timesteps      | 10158080    |
| train/                  |             |
|    approx_kl            | 0.017495908 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.11        |
|    value_loss           | 0.0628      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 246         |
|    ep_rew_mean          | 2.91e+03    |
| time/                   |             |
|    fps                  | 5691        |
|    iterations           | 320         |
|    time_elapsed         | 1842        |
|    total_timesteps      | 10485760    |
| train/                  |             |
|    approx_kl            | 0.017133415 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.1       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.147      |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.12        |
|    value_loss           | 0.0646      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 246         |
|    ep_rew_mean          | 2.91e+03    |
| time/                   |             |
|    fps                  | 5693        |
|    iterations           | 330         |
|    time_elapsed         | 1899        |
|    total_timesteps      | 10813440    |
| train/                  |             |
|    approx_kl            | 0.017405305 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.149      |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.12        |
|    value_loss           | 0.0553      |
-----------------------------------------
Eval num_timesteps=11000000, episode_reward=5303.97 +/- 1231.47
Episode length: 458.20 +/- 144.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 458         |
|    mean_reward          | 5.3e+03     |
| time/                   |             |
|    total_timesteps      | 11000000    |
| train/                  |             |
|    approx_kl            | 0.016893815 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.149      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.13        |
|    value_loss           | 0.0533      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 269         |
|    ep_rew_mean          | 3.29e+03    |
| time/                   |             |
|    fps                  | 5691        |
|    iterations           | 340         |
|    time_elapsed         | 1957        |
|    total_timesteps      | 11141120    |
| train/                  |             |
|    approx_kl            | 0.016570393 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.15       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.13        |
|    value_loss           | 0.0504      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 304         |
|    ep_rew_mean          | 3.74e+03    |
| time/                   |             |
|    fps                  | 5693        |
|    iterations           | 350         |
|    time_elapsed         | 2014        |
|    total_timesteps      | 11468800    |
| train/                  |             |
|    approx_kl            | 0.017441876 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.4       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.15       |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.14        |
|    value_loss           | 0.0517      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 315         |
|    ep_rew_mean          | 3.97e+03    |
| time/                   |             |
|    fps                  | 5694        |
|    iterations           | 360         |
|    time_elapsed         | 2071        |
|    total_timesteps      | 11796480    |
| train/                  |             |
|    approx_kl            | 0.018141473 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.15        |
|    value_loss           | 0.0463      |
-----------------------------------------
Eval num_timesteps=12000000, episode_reward=6611.49 +/- 346.69
Episode length: 500.80 +/- 51.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 501         |
|    mean_reward          | 6.61e+03    |
| time/                   |             |
|    total_timesteps      | 12000000    |
| train/                  |             |
|    approx_kl            | 0.018183626 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.15        |
|    value_loss           | 0.0462      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 319        |
|    ep_rew_mean          | 4.07e+03   |
| time/                   |            |
|    fps                  | 5691       |
|    iterations           | 370        |
|    time_elapsed         | 2130       |
|    total_timesteps      | 12124160   |
| train/                  |            |
|    approx_kl            | 0.01813705 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.7      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.155     |
|    n_updates            | 3690       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 1.16       |
|    value_loss           | 0.0434     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 311        |
|    ep_rew_mean          | 4.05e+03   |
| time/                   |            |
|    fps                  | 5694       |
|    iterations           | 380        |
|    time_elapsed         | 2186       |
|    total_timesteps      | 12451840   |
| train/                  |            |
|    approx_kl            | 0.01854596 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.8      |
|    explained_variance   | 0.947      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.156     |
|    n_updates            | 3790       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 1.17       |
|    value_loss           | 0.0428     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 346        |
|    ep_rew_mean          | 4.61e+03   |
| time/                   |            |
|    fps                  | 5696       |
|    iterations           | 390        |
|    time_elapsed         | 2243       |
|    total_timesteps      | 12779520   |
| train/                  |            |
|    approx_kl            | 0.01786194 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.9      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.157     |
|    n_updates            | 3890       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.18       |
|    value_loss           | 0.0358     |
----------------------------------------
Eval num_timesteps=13000000, episode_reward=4171.80 +/- 1820.09
Episode length: 304.80 +/- 101.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 4.17e+03    |
| time/                   |             |
|    total_timesteps      | 13000000    |
| train/                  |             |
|    approx_kl            | 0.017254282 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.158      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.18        |
|    value_loss           | 0.0323      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 4.75e+03    |
| time/                   |             |
|    fps                  | 5694        |
|    iterations           | 400         |
|    time_elapsed         | 2301        |
|    total_timesteps      | 13107200    |
| train/                  |             |
|    approx_kl            | 0.018113159 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -26         |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.159      |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.19        |
|    value_loss           | 0.0342      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 355         |
|    ep_rew_mean          | 4.59e+03    |
| time/                   |             |
|    fps                  | 5696        |
|    iterations           | 410         |
|    time_elapsed         | 2358        |
|    total_timesteps      | 13434880    |
| train/                  |             |
|    approx_kl            | 0.017525256 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.2       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.2         |
|    value_loss           | 0.0291      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 5.01e+03    |
| time/                   |             |
|    fps                  | 5697        |
|    iterations           | 420         |
|    time_elapsed         | 2415        |
|    total_timesteps      | 13762560    |
| train/                  |             |
|    approx_kl            | 0.017855747 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.3       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.22        |
|    value_loss           | 0.0312      |
-----------------------------------------
Eval num_timesteps=14000000, episode_reward=6093.95 +/- 745.43
Episode length: 410.40 +/- 66.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 410         |
|    mean_reward          | 6.09e+03    |
| time/                   |             |
|    total_timesteps      | 14000000    |
| train/                  |             |
|    approx_kl            | 0.016710564 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.23        |
|    value_loss           | 0.0303      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 342        |
|    ep_rew_mean          | 4.75e+03   |
| time/                   |            |
|    fps                  | 5695       |
|    iterations           | 430        |
|    time_elapsed         | 2474       |
|    total_timesteps      | 14090240   |
| train/                  |            |
|    approx_kl            | 0.01771182 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.5      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.16      |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 1.23       |
|    value_loss           | 0.0306     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 5.16e+03    |
| time/                   |             |
|    fps                  | 5695        |
|    iterations           | 440         |
|    time_elapsed         | 2531        |
|    total_timesteps      | 14417920    |
| train/                  |             |
|    approx_kl            | 0.017846227 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.7       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.162      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.25        |
|    value_loss           | 0.0259      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 380        |
|    ep_rew_mean          | 5.32e+03   |
| time/                   |            |
|    fps                  | 5697       |
|    iterations           | 450        |
|    time_elapsed         | 2588       |
|    total_timesteps      | 14745600   |
| train/                  |            |
|    approx_kl            | 0.01738758 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.8      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.165     |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 1.26       |
|    value_loss           | 0.0196     |
----------------------------------------
Eval num_timesteps=15000000, episode_reward=5929.68 +/- 215.08
Episode length: 393.00 +/- 32.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 393         |
|    mean_reward          | 5.93e+03    |
| time/                   |             |
|    total_timesteps      | 15000000    |
| train/                  |             |
|    approx_kl            | 0.016966175 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.165      |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 1.27        |
|    value_loss           | 0.0209      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 389         |
|    ep_rew_mean          | 5.27e+03    |
| time/                   |             |
|    fps                  | 5694        |
|    iterations           | 460         |
|    time_elapsed         | 2646        |
|    total_timesteps      | 15073280    |
| train/                  |             |
|    approx_kl            | 0.017621266 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.165      |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.27        |
|    value_loss           | 0.0235      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | 5.25e+03    |
| time/                   |             |
|    fps                  | 5695        |
|    iterations           | 470         |
|    time_elapsed         | 2704        |
|    total_timesteps      | 15400960    |
| train/                  |             |
|    approx_kl            | 0.018137025 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.168      |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.28        |
|    value_loss           | 0.0209      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 378         |
|    ep_rew_mean          | 5.47e+03    |
| time/                   |             |
|    fps                  | 5695        |
|    iterations           | 480         |
|    time_elapsed         | 2761        |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.018411413 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.168      |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.29        |
|    value_loss           | 0.0211      |
-----------------------------------------
Eval num_timesteps=16000000, episode_reward=5978.35 +/- 134.61
Episode length: 376.00 +/- 10.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 376        |
|    mean_reward          | 5.98e+03   |
| time/                   |            |
|    total_timesteps      | 16000000   |
| train/                  |            |
|    approx_kl            | 0.01677842 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.3      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.167     |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 1.31       |
|    value_loss           | 0.016      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 5.14e+03    |
| time/                   |             |
|    fps                  | 5690        |
|    iterations           | 490         |
|    time_elapsed         | 2821        |
|    total_timesteps      | 16056320    |
| train/                  |             |
|    approx_kl            | 0.018677874 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.3       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.166      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 1.31        |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 380         |
|    ep_rew_mean          | 5.45e+03    |
| time/                   |             |
|    fps                  | 5689        |
|    iterations           | 500         |
|    time_elapsed         | 2879        |
|    total_timesteps      | 16384000    |
| train/                  |             |
|    approx_kl            | 0.018801142 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.5       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.17       |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 1.32        |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 347         |
|    ep_rew_mean          | 5.01e+03    |
| time/                   |             |
|    fps                  | 5688        |
|    iterations           | 510         |
|    time_elapsed         | 2937        |
|    total_timesteps      | 16711680    |
| train/                  |             |
|    approx_kl            | 0.018302532 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.172      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.34        |
|    value_loss           | 0.0168      |
-----------------------------------------
Eval num_timesteps=17000000, episode_reward=6013.40 +/- 236.80
Episode length: 382.60 +/- 20.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 383        |
|    mean_reward          | 6.01e+03   |
| time/                   |            |
|    total_timesteps      | 17000000   |
| train/                  |            |
|    approx_kl            | 0.01857318 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.176     |
|    n_updates            | 5180       |
|    policy_gradient_loss | -0.0218    |
|    std                  | 1.35       |
|    value_loss           | 0.0117     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 348         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 5684        |
|    iterations           | 520         |
|    time_elapsed         | 2997        |
|    total_timesteps      | 17039360    |
| train/                  |             |
|    approx_kl            | 0.018372212 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.172      |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.36        |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 5.45e+03    |
| time/                   |             |
|    fps                  | 5681        |
|    iterations           | 530         |
|    time_elapsed         | 3056        |
|    total_timesteps      | 17367040    |
| train/                  |             |
|    approx_kl            | 0.019430272 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.0219     |
|    std                  | 1.37        |
|    value_loss           | 0.0185      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 344         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 5680        |
|    iterations           | 540         |
|    time_elapsed         | 3115        |
|    total_timesteps      | 17694720    |
| train/                  |             |
|    approx_kl            | 0.018072862 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.171      |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.39        |
|    value_loss           | 0.0209      |
-----------------------------------------
Eval num_timesteps=18000000, episode_reward=2837.01 +/- 2892.29
Episode length: 219.00 +/- 162.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 2.84e+03    |
| time/                   |             |
|    total_timesteps      | 18000000    |
| train/                  |             |
|    approx_kl            | 0.018590545 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.175      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.4         |
|    value_loss           | 0.0177      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 5.31e+03 |
| time/              |          |
|    fps             | 5677     |
|    iterations      | 550      |
|    time_elapsed    | 3174     |
|    total_timesteps | 18022400 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 427         |
|    ep_rew_mean          | 5.52e+03    |
| time/                   |             |
|    fps                  | 5674        |
|    iterations           | 560         |
|    time_elapsed         | 3233        |
|    total_timesteps      | 18350080    |
| train/                  |             |
|    approx_kl            | 0.019482937 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0228     |
|    std                  | 1.42        |
|    value_loss           | 0.0146      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.43e+03    |
| time/                   |             |
|    fps                  | 5672        |
|    iterations           | 570         |
|    time_elapsed         | 3292        |
|    total_timesteps      | 18677760    |
| train/                  |             |
|    approx_kl            | 0.019846879 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.177      |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0214     |
|    std                  | 1.43        |
|    value_loss           | 0.0158      |
-----------------------------------------
Eval num_timesteps=19000000, episode_reward=5388.61 +/- 2498.27
Episode length: 388.40 +/- 162.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 388         |
|    mean_reward          | 5.39e+03    |
| time/                   |             |
|    total_timesteps      | 19000000    |
| train/                  |             |
|    approx_kl            | 0.019195959 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.18       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.0227     |
|    std                  | 1.45        |
|    value_loss           | 0.0134      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 439      |
|    ep_rew_mean     | 5.77e+03 |
| time/              |          |
|    fps             | 5667     |
|    iterations      | 580      |
|    time_elapsed    | 3353     |
|    total_timesteps | 19005440 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 483         |
|    ep_rew_mean          | 5.89e+03    |
| time/                   |             |
|    fps                  | 5664        |
|    iterations           | 590         |
|    time_elapsed         | 3413        |
|    total_timesteps      | 19333120    |
| train/                  |             |
|    approx_kl            | 0.019362781 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29         |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.182      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.0226     |
|    std                  | 1.47        |
|    value_loss           | 0.014       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 471         |
|    ep_rew_mean          | 5.85e+03    |
| time/                   |             |
|    fps                  | 5662        |
|    iterations           | 600         |
|    time_elapsed         | 3471        |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.019506339 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.186      |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0237     |
|    std                  | 1.49        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 434         |
|    ep_rew_mean          | 5.6e+03     |
| time/                   |             |
|    fps                  | 5660        |
|    iterations           | 610         |
|    time_elapsed         | 3531        |
|    total_timesteps      | 19988480    |
| train/                  |             |
|    approx_kl            | 0.020057648 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0229     |
|    std                  | 1.51        |
|    value_loss           | 0.0131      |
-----------------------------------------
Eval num_timesteps=20000000, episode_reward=5495.96 +/- 2685.96
Episode length: 428.80 +/- 208.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | 5.5e+03     |
| time/                   |             |
|    total_timesteps      | 20000000    |
| train/                  |             |
|    approx_kl            | 0.019199617 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.51        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 395         |
|    ep_rew_mean          | 5.52e+03    |
| time/                   |             |
|    fps                  | 5656        |
|    iterations           | 620         |
|    time_elapsed         | 3591        |
|    total_timesteps      | 20316160    |
| train/                  |             |
|    approx_kl            | 0.018922387 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.184      |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.0219     |
|    std                  | 1.53        |
|    value_loss           | 0.0158      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 5.66e+03    |
| time/                   |             |
|    fps                  | 5653        |
|    iterations           | 630         |
|    time_elapsed         | 3651        |
|    total_timesteps      | 20643840    |
| train/                  |             |
|    approx_kl            | 0.020030808 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0223     |
|    std                  | 1.55        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 465         |
|    ep_rew_mean          | 5.62e+03    |
| time/                   |             |
|    fps                  | 5652        |
|    iterations           | 640         |
|    time_elapsed         | 3709        |
|    total_timesteps      | 20971520    |
| train/                  |             |
|    approx_kl            | 0.019041957 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.0221     |
|    std                  | 1.57        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=21000000, episode_reward=5052.99 +/- 2062.65
Episode length: 360.40 +/- 125.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 360         |
|    mean_reward          | 5.05e+03    |
| time/                   |             |
|    total_timesteps      | 21000000    |
| train/                  |             |
|    approx_kl            | 0.019659359 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 6400        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 1.57        |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 460         |
|    ep_rew_mean          | 5.8e+03     |
| time/                   |             |
|    fps                  | 5648        |
|    iterations           | 650         |
|    time_elapsed         | 3770        |
|    total_timesteps      | 21299200    |
| train/                  |             |
|    approx_kl            | 0.020760424 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.189      |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0236     |
|    std                  | 1.59        |
|    value_loss           | 0.00947     |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 421       |
|    ep_rew_mean          | 5.64e+03  |
| time/                   |           |
|    fps                  | 5647      |
|    iterations           | 660       |
|    time_elapsed         | 3829      |
|    total_timesteps      | 21626880  |
| train/                  |           |
|    approx_kl            | 0.0191878 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.2       |
|    entropy_loss         | -30.1     |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.187    |
|    n_updates            | 6590      |
|    policy_gradient_loss | -0.0219   |
|    std                  | 1.61      |
|    value_loss           | 0.0126    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 5.78e+03    |
| time/                   |             |
|    fps                  | 5646        |
|    iterations           | 670         |
|    time_elapsed         | 3888        |
|    total_timesteps      | 21954560    |
| train/                  |             |
|    approx_kl            | 0.018973332 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.188      |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.022      |
|    std                  | 1.63        |
|    value_loss           | 0.0148      |
-----------------------------------------
Eval num_timesteps=22000000, episode_reward=6319.35 +/- 563.96
Episode length: 429.40 +/- 83.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | 6.32e+03    |
| time/                   |             |
|    total_timesteps      | 22000000    |
| train/                  |             |
|    approx_kl            | 0.020559119 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 6710        |
|    policy_gradient_loss | -0.0232     |
|    std                  | 1.63        |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | 5.96e+03    |
| time/                   |             |
|    fps                  | 5641        |
|    iterations           | 680         |
|    time_elapsed         | 3949        |
|    total_timesteps      | 22282240    |
| train/                  |             |
|    approx_kl            | 0.019928958 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.65        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 477         |
|    ep_rew_mean          | 6.08e+03    |
| time/                   |             |
|    fps                  | 5640        |
|    iterations           | 690         |
|    time_elapsed         | 4008        |
|    total_timesteps      | 22609920    |
| train/                  |             |
|    approx_kl            | 0.019312568 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 1.68        |
|    value_loss           | 0.0118      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 417         |
|    ep_rew_mean          | 5.49e+03    |
| time/                   |             |
|    fps                  | 5638        |
|    iterations           | 700         |
|    time_elapsed         | 4067        |
|    total_timesteps      | 22937600    |
| train/                  |             |
|    approx_kl            | 0.018861506 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0215     |
|    std                  | 1.69        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=23000000, episode_reward=6443.91 +/- 1362.14
Episode length: 492.20 +/- 270.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 492         |
|    mean_reward          | 6.44e+03    |
| time/                   |             |
|    total_timesteps      | 23000000    |
| train/                  |             |
|    approx_kl            | 0.020306684 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.189      |
|    n_updates            | 7010        |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.7         |
|    value_loss           | 0.0141      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 582         |
|    ep_rew_mean          | 6.24e+03    |
| time/                   |             |
|    fps                  | 5634        |
|    iterations           | 710         |
|    time_elapsed         | 4128        |
|    total_timesteps      | 23265280    |
| train/                  |             |
|    approx_kl            | 0.018128134 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0216     |
|    std                  | 1.71        |
|    value_loss           | 0.0104      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 483        |
|    ep_rew_mean          | 5.77e+03   |
| time/                   |            |
|    fps                  | 5633       |
|    iterations           | 720        |
|    time_elapsed         | 4187       |
|    total_timesteps      | 23592960   |
| train/                  |            |
|    approx_kl            | 0.01933844 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.2      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.193     |
|    n_updates            | 7190       |
|    policy_gradient_loss | -0.0214    |
|    std                  | 1.74       |
|    value_loss           | 0.0124     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 559         |
|    ep_rew_mean          | 6.35e+03    |
| time/                   |             |
|    fps                  | 5632        |
|    iterations           | 730         |
|    time_elapsed         | 4247        |
|    total_timesteps      | 23920640    |
| train/                  |             |
|    approx_kl            | 0.019507872 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.4       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.76        |
|    value_loss           | 0.00993     |
-----------------------------------------
Eval num_timesteps=24000000, episode_reward=12253.09 +/- 11089.07
Episode length: 2227.40 +/- 2280.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.23e+03    |
|    mean_reward          | 1.23e+04    |
| time/                   |             |
|    total_timesteps      | 24000000    |
| train/                  |             |
|    approx_kl            | 0.020466879 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.023      |
|    std                  | 1.77        |
|    value_loss           | 0.0138      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.43e+03    |
| time/                   |             |
|    fps                  | 5618        |
|    iterations           | 740         |
|    time_elapsed         | 4315        |
|    total_timesteps      | 24248320    |
| train/                  |             |
|    approx_kl            | 0.019633748 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.194      |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.0223     |
|    std                  | 1.78        |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 528         |
|    ep_rew_mean          | 5.82e+03    |
| time/                   |             |
|    fps                  | 5617        |
|    iterations           | 750         |
|    time_elapsed         | 4375        |
|    total_timesteps      | 24576000    |
| train/                  |             |
|    approx_kl            | 0.019729203 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.8       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.194      |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.0222     |
|    std                  | 1.81        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 508         |
|    ep_rew_mean          | 5.84e+03    |
| time/                   |             |
|    fps                  | 5616        |
|    iterations           | 760         |
|    time_elapsed         | 4434        |
|    total_timesteps      | 24903680    |
| train/                  |             |
|    approx_kl            | 0.019109793 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -32         |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.0223     |
|    std                  | 1.84        |
|    value_loss           | 0.0114      |
-----------------------------------------
Eval num_timesteps=25000000, episode_reward=6259.87 +/- 386.99
Episode length: 419.60 +/- 77.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 420         |
|    mean_reward          | 6.26e+03    |
| time/                   |             |
|    total_timesteps      | 25000000    |
| train/                  |             |
|    approx_kl            | 0.019480044 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32         |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.199      |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 1.84        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 480         |
|    ep_rew_mean          | 5.87e+03    |
| time/                   |             |
|    fps                  | 5613        |
|    iterations           | 770         |
|    time_elapsed         | 4494        |
|    total_timesteps      | 25231360    |
| train/                  |             |
|    approx_kl            | 0.019950414 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.2       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.2        |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.0223     |
|    std                  | 1.86        |
|    value_loss           | 0.0113      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 567         |
|    ep_rew_mean          | 6.03e+03    |
| time/                   |             |
|    fps                  | 5611        |
|    iterations           | 780         |
|    time_elapsed         | 4554        |
|    total_timesteps      | 25559040    |
| train/                  |             |
|    approx_kl            | 0.019414376 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.4       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.199      |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.89        |
|    value_loss           | 0.011       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 577         |
|    ep_rew_mean          | 6.44e+03    |
| time/                   |             |
|    fps                  | 5610        |
|    iterations           | 790         |
|    time_elapsed         | 4614        |
|    total_timesteps      | 25886720    |
| train/                  |             |
|    approx_kl            | 0.020435262 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.6       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.204      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0238     |
|    std                  | 1.92        |
|    value_loss           | 0.00779     |
-----------------------------------------
Eval num_timesteps=26000000, episode_reward=10267.48 +/- 7798.67
Episode length: 1355.80 +/- 1822.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.36e+03   |
|    mean_reward          | 1.03e+04   |
| time/                   |            |
|    total_timesteps      | 26000000   |
| train/                  |            |
|    approx_kl            | 0.01934794 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.7      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.203     |
|    n_updates            | 7930       |
|    policy_gradient_loss | -0.023     |
|    std                  | 1.93       |
|    value_loss           | 0.00923    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 607         |
|    ep_rew_mean          | 6.53e+03    |
| time/                   |             |
|    fps                  | 5602        |
|    iterations           | 800         |
|    time_elapsed         | 4679        |
|    total_timesteps      | 26214400    |
| train/                  |             |
|    approx_kl            | 0.018123291 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.8       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0219     |
|    std                  | 1.95        |
|    value_loss           | 0.0097      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 646        |
|    ep_rew_mean          | 6.58e+03   |
| time/                   |            |
|    fps                  | 5601       |
|    iterations           | 810        |
|    time_elapsed         | 4738       |
|    total_timesteps      | 26542080   |
| train/                  |            |
|    approx_kl            | 0.01936351 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.9      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.204     |
|    n_updates            | 8090       |
|    policy_gradient_loss | -0.0226    |
|    std                  | 1.96       |
|    value_loss           | 0.00893    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 592         |
|    ep_rew_mean          | 6.2e+03     |
| time/                   |             |
|    fps                  | 5600        |
|    iterations           | 820         |
|    time_elapsed         | 4797        |
|    total_timesteps      | 26869760    |
| train/                  |             |
|    approx_kl            | 0.021050343 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33         |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.206      |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0243     |
|    std                  | 1.98        |
|    value_loss           | 0.00754     |
-----------------------------------------
Eval num_timesteps=27000000, episode_reward=15693.63 +/- 7915.97
Episode length: 3153.00 +/- 2262.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.15e+03    |
|    mean_reward          | 1.57e+04    |
| time/                   |             |
|    total_timesteps      | 27000000    |
| train/                  |             |
|    approx_kl            | 0.021145964 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.1       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.206      |
|    n_updates            | 8230        |
|    policy_gradient_loss | -0.0237     |
|    std                  | 1.99        |
|    value_loss           | 0.00948     |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 668         |
|    ep_rew_mean          | 6.35e+03    |
| time/                   |             |
|    fps                  | 5584        |
|    iterations           | 830         |
|    time_elapsed         | 4870        |
|    total_timesteps      | 27197440    |
| train/                  |             |
|    approx_kl            | 0.018781938 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.2       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.199      |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.01        |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 532         |
|    ep_rew_mean          | 6.21e+03    |
| time/                   |             |
|    fps                  | 5583        |
|    iterations           | 840         |
|    time_elapsed         | 4929        |
|    total_timesteps      | 27525120    |
| train/                  |             |
|    approx_kl            | 0.019033685 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.204      |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.0219     |
|    std                  | 2.03        |
|    value_loss           | 0.0103      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 549         |
|    ep_rew_mean          | 5.78e+03    |
| time/                   |             |
|    fps                  | 5582        |
|    iterations           | 850         |
|    time_elapsed         | 4989        |
|    total_timesteps      | 27852800    |
| train/                  |             |
|    approx_kl            | 0.019180797 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.6       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.205      |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.022      |
|    std                  | 2.07        |
|    value_loss           | 0.0146      |
-----------------------------------------
Eval num_timesteps=28000000, episode_reward=7207.70 +/- 1279.85
Episode length: 632.60 +/- 295.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 633       |
|    mean_reward          | 7.21e+03  |
| time/                   |           |
|    total_timesteps      | 28000000  |
| train/                  |           |
|    approx_kl            | 0.0204609 |
|    clip_fraction        | 0.152     |
|    clip_range           | 0.2       |
|    entropy_loss         | -33.7     |
|    explained_variance   | 0.991     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.211    |
|    n_updates            | 8540      |
|    policy_gradient_loss | -0.0241   |
|    std                  | 2.07      |
|    value_loss           | 0.00682   |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 658         |
|    ep_rew_mean          | 6.57e+03    |
| time/                   |             |
|    fps                  | 5578        |
|    iterations           | 860         |
|    time_elapsed         | 5051        |
|    total_timesteps      | 28180480    |
| train/                  |             |
|    approx_kl            | 0.020873478 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.7       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.211      |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.0245     |
|    std                  | 2.08        |
|    value_loss           | 0.00885     |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 685       |
|    ep_rew_mean          | 6.5e+03   |
| time/                   |           |
|    fps                  | 5577      |
|    iterations           | 870       |
|    time_elapsed         | 5111      |
|    total_timesteps      | 28508160  |
| train/                  |           |
|    approx_kl            | 0.0217847 |
|    clip_fraction        | 0.162     |
|    clip_range           | 0.2       |
|    entropy_loss         | -33.9     |
|    explained_variance   | 0.986     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.208    |
|    n_updates            | 8690      |
|    policy_gradient_loss | -0.0228   |
|    std                  | 2.11      |
|    value_loss           | 0.0115    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 678       |
|    ep_rew_mean          | 6.56e+03  |
| time/                   |           |
|    fps                  | 5576      |
|    iterations           | 880       |
|    time_elapsed         | 5171      |
|    total_timesteps      | 28835840  |
| train/                  |           |
|    approx_kl            | 0.0219424 |
|    clip_fraction        | 0.161     |
|    clip_range           | 0.2       |
|    entropy_loss         | -34.1     |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.211    |
|    n_updates            | 8790      |
|    policy_gradient_loss | -0.0241   |
|    std                  | 2.15      |
|    value_loss           | 0.0113    |
---------------------------------------
Eval num_timesteps=29000000, episode_reward=7337.65 +/- 3771.56
Episode length: 811.60 +/- 612.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | 7.34e+03    |
| time/                   |             |
|    total_timesteps      | 29000000    |
| train/                  |             |
|    approx_kl            | 0.020230135 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.212      |
|    n_updates            | 8850        |
|    policy_gradient_loss | -0.0236     |
|    std                  | 2.16        |
|    value_loss           | 0.00658     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 878         |
|    ep_rew_mean          | 7.23e+03    |
| time/                   |             |
|    fps                  | 5571        |
|    iterations           | 890         |
|    time_elapsed         | 5234        |
|    total_timesteps      | 29163520    |
| train/                  |             |
|    approx_kl            | 0.019666864 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.4       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0226     |
|    std                  | 2.19        |
|    value_loss           | 0.00969     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 691        |
|    ep_rew_mean          | 6.61e+03   |
| time/                   |            |
|    fps                  | 5570       |
|    iterations           | 900        |
|    time_elapsed         | 5293       |
|    total_timesteps      | 29491200   |
| train/                  |            |
|    approx_kl            | 0.02035629 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.6      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.212     |
|    n_updates            | 8990       |
|    policy_gradient_loss | -0.0232    |
|    std                  | 2.22       |
|    value_loss           | 0.0109     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 575         |
|    ep_rew_mean          | 6.33e+03    |
| time/                   |             |
|    fps                  | 5570        |
|    iterations           | 910         |
|    time_elapsed         | 5353        |
|    total_timesteps      | 29818880    |
| train/                  |             |
|    approx_kl            | 0.019152869 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.212      |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0225     |
|    std                  | 2.27        |
|    value_loss           | 0.0113      |
-----------------------------------------
Eval num_timesteps=30000000, episode_reward=10494.09 +/- 7430.70
Episode length: 1425.40 +/- 1796.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.43e+03   |
|    mean_reward          | 1.05e+04   |
| time/                   |            |
|    total_timesteps      | 30000000   |
| train/                  |            |
|    approx_kl            | 0.01861711 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.211     |
|    n_updates            | 9150       |
|    policy_gradient_loss | -0.0226    |
|    std                  | 2.28       |
|    value_loss           | 0.0158     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 570         |
|    ep_rew_mean          | 6.15e+03    |
| time/                   |             |
|    fps                  | 5563        |
|    iterations           | 920         |
|    time_elapsed         | 5418        |
|    total_timesteps      | 30146560    |
| train/                  |             |
|    approx_kl            | 0.020314898 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.216      |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0237     |
|    std                  | 2.3         |
|    value_loss           | 0.0107      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 510         |
|    ep_rew_mean          | 5.63e+03    |
| time/                   |             |
|    fps                  | 5563        |
|    iterations           | 930         |
|    time_elapsed         | 5477        |
|    total_timesteps      | 30474240    |
| train/                  |             |
|    approx_kl            | 0.020515522 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.216      |
|    n_updates            | 9290        |
|    policy_gradient_loss | -0.0236     |
|    std                  | 2.33        |
|    value_loss           | 0.0129      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 802       |
|    ep_rew_mean          | 7.22e+03  |
| time/                   |           |
|    fps                  | 5562      |
|    iterations           | 940       |
|    time_elapsed         | 5537      |
|    total_timesteps      | 30801920  |
| train/                  |           |
|    approx_kl            | 0.0196955 |
|    clip_fraction        | 0.151     |
|    clip_range           | 0.2       |
|    entropy_loss         | -35.5     |
|    explained_variance   | 0.984     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.213    |
|    n_updates            | 9390      |
|    policy_gradient_loss | -0.0216   |
|    std                  | 2.36      |
|    value_loss           | 0.00995   |
---------------------------------------
Eval num_timesteps=31000000, episode_reward=15817.87 +/- 8711.28
Episode length: 2869.40 +/- 2075.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.87e+03    |
|    mean_reward          | 1.58e+04    |
| time/                   |             |
|    total_timesteps      | 31000000    |
| train/                  |             |
|    approx_kl            | 0.019847844 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.6       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 9460        |
|    policy_gradient_loss | -0.0238     |
|    std                  | 2.38        |
|    value_loss           | 0.00876     |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 934         |
|    ep_rew_mean          | 7.48e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 950         |
|    time_elapsed         | 5608        |
|    total_timesteps      | 31129600    |
| train/                  |             |
|    approx_kl            | 0.020799693 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 9490        |
|    policy_gradient_loss | -0.0233     |
|    std                  | 2.39        |
|    value_loss           | 0.00797     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 569         |
|    ep_rew_mean          | 6.13e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 960         |
|    time_elapsed         | 5667        |
|    total_timesteps      | 31457280    |
| train/                  |             |
|    approx_kl            | 0.018929984 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.216      |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 2.44        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 745         |
|    ep_rew_mean          | 6.94e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 970         |
|    time_elapsed         | 5726        |
|    total_timesteps      | 31784960    |
| train/                  |             |
|    approx_kl            | 0.019481739 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.0223     |
|    std                  | 2.47        |
|    value_loss           | 0.00976     |
-----------------------------------------
Eval num_timesteps=32000000, episode_reward=19213.98 +/- 7720.39
Episode length: 3648.60 +/- 1709.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.65e+03    |
|    mean_reward          | 1.92e+04    |
| time/                   |             |
|    total_timesteps      | 32000000    |
| train/                  |             |
|    approx_kl            | 0.020147745 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.219      |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.0225     |
|    std                  | 2.5         |
|    value_loss           | 0.0136      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1e+03      |
|    ep_rew_mean          | 8.04e+03   |
| time/                   |            |
|    fps                  | 5535       |
|    iterations           | 980        |
|    time_elapsed         | 5800       |
|    total_timesteps      | 32112640   |
| train/                  |            |
|    approx_kl            | 0.01766624 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.4      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.217     |
|    n_updates            | 9790       |
|    policy_gradient_loss | -0.0206    |
|    std                  | 2.52       |
|    value_loss           | 0.00855    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 995         |
|    ep_rew_mean          | 8.06e+03    |
| time/                   |             |
|    fps                  | 5535        |
|    iterations           | 990         |
|    time_elapsed         | 5860        |
|    total_timesteps      | 32440320    |
| train/                  |             |
|    approx_kl            | 0.020764317 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.6       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.223      |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0232     |
|    std                  | 2.56        |
|    value_loss           | 0.0059      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.11e+03    |
|    ep_rew_mean          | 8.68e+03    |
| time/                   |             |
|    fps                  | 5534        |
|    iterations           | 1000        |
|    time_elapsed         | 5920        |
|    total_timesteps      | 32768000    |
| train/                  |             |
|    approx_kl            | 0.017572403 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.8       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.222      |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 2.6         |
|    value_loss           | 0.0067      |
-----------------------------------------
Eval num_timesteps=33000000, episode_reward=11636.58 +/- 7808.95
Episode length: 1784.00 +/- 1840.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.78e+03    |
|    mean_reward          | 1.16e+04    |
| time/                   |             |
|    total_timesteps      | 33000000    |
| train/                  |             |
|    approx_kl            | 0.020308888 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37         |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 2.63        |
|    value_loss           | 0.0086      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 858        |
|    ep_rew_mean          | 7.3e+03    |
| time/                   |            |
|    fps                  | 5527       |
|    iterations           | 1010       |
|    time_elapsed         | 5987       |
|    total_timesteps      | 33095680   |
| train/                  |            |
|    approx_kl            | 0.01944729 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -37        |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.225     |
|    n_updates            | 10090      |
|    policy_gradient_loss | -0.024     |
|    std                  | 2.64       |
|    value_loss           | 0.00918    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 688         |
|    ep_rew_mean          | 6.84e+03    |
| time/                   |             |
|    fps                  | 5526        |
|    iterations           | 1020        |
|    time_elapsed         | 6047        |
|    total_timesteps      | 33423360    |
| train/                  |             |
|    approx_kl            | 0.019742228 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.3       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.227      |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 2.69        |
|    value_loss           | 0.00929     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.04e+03    |
|    ep_rew_mean          | 8.11e+03    |
| time/                   |             |
|    fps                  | 5526        |
|    iterations           | 1030        |
|    time_elapsed         | 6107        |
|    total_timesteps      | 33751040    |
| train/                  |             |
|    approx_kl            | 0.019579532 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.4       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.224      |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0222     |
|    std                  | 2.72        |
|    value_loss           | 0.0084      |
-----------------------------------------
Eval num_timesteps=34000000, episode_reward=19237.36 +/- 5412.11
Episode length: 3137.80 +/- 1043.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.14e+03    |
|    mean_reward          | 1.92e+04    |
| time/                   |             |
|    total_timesteps      | 34000000    |
| train/                  |             |
|    approx_kl            | 0.017654058 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.7       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.227      |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.0223     |
|    std                  | 2.76        |
|    value_loss           | 0.00478     |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.19e+03    |
|    ep_rew_mean          | 9.15e+03    |
| time/                   |             |
|    fps                  | 5514        |
|    iterations           | 1040        |
|    time_elapsed         | 6179        |
|    total_timesteps      | 34078720    |
| train/                  |             |
|    approx_kl            | 0.017961469 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.7       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 2.76        |
|    value_loss           | 0.00736     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 771         |
|    ep_rew_mean          | 6.98e+03    |
| time/                   |             |
|    fps                  | 5514        |
|    iterations           | 1050        |
|    time_elapsed         | 6239        |
|    total_timesteps      | 34406400    |
| train/                  |             |
|    approx_kl            | 0.019005008 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.224      |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 2.81        |
|    value_loss           | 0.0163      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 715        |
|    ep_rew_mean          | 6.68e+03   |
| time/                   |            |
|    fps                  | 5513       |
|    iterations           | 1060       |
|    time_elapsed         | 6299       |
|    total_timesteps      | 34734080   |
| train/                  |            |
|    approx_kl            | 0.01829012 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -38.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.227     |
|    n_updates            | 10590      |
|    policy_gradient_loss | -0.0219    |
|    std                  | 2.85       |
|    value_loss           | 0.0101     |
----------------------------------------
Eval num_timesteps=35000000, episode_reward=12692.74 +/- 7974.97
Episode length: 2121.80 +/- 1919.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 2.12e+03    |
|    mean_reward          | 1.27e+04    |
| time/                   |             |
|    total_timesteps      | 35000000    |
| train/                  |             |
|    approx_kl            | 0.017790195 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.0219     |
|    std                  | 2.89        |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 8.24e+03    |
| time/                   |             |
|    fps                  | 5506        |
|    iterations           | 1070        |
|    time_elapsed         | 6367        |
|    total_timesteps      | 35061760    |
| train/                  |             |
|    approx_kl            | 0.019666124 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.3       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.232      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 2.89        |
|    value_loss           | 0.0061      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.07e+03    |
|    ep_rew_mean          | 8.36e+03    |
| time/                   |             |
|    fps                  | 5505        |
|    iterations           | 1080        |
|    time_elapsed         | 6428        |
|    total_timesteps      | 35389440    |
| train/                  |             |
|    approx_kl            | 0.019331938 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.231      |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0231     |
|    std                  | 2.93        |
|    value_loss           | 0.00856     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 972         |
|    ep_rew_mean          | 7.9e+03     |
| time/                   |             |
|    fps                  | 5504        |
|    iterations           | 1090        |
|    time_elapsed         | 6488        |
|    total_timesteps      | 35717120    |
| train/                  |             |
|    approx_kl            | 0.020535324 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.234      |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.024      |
|    std                  | 2.97        |
|    value_loss           | 0.00907     |
-----------------------------------------
Eval num_timesteps=36000000, episode_reward=10450.67 +/- 5519.06
Episode length: 1812.40 +/- 1845.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.81e+03    |
|    mean_reward          | 1.05e+04    |
| time/                   |             |
|    total_timesteps      | 36000000    |
| train/                  |             |
|    approx_kl            | 0.019514835 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.9       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 3.03        |
|    value_loss           | 0.00849     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 788         |
|    ep_rew_mean          | 6.95e+03    |
| time/                   |             |
|    fps                  | 5498        |
|    iterations           | 1100        |
|    time_elapsed         | 6555        |
|    total_timesteps      | 36044800    |
| train/                  |             |
|    approx_kl            | 0.018569853 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 3.03        |
|    value_loss           | 0.0111      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.14e+03    |
|    ep_rew_mean          | 8.68e+03    |
| time/                   |             |
|    fps                  | 5498        |
|    iterations           | 1110        |
|    time_elapsed         | 6614        |
|    total_timesteps      | 36372480    |
| train/                  |             |
|    approx_kl            | 0.020166378 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.1       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.238      |
|    n_updates            | 11090       |
|    policy_gradient_loss | -0.0247     |
|    std                  | 3.07        |
|    value_loss           | 0.00494     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 881         |
|    ep_rew_mean          | 7.27e+03    |
| time/                   |             |
|    fps                  | 5498        |
|    iterations           | 1120        |
|    time_elapsed         | 6674        |
|    total_timesteps      | 36700160    |
| train/                  |             |
|    approx_kl            | 0.020685202 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.3       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 3.12        |
|    value_loss           | 0.00964     |
-----------------------------------------
Eval num_timesteps=37000000, episode_reward=6649.31 +/- 715.48
Episode length: 504.80 +/- 131.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 505        |
|    mean_reward          | 6.65e+03   |
| time/                   |            |
|    total_timesteps      | 37000000   |
| train/                  |            |
|    approx_kl            | 0.02081527 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -39.5      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.239     |
|    n_updates            | 11290      |
|    policy_gradient_loss | -0.0248    |
|    std                  | 3.18       |
|    value_loss           | 0.00845    |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 849      |
|    ep_rew_mean     | 7.39e+03 |
| time/              |          |
|    fps             | 5496     |
|    iterations      | 1130     |
|    time_elapsed    | 6736     |
|    total_timesteps | 37027840 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.08e+03    |
|    ep_rew_mean          | 8.35e+03    |
| time/                   |             |
|    fps                  | 5495        |
|    iterations           | 1140        |
|    time_elapsed         | 6797        |
|    total_timesteps      | 37355520    |
| train/                  |             |
|    approx_kl            | 0.018379152 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 3.22        |
|    value_loss           | 0.0101      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 954         |
|    ep_rew_mean          | 7.84e+03    |
| time/                   |             |
|    fps                  | 5495        |
|    iterations           | 1150        |
|    time_elapsed         | 6857        |
|    total_timesteps      | 37683200    |
| train/                  |             |
|    approx_kl            | 0.019507496 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 3.27        |
|    value_loss           | 0.00905     |
-----------------------------------------
Eval num_timesteps=38000000, episode_reward=8909.79 +/- 8498.96
Episode length: 1271.80 +/- 1869.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.27e+03    |
|    mean_reward          | 8.91e+03    |
| time/                   |             |
|    total_timesteps      | 38000000    |
| train/                  |             |
|    approx_kl            | 0.018934486 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.2       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.0216     |
|    std                  | 3.33        |
|    value_loss           | 0.0105      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 986      |
|    ep_rew_mean     | 7.89e+03 |
| time/              |          |
|    fps             | 5491     |
|    iterations      | 1160     |
|    time_elapsed    | 6921     |
|    total_timesteps | 38010880 |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.23e+03   |
|    ep_rew_mean          | 9.07e+03   |
| time/                   |            |
|    fps                  | 5491       |
|    iterations           | 1170       |
|    time_elapsed         | 6981       |
|    total_timesteps      | 38338560   |
| train/                  |            |
|    approx_kl            | 0.01843239 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.4      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.241     |
|    n_updates            | 11690      |
|    policy_gradient_loss | -0.0227    |
|    std                  | 3.38       |
|    value_loss           | 0.0071     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.07e+03    |
|    ep_rew_mean          | 8.24e+03    |
| time/                   |             |
|    fps                  | 5491        |
|    iterations           | 1180        |
|    time_elapsed         | 7041        |
|    total_timesteps      | 38666240    |
| train/                  |             |
|    approx_kl            | 0.017680744 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.244      |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 3.43        |
|    value_loss           | 0.00516     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.06e+03    |
|    ep_rew_mean          | 8.27e+03    |
| time/                   |             |
|    fps                  | 5490        |
|    iterations           | 1190        |
|    time_elapsed         | 7101        |
|    total_timesteps      | 38993920    |
| train/                  |             |
|    approx_kl            | 0.019721001 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.242      |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 3.48        |
|    value_loss           | 0.0095      |
-----------------------------------------
Eval num_timesteps=39000000, episode_reward=10546.88 +/- 7118.36
Episode length: 1388.60 +/- 1810.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.39e+03    |
|    mean_reward          | 1.05e+04    |
| time/                   |             |
|    total_timesteps      | 39000000    |
| train/                  |             |
|    approx_kl            | 0.020489536 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.8       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.245      |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0237     |
|    std                  | 3.49        |
|    value_loss           | 0.00698     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.29e+03    |
|    ep_rew_mean          | 9.33e+03    |
| time/                   |             |
|    fps                  | 5485        |
|    iterations           | 1200        |
|    time_elapsed         | 7167        |
|    total_timesteps      | 39321600    |
| train/                  |             |
|    approx_kl            | 0.019774819 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.9       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.243      |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.0224     |
|    std                  | 3.52        |
|    value_loss           | 0.0094      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 931        |
|    ep_rew_mean          | 7.17e+03   |
| time/                   |            |
|    fps                  | 5485       |
|    iterations           | 1210       |
|    time_elapsed         | 7227       |
|    total_timesteps      | 39649280   |
| train/                  |            |
|    approx_kl            | 0.01894244 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -41.1      |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.244     |
|    n_updates            | 12090      |
|    policy_gradient_loss | -0.0223    |
|    std                  | 3.58       |
|    value_loss           | 0.00882    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 717         |
|    ep_rew_mean          | 6.59e+03    |
| time/                   |             |
|    fps                  | 5485        |
|    iterations           | 1220        |
|    time_elapsed         | 7287        |
|    total_timesteps      | 39976960    |
| train/                  |             |
|    approx_kl            | 0.019188896 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.4       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.023      |
|    std                  | 3.64        |
|    value_loss           | 0.00727     |
-----------------------------------------
Eval num_timesteps=40000000, episode_reward=12613.35 +/- 9722.25
Episode length: 1956.00 +/- 1833.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.96e+03    |
|    mean_reward          | 1.26e+04    |
| time/                   |             |
|    total_timesteps      | 40000000    |
| train/                  |             |
|    approx_kl            | 0.017732348 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.4       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 3.65        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 845         |
|    ep_rew_mean          | 6.99e+03    |
| time/                   |             |
|    fps                  | 5480        |
|    iterations           | 1230        |
|    time_elapsed         | 7354        |
|    total_timesteps      | 40304640    |
| train/                  |             |
|    approx_kl            | 0.019597843 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.6       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.248      |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 3.71        |
|    value_loss           | 0.00786     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 993         |
|    ep_rew_mean          | 8.04e+03    |
| time/                   |             |
|    fps                  | 5480        |
|    iterations           | 1240        |
|    time_elapsed         | 7414        |
|    total_timesteps      | 40632320    |
| train/                  |             |
|    approx_kl            | 0.018996801 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.247      |
|    n_updates            | 12390       |
|    policy_gradient_loss | -0.022      |
|    std                  | 3.77        |
|    value_loss           | 0.00997     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 744        |
|    ep_rew_mean          | 6.22e+03   |
| time/                   |            |
|    fps                  | 5480       |
|    iterations           | 1250       |
|    time_elapsed         | 7474       |
|    total_timesteps      | 40960000   |
| train/                  |            |
|    approx_kl            | 0.01880404 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.248     |
|    n_updates            | 12490      |
|    policy_gradient_loss | -0.023     |
|    std                  | 3.81       |
|    value_loss           | 0.0133     |
----------------------------------------
Eval num_timesteps=41000000, episode_reward=20765.59 +/- 2451.72
Episode length: 3706.20 +/- 904.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 3.71e+03    |
|    mean_reward          | 2.08e+04    |
| time/                   |             |
|    total_timesteps      | 41000000    |
| train/                  |             |
|    approx_kl            | 0.018393151 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -42         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.248      |
|    n_updates            | 12510       |
|    policy_gradient_loss | -0.0224     |
|    std                  | 3.82        |
|    value_loss           | 0.0103      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 804         |
|    ep_rew_mean          | 6.51e+03    |
| time/                   |             |
|    fps                  | 5469        |
|    iterations           | 1260        |
|    time_elapsed         | 7548        |
|    total_timesteps      | 41287680    |
| train/                  |             |
|    approx_kl            | 0.017702581 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0212     |
|    std                  | 3.86        |
|    value_loss           | 0.0111      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 576        |
|    ep_rew_mean          | 5.42e+03   |
| time/                   |            |
|    fps                  | 5470       |
|    iterations           | 1270       |
|    time_elapsed         | 7607       |
|    total_timesteps      | 41615360   |
| train/                  |            |
|    approx_kl            | 0.01792679 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.246     |
|    n_updates            | 12690      |
|    policy_gradient_loss | -0.0214    |
|    std                  | 3.93       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 7.54e+03    |
| time/                   |             |
|    fps                  | 5470        |
|    iterations           | 1280        |
|    time_elapsed         | 7667        |
|    total_timesteps      | 41943040    |
| train/                  |             |
|    approx_kl            | 0.019132849 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.6       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.252      |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 3.99        |
|    value_loss           | 0.0074      |
-----------------------------------------
Eval num_timesteps=42000000, episode_reward=10052.19 +/- 5220.00
Episode length: 1575.00 +/- 1746.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.58e+03    |
|    mean_reward          | 1.01e+04    |
| time/                   |             |
|    total_timesteps      | 42000000    |
| train/                  |             |
|    approx_kl            | 0.019264258 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.248      |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0215     |
|    std                  | 4           |
|    value_loss           | 0.015       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 917         |
|    ep_rew_mean          | 7.19e+03    |
| time/                   |             |
|    fps                  | 5465        |
|    iterations           | 1290        |
|    time_elapsed         | 7733        |
|    total_timesteps      | 42270720    |
| train/                  |             |
|    approx_kl            | 0.021006044 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.257      |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.0247     |
|    std                  | 4.03        |
|    value_loss           | 0.00509     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 896         |
|    ep_rew_mean          | 6.85e+03    |
| time/                   |             |
|    fps                  | 5466        |
|    iterations           | 1300        |
|    time_elapsed         | 7793        |
|    total_timesteps      | 42598400    |
| train/                  |             |
|    approx_kl            | 0.019041654 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.254      |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 4.09        |
|    value_loss           | 0.0102      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 874         |
|    ep_rew_mean          | 6.99e+03    |
| time/                   |             |
|    fps                  | 5466        |
|    iterations           | 1310        |
|    time_elapsed         | 7852        |
|    total_timesteps      | 42926080    |
| train/                  |             |
|    approx_kl            | 0.018513165 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.254      |
|    n_updates            | 13090       |
|    policy_gradient_loss | -0.0221     |
|    std                  | 4.16        |
|    value_loss           | 0.00794     |
-----------------------------------------
Eval num_timesteps=43000000, episode_reward=8201.46 +/- 6778.01
Episode length: 1188.40 +/- 1195.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.19e+03    |
|    mean_reward          | 8.2e+03     |
| time/                   |             |
|    total_timesteps      | 43000000    |
| train/                  |             |
|    approx_kl            | 0.018877871 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.25       |
|    n_updates            | 13120       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 4.18        |
|    value_loss           | 0.0186      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 882         |
|    ep_rew_mean          | 7.17e+03    |
| time/                   |             |
|    fps                  | 5463        |
|    iterations           | 1320        |
|    time_elapsed         | 7916        |
|    total_timesteps      | 43253760    |
| train/                  |             |
|    approx_kl            | 0.019610686 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.2       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 13190       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 4.23        |
|    value_loss           | 0.011       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.1e+03     |
|    ep_rew_mean          | 8.07e+03    |
| time/                   |             |
|    fps                  | 5463        |
|    iterations           | 1330        |
|    time_elapsed         | 7976        |
|    total_timesteps      | 43581440    |
| train/                  |             |
|    approx_kl            | 0.019829834 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 4.27        |
|    value_loss           | 0.00788     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 749        |
|    ep_rew_mean          | 6.73e+03   |
| time/                   |            |
|    fps                  | 5464       |
|    iterations           | 1340       |
|    time_elapsed         | 8035       |
|    total_timesteps      | 43909120   |
| train/                  |            |
|    approx_kl            | 0.01892373 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.256     |
|    n_updates            | 13390      |
|    policy_gradient_loss | -0.0224    |
|    std                  | 4.33       |
|    value_loss           | 0.0121     |
----------------------------------------
Eval num_timesteps=44000000, episode_reward=11210.84 +/- 9581.35
Episode length: 1793.80 +/- 1956.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.79e+03    |
|    mean_reward          | 1.12e+04    |
| time/                   |             |
|    total_timesteps      | 44000000    |
| train/                  |             |
|    approx_kl            | 0.018742554 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.0223     |
|    std                  | 4.35        |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 903         |
|    ep_rew_mean          | 7.54e+03    |
| time/                   |             |
|    fps                  | 5459        |
|    iterations           | 1350        |
|    time_elapsed         | 8102        |
|    total_timesteps      | 44236800    |
| train/                  |             |
|    approx_kl            | 0.021257417 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.261      |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.0244     |
|    std                  | 4.41        |
|    value_loss           | 0.00733     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 793         |
|    ep_rew_mean          | 6.62e+03    |
| time/                   |             |
|    fps                  | 5459        |
|    iterations           | 1360        |
|    time_elapsed         | 8162        |
|    total_timesteps      | 44564480    |
| train/                  |             |
|    approx_kl            | 0.019213047 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.253      |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0212     |
|    std                  | 4.47        |
|    value_loss           | 0.0159      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 893        |
|    ep_rew_mean          | 7.49e+03   |
| time/                   |            |
|    fps                  | 5460       |
|    iterations           | 1370       |
|    time_elapsed         | 8221       |
|    total_timesteps      | 44892160   |
| train/                  |            |
|    approx_kl            | 0.01782202 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -44.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.258     |
|    n_updates            | 13690      |
|    policy_gradient_loss | -0.0218    |
|    std                  | 4.54       |
|    value_loss           | 0.0121     |
----------------------------------------
Eval num_timesteps=45000000, episode_reward=13314.15 +/- 4409.39
Episode length: 1892.00 +/- 855.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.89e+03    |
|    mean_reward          | 1.33e+04    |
| time/                   |             |
|    total_timesteps      | 45000000    |
| train/                  |             |
|    approx_kl            | 0.019181529 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.2       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.261      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 4.57        |
|    value_loss           | 0.0088      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 917         |
|    ep_rew_mean          | 7.34e+03    |
| time/                   |             |
|    fps                  | 5455        |
|    iterations           | 1380        |
|    time_elapsed         | 8288        |
|    total_timesteps      | 45219840    |
| train/                  |             |
|    approx_kl            | 0.017977396 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.022      |
|    std                  | 4.62        |
|    value_loss           | 0.0105      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 925         |
|    ep_rew_mean          | 7.53e+03    |
| time/                   |             |
|    fps                  | 5456        |
|    iterations           | 1390        |
|    time_elapsed         | 8347        |
|    total_timesteps      | 45547520    |
| train/                  |             |
|    approx_kl            | 0.019489383 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.5       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.264      |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 4.69        |
|    value_loss           | 0.00795     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.07e+03    |
|    ep_rew_mean          | 8e+03       |
| time/                   |             |
|    fps                  | 5456        |
|    iterations           | 1400        |
|    time_elapsed         | 8407        |
|    total_timesteps      | 45875200    |
| train/                  |             |
|    approx_kl            | 0.018154152 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0217     |
|    std                  | 4.78        |
|    value_loss           | 0.0104      |
-----------------------------------------
Eval num_timesteps=46000000, episode_reward=12624.78 +/- 2839.75
Episode length: 1630.60 +/- 565.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.63e+03    |
|    mean_reward          | 1.26e+04    |
| time/                   |             |
|    total_timesteps      | 46000000    |
| train/                  |             |
|    approx_kl            | 0.019231556 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.9       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.264      |
|    n_updates            | 14030       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 4.82        |
|    value_loss           | 0.00826     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 973         |
|    ep_rew_mean          | 7.89e+03    |
| time/                   |             |
|    fps                  | 5452        |
|    iterations           | 1410        |
|    time_elapsed         | 8473        |
|    total_timesteps      | 46202880    |
| train/                  |             |
|    approx_kl            | 0.019139685 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45         |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.264      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.023      |
|    std                  | 4.86        |
|    value_loss           | 0.00726     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 905         |
|    ep_rew_mean          | 7.44e+03    |
| time/                   |             |
|    fps                  | 5453        |
|    iterations           | 1420        |
|    time_elapsed         | 8532        |
|    total_timesteps      | 46530560    |
| train/                  |             |
|    approx_kl            | 0.018250361 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.2       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.265      |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.0224     |
|    std                  | 4.95        |
|    value_loss           | 0.0103      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.11e+03    |
|    ep_rew_mean          | 8.58e+03    |
| time/                   |             |
|    fps                  | 5453        |
|    iterations           | 1430        |
|    time_elapsed         | 8592        |
|    total_timesteps      | 46858240    |
| train/                  |             |
|    approx_kl            | 0.019750733 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.265      |
|    n_updates            | 14290       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 5.04        |
|    value_loss           | 0.0118      |
-----------------------------------------
Eval num_timesteps=47000000, episode_reward=9941.45 +/- 6741.75
Episode length: 1255.60 +/- 1468.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.26e+03   |
|    mean_reward          | 9.94e+03   |
| time/                   |            |
|    total_timesteps      | 47000000   |
| train/                  |            |
|    approx_kl            | 0.01941067 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.5      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.265     |
|    n_updates            | 14340      |
|    policy_gradient_loss | -0.0228    |
|    std                  | 5.07       |
|    value_loss           | 0.0109     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.22e+03    |
|    ep_rew_mean          | 8.87e+03    |
| time/                   |             |
|    fps                  | 5451        |
|    iterations           | 1440        |
|    time_elapsed         | 8656        |
|    total_timesteps      | 47185920    |
| train/                  |             |
|    approx_kl            | 0.021155693 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.5       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.269      |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0237     |
|    std                  | 5.1         |
|    value_loss           | 0.00695     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.01e+03   |
|    ep_rew_mean          | 7.62e+03   |
| time/                   |            |
|    fps                  | 5451       |
|    iterations           | 1450       |
|    time_elapsed         | 8715       |
|    total_timesteps      | 47513600   |
| train/                  |            |
|    approx_kl            | 0.01931747 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.7      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.266     |
|    n_updates            | 14490      |
|    policy_gradient_loss | -0.0219    |
|    std                  | 5.17       |
|    value_loss           | 0.0168     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 927         |
|    ep_rew_mean          | 7.93e+03    |
| time/                   |             |
|    fps                  | 5451        |
|    iterations           | 1460        |
|    time_elapsed         | 8775        |
|    total_timesteps      | 47841280    |
| train/                  |             |
|    approx_kl            | 0.019068828 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -46         |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.268      |
|    n_updates            | 14590       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 5.26        |
|    value_loss           | 0.0104      |
-----------------------------------------
Eval num_timesteps=48000000, episode_reward=12516.89 +/- 3622.81
Episode length: 1365.20 +/- 581.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.37e+03    |
|    mean_reward          | 1.25e+04    |
| time/                   |             |
|    total_timesteps      | 48000000    |
| train/                  |             |
|    approx_kl            | 0.020428546 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.1       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 14640       |
|    policy_gradient_loss | -0.0244     |
|    std                  | 5.3         |
|    value_loss           | 0.0113      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 916         |
|    ep_rew_mean          | 7.57e+03    |
| time/                   |             |
|    fps                  | 5448        |
|    iterations           | 1470        |
|    time_elapsed         | 8840        |
|    total_timesteps      | 48168960    |
| train/                  |             |
|    approx_kl            | 0.019536372 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.271      |
|    n_updates            | 14690       |
|    policy_gradient_loss | -0.024      |
|    std                  | 5.33        |
|    value_loss           | 0.0102      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 943         |
|    ep_rew_mean          | 7.73e+03    |
| time/                   |             |
|    fps                  | 5449        |
|    iterations           | 1480        |
|    time_elapsed         | 8899        |
|    total_timesteps      | 48496640    |
| train/                  |             |
|    approx_kl            | 0.019402336 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.27       |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 5.4         |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 902         |
|    ep_rew_mean          | 7.79e+03    |
| time/                   |             |
|    fps                  | 5449        |
|    iterations           | 1490        |
|    time_elapsed         | 8959        |
|    total_timesteps      | 48824320    |
| train/                  |             |
|    approx_kl            | 0.019043691 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.271      |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.0223     |
|    std                  | 5.48        |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=49000000, episode_reward=11221.10 +/- 4411.65
Episode length: 1341.20 +/- 815.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.34e+03    |
|    mean_reward          | 1.12e+04    |
| time/                   |             |
|    total_timesteps      | 49000000    |
| train/                  |             |
|    approx_kl            | 0.019090392 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 14950       |
|    policy_gradient_loss | -0.023      |
|    std                  | 5.56        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 865         |
|    ep_rew_mean          | 7.39e+03    |
| time/                   |             |
|    fps                  | 5446        |
|    iterations           | 1500        |
|    time_elapsed         | 9023        |
|    total_timesteps      | 49152000    |
| train/                  |             |
|    approx_kl            | 0.019843493 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 14990       |
|    policy_gradient_loss | -0.0238     |
|    std                  | 5.58        |
|    value_loss           | 0.0121      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 978         |
|    ep_rew_mean          | 7.94e+03    |
| time/                   |             |
|    fps                  | 5447        |
|    iterations           | 1510        |
|    time_elapsed         | 9083        |
|    total_timesteps      | 49479680    |
| train/                  |             |
|    approx_kl            | 0.019961996 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.9       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 15090       |
|    policy_gradient_loss | -0.0235     |
|    std                  | 5.68        |
|    value_loss           | 0.00976     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 886         |
|    ep_rew_mean          | 7.79e+03    |
| time/                   |             |
|    fps                  | 5447        |
|    iterations           | 1520        |
|    time_elapsed         | 9142        |
|    total_timesteps      | 49807360    |
| train/                  |             |
|    approx_kl            | 0.018096827 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47         |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.0217     |
|    std                  | 5.76        |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=50000000, episode_reward=10937.11 +/- 5138.38
Episode length: 1341.20 +/- 1020.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.34e+03    |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 50000000    |
| train/                  |             |
|    approx_kl            | 0.020066135 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 15250       |
|    policy_gradient_loss | -0.0221     |
|    std                  | 5.81        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 716         |
|    ep_rew_mean          | 6.76e+03    |
| time/                   |             |
|    fps                  | 5445        |
|    iterations           | 1530        |
|    time_elapsed         | 9206        |
|    total_timesteps      | 50135040    |
| train/                  |             |
|    approx_kl            | 0.017619092 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.0215     |
|    std                  | 5.83        |
|    value_loss           | 0.015       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 973        |
|    ep_rew_mean          | 8.37e+03   |
| time/                   |            |
|    fps                  | 5445       |
|    iterations           | 1540       |
|    time_elapsed         | 9266       |
|    total_timesteps      | 50462720   |
| train/                  |            |
|    approx_kl            | 0.01830336 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -47.4      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.275     |
|    n_updates            | 15390      |
|    policy_gradient_loss | -0.0224    |
|    std                  | 5.96       |
|    value_loss           | 0.0112     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 804         |
|    ep_rew_mean          | 7.35e+03    |
| time/                   |             |
|    fps                  | 5446        |
|    iterations           | 1550        |
|    time_elapsed         | 9325        |
|    total_timesteps      | 50790400    |
| train/                  |             |
|    approx_kl            | 0.018203603 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.274      |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0213     |
|    std                  | 6.05        |
|    value_loss           | 0.0135      |
-----------------------------------------
Eval num_timesteps=51000000, episode_reward=12213.73 +/- 2932.64
Episode length: 1580.60 +/- 620.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.58e+03   |
|    mean_reward          | 1.22e+04   |
| time/                   |            |
|    total_timesteps      | 51000000   |
| train/                  |            |
|    approx_kl            | 0.01744591 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -47.7      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.273     |
|    n_updates            | 15560      |
|    policy_gradient_loss | -0.0212    |
|    std                  | 6.12       |
|    value_loss           | 0.0158     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 854         |
|    ep_rew_mean          | 7.16e+03    |
| time/                   |             |
|    fps                  | 5443        |
|    iterations           | 1560        |
|    time_elapsed         | 9391        |
|    total_timesteps      | 51118080    |
| train/                  |             |
|    approx_kl            | 0.017539509 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15590       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 6.15        |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 858         |
|    ep_rew_mean          | 7.21e+03    |
| time/                   |             |
|    fps                  | 5443        |
|    iterations           | 1570        |
|    time_elapsed         | 9450        |
|    total_timesteps      | 51445760    |
| train/                  |             |
|    approx_kl            | 0.020229243 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48         |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.282      |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.0243     |
|    std                  | 6.22        |
|    value_loss           | 0.0114      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 733         |
|    ep_rew_mean          | 6.87e+03    |
| time/                   |             |
|    fps                  | 5444        |
|    iterations           | 1580        |
|    time_elapsed         | 9509        |
|    total_timesteps      | 51773440    |
| train/                  |             |
|    approx_kl            | 0.019048594 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.2       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.28       |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 6.34        |
|    value_loss           | 0.0159      |
-----------------------------------------
Eval num_timesteps=52000000, episode_reward=10029.07 +/- 4584.84
Episode length: 1037.40 +/- 760.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.04e+03    |
|    mean_reward          | 1e+04       |
| time/                   |             |
|    total_timesteps      | 52000000    |
| train/                  |             |
|    approx_kl            | 0.018510869 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 15860       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 6.41        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 943         |
|    ep_rew_mean          | 8.06e+03    |
| time/                   |             |
|    fps                  | 5442        |
|    iterations           | 1590        |
|    time_elapsed         | 9573        |
|    total_timesteps      | 52101120    |
| train/                  |             |
|    approx_kl            | 0.019479886 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.0215     |
|    std                  | 6.43        |
|    value_loss           | 0.011       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 877        |
|    ep_rew_mean          | 8.09e+03   |
| time/                   |            |
|    fps                  | 5442       |
|    iterations           | 1600       |
|    time_elapsed         | 9632       |
|    total_timesteps      | 52428800   |
| train/                  |            |
|    approx_kl            | 0.01878468 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.282     |
|    n_updates            | 15990      |
|    policy_gradient_loss | -0.023     |
|    std                  | 6.53       |
|    value_loss           | 0.0116     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 925        |
|    ep_rew_mean          | 8.01e+03   |
| time/                   |            |
|    fps                  | 5443       |
|    iterations           | 1610       |
|    time_elapsed         | 9691       |
|    total_timesteps      | 52756480   |
| train/                  |            |
|    approx_kl            | 0.01989339 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.7      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.282     |
|    n_updates            | 16090      |
|    policy_gradient_loss | -0.0236    |
|    std                  | 6.64       |
|    value_loss           | 0.011      |
----------------------------------------
Eval num_timesteps=53000000, episode_reward=12930.20 +/- 3579.33
Episode length: 1499.20 +/- 606.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.5e+03     |
|    mean_reward          | 1.29e+04    |
| time/                   |             |
|    total_timesteps      | 53000000    |
| train/                  |             |
|    approx_kl            | 0.018348213 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.282      |
|    n_updates            | 16170       |
|    policy_gradient_loss | -0.0219     |
|    std                  | 6.7         |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 852         |
|    ep_rew_mean          | 7.47e+03    |
| time/                   |             |
|    fps                  | 5440        |
|    iterations           | 1620        |
|    time_elapsed         | 9756        |
|    total_timesteps      | 53084160    |
| train/                  |             |
|    approx_kl            | 0.018135322 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.28       |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.0218     |
|    std                  | 6.7         |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 737         |
|    ep_rew_mean          | 6.98e+03    |
| time/                   |             |
|    fps                  | 5441        |
|    iterations           | 1630        |
|    time_elapsed         | 9816        |
|    total_timesteps      | 53411840    |
| train/                  |             |
|    approx_kl            | 0.018482279 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.282      |
|    n_updates            | 16290       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 6.79        |
|    value_loss           | 0.0139      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 752        |
|    ep_rew_mean          | 7.17e+03   |
| time/                   |            |
|    fps                  | 5441       |
|    iterations           | 1640       |
|    time_elapsed         | 9875       |
|    total_timesteps      | 53739520   |
| train/                  |            |
|    approx_kl            | 0.01862363 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -49.2      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.283     |
|    n_updates            | 16390      |
|    policy_gradient_loss | -0.0221    |
|    std                  | 6.91       |
|    value_loss           | 0.0125     |
----------------------------------------
Eval num_timesteps=54000000, episode_reward=13301.25 +/- 5462.23
Episode length: 1818.60 +/- 1401.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.82e+03    |
|    mean_reward          | 1.33e+04    |
| time/                   |             |
|    total_timesteps      | 54000000    |
| train/                  |             |
|    approx_kl            | 0.020526867 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.4       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 16470       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 7           |
|    value_loss           | 0.0102      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 874         |
|    ep_rew_mean          | 7.68e+03    |
| time/                   |             |
|    fps                  | 5438        |
|    iterations           | 1650        |
|    time_elapsed         | 9941        |
|    total_timesteps      | 54067200    |
| train/                  |             |
|    approx_kl            | 0.018918432 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.284      |
|    n_updates            | 16490       |
|    policy_gradient_loss | -0.022      |
|    std                  | 7.02        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 987         |
|    ep_rew_mean          | 8.28e+03    |
| time/                   |             |
|    fps                  | 5438        |
|    iterations           | 1660        |
|    time_elapsed         | 10001       |
|    total_timesteps      | 54394880    |
| train/                  |             |
|    approx_kl            | 0.018196119 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.285      |
|    n_updates            | 16590       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 7.14        |
|    value_loss           | 0.0122      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 847         |
|    ep_rew_mean          | 7.57e+03    |
| time/                   |             |
|    fps                  | 5439        |
|    iterations           | 1670        |
|    time_elapsed         | 10060       |
|    total_timesteps      | 54722560    |
| train/                  |             |
|    approx_kl            | 0.018974518 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.285      |
|    n_updates            | 16690       |
|    policy_gradient_loss | -0.022      |
|    std                  | 7.24        |
|    value_loss           | 0.0162      |
-----------------------------------------
Eval num_timesteps=55000000, episode_reward=8808.61 +/- 3414.45
Episode length: 733.40 +/- 444.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 733         |
|    mean_reward          | 8.81e+03    |
| time/                   |             |
|    total_timesteps      | 55000000    |
| train/                  |             |
|    approx_kl            | 0.019779213 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 7.37        |
|    value_loss           | 0.0179      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 638         |
|    ep_rew_mean          | 6.54e+03    |
| time/                   |             |
|    fps                  | 5438        |
|    iterations           | 1680        |
|    time_elapsed         | 10122       |
|    total_timesteps      | 55050240    |
| train/                  |             |
|    approx_kl            | 0.019055896 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50         |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.285      |
|    n_updates            | 16790       |
|    policy_gradient_loss | -0.0218     |
|    std                  | 7.39        |
|    value_loss           | 0.0198      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 770         |
|    ep_rew_mean          | 7.34e+03    |
| time/                   |             |
|    fps                  | 5438        |
|    iterations           | 1690        |
|    time_elapsed         | 10182       |
|    total_timesteps      | 55377920    |
| train/                  |             |
|    approx_kl            | 0.017788656 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.2       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.29       |
|    n_updates            | 16890       |
|    policy_gradient_loss | -0.0228     |
|    std                  | 7.51        |
|    value_loss           | 0.0111      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 841        |
|    ep_rew_mean          | 7.58e+03   |
| time/                   |            |
|    fps                  | 5439       |
|    iterations           | 1700       |
|    time_elapsed         | 10241      |
|    total_timesteps      | 55705600   |
| train/                  |            |
|    approx_kl            | 0.01888872 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.3      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.291     |
|    n_updates            | 16990      |
|    policy_gradient_loss | -0.0235    |
|    std                  | 7.64       |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=56000000, episode_reward=13488.87 +/- 4601.00
Episode length: 1642.40 +/- 941.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.64e+03    |
|    mean_reward          | 1.35e+04    |
| time/                   |             |
|    total_timesteps      | 56000000    |
| train/                  |             |
|    approx_kl            | 0.018693293 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.291      |
|    n_updates            | 17080       |
|    policy_gradient_loss | -0.023      |
|    std                  | 7.75        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 750         |
|    ep_rew_mean          | 7.34e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1710        |
|    time_elapsed         | 10306       |
|    total_timesteps      | 56033280    |
| train/                  |             |
|    approx_kl            | 0.018718444 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.289      |
|    n_updates            | 17090       |
|    policy_gradient_loss | -0.0218     |
|    std                  | 7.77        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 907         |
|    ep_rew_mean          | 7.72e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 1720        |
|    time_elapsed         | 10366       |
|    total_timesteps      | 56360960    |
| train/                  |             |
|    approx_kl            | 0.019673279 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.6       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 17190       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 7.9         |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1e+03       |
|    ep_rew_mean          | 8.43e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 1730        |
|    time_elapsed         | 10425       |
|    total_timesteps      | 56688640    |
| train/                  |             |
|    approx_kl            | 0.021281028 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0241     |
|    std                  | 8           |
|    value_loss           | 0.0133      |
-----------------------------------------
Eval num_timesteps=57000000, episode_reward=11349.94 +/- 4146.92
Episode length: 1158.40 +/- 600.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.16e+03    |
|    mean_reward          | 1.13e+04    |
| time/                   |             |
|    total_timesteps      | 57000000    |
| train/                  |             |
|    approx_kl            | 0.017872285 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.289      |
|    n_updates            | 17390       |
|    policy_gradient_loss | -0.0213     |
|    std                  | 8.13        |
|    value_loss           | 0.0154      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 863      |
|    ep_rew_mean     | 8.15e+03 |
| time/              |          |
|    fps             | 5435     |
|    iterations      | 1740     |
|    time_elapsed    | 10488    |
|    total_timesteps | 57016320 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 783         |
|    ep_rew_mean          | 7.43e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1750        |
|    time_elapsed         | 10547       |
|    total_timesteps      | 57344000    |
| train/                  |             |
|    approx_kl            | 0.020134477 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 17490       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 8.26        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 842         |
|    ep_rew_mean          | 7.87e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 1760        |
|    time_elapsed         | 10606       |
|    total_timesteps      | 57671680    |
| train/                  |             |
|    approx_kl            | 0.021426883 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.298      |
|    n_updates            | 17590       |
|    policy_gradient_loss | -0.0245     |
|    std                  | 8.4         |
|    value_loss           | 0.0144      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 687         |
|    ep_rew_mean          | 6.99e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 1770        |
|    time_elapsed         | 10665       |
|    total_timesteps      | 57999360    |
| train/                  |             |
|    approx_kl            | 0.019189637 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.0227     |
|    std                  | 8.53        |
|    value_loss           | 0.0174      |
-----------------------------------------
Eval num_timesteps=58000000, episode_reward=10465.15 +/- 3546.08
Episode length: 1027.80 +/- 528.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.03e+03   |
|    mean_reward          | 1.05e+04   |
| time/                   |            |
|    total_timesteps      | 58000000   |
| train/                  |            |
|    approx_kl            | 0.02035445 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -51.6      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 17700      |
|    policy_gradient_loss | -0.0225    |
|    std                  | 8.54       |
|    value_loss           | 0.0152     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 777         |
|    ep_rew_mean          | 7.76e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1780        |
|    time_elapsed         | 10728       |
|    total_timesteps      | 58327040    |
| train/                  |             |
|    approx_kl            | 0.020507125 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.298      |
|    n_updates            | 17790       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 8.65        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 746         |
|    ep_rew_mean          | 7.26e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1790        |
|    time_elapsed         | 10788       |
|    total_timesteps      | 58654720    |
| train/                  |             |
|    approx_kl            | 0.019679714 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.298      |
|    n_updates            | 17890       |
|    policy_gradient_loss | -0.023      |
|    std                  | 8.75        |
|    value_loss           | 0.0139      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 705        |
|    ep_rew_mean          | 7.16e+03   |
| time/                   |            |
|    fps                  | 5437       |
|    iterations           | 1800       |
|    time_elapsed         | 10847      |
|    total_timesteps      | 58982400   |
| train/                  |            |
|    approx_kl            | 0.01996286 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.298     |
|    n_updates            | 17990      |
|    policy_gradient_loss | -0.023     |
|    std                  | 8.88       |
|    value_loss           | 0.0146     |
----------------------------------------
Eval num_timesteps=59000000, episode_reward=10150.43 +/- 3460.67
Episode length: 921.00 +/- 459.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 921        |
|    mean_reward          | 1.02e+04   |
| time/                   |            |
|    total_timesteps      | 59000000   |
| train/                  |            |
|    approx_kl            | 0.01866502 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.1      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 18000      |
|    policy_gradient_loss | -0.022     |
|    std                  | 8.89       |
|    value_loss           | 0.0171     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 633         |
|    ep_rew_mean          | 6.27e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1810        |
|    time_elapsed         | 10910       |
|    total_timesteps      | 59310080    |
| train/                  |             |
|    approx_kl            | 0.018197896 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.2       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18090       |
|    policy_gradient_loss | -0.0213     |
|    std                  | 9.02        |
|    value_loss           | 0.0199      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 575         |
|    ep_rew_mean          | 6.3e+03     |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1820        |
|    time_elapsed         | 10969       |
|    total_timesteps      | 59637760    |
| train/                  |             |
|    approx_kl            | 0.017345037 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 9.17        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 708         |
|    ep_rew_mean          | 7.37e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 1830        |
|    time_elapsed         | 11028       |
|    total_timesteps      | 59965440    |
| train/                  |             |
|    approx_kl            | 0.016389668 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.5       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 9.29        |
|    value_loss           | 0.0187      |
-----------------------------------------
Eval num_timesteps=60000000, episode_reward=10932.81 +/- 4009.58
Episode length: 1125.80 +/- 608.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 60000000    |
| train/                  |             |
|    approx_kl            | 0.020431647 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.302      |
|    n_updates            | 18310       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 9.32        |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 742         |
|    ep_rew_mean          | 7.3e+03     |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1840        |
|    time_elapsed         | 11092       |
|    total_timesteps      | 60293120    |
| train/                  |             |
|    approx_kl            | 0.019450843 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.301      |
|    n_updates            | 18390       |
|    policy_gradient_loss | -0.0231     |
|    std                  | 9.45        |
|    value_loss           | 0.0143      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 660         |
|    ep_rew_mean          | 6.85e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1850        |
|    time_elapsed         | 11151       |
|    total_timesteps      | 60620800    |
| train/                  |             |
|    approx_kl            | 0.020210005 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 18490       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 9.59        |
|    value_loss           | 0.0151      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 773         |
|    ep_rew_mean          | 7.17e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1860        |
|    time_elapsed         | 11210       |
|    total_timesteps      | 60948480    |
| train/                  |             |
|    approx_kl            | 0.018326867 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.301      |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 9.76        |
|    value_loss           | 0.0152      |
-----------------------------------------
Eval num_timesteps=61000000, episode_reward=10832.92 +/- 4668.80
Episode length: 1204.40 +/- 554.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.2e+03     |
|    mean_reward          | 1.08e+04    |
| time/                   |             |
|    total_timesteps      | 61000000    |
| train/                  |             |
|    approx_kl            | 0.020942857 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 18610       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 9.8         |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 584         |
|    ep_rew_mean          | 6.22e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1870        |
|    time_elapsed         | 11274       |
|    total_timesteps      | 61276160    |
| train/                  |             |
|    approx_kl            | 0.020438245 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.2       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 18690       |
|    policy_gradient_loss | -0.0228     |
|    std                  | 9.94        |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 783         |
|    ep_rew_mean          | 7.25e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1880        |
|    time_elapsed         | 11333       |
|    total_timesteps      | 61603840    |
| train/                  |             |
|    approx_kl            | 0.020915028 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.4       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.308      |
|    n_updates            | 18790       |
|    policy_gradient_loss | -0.024      |
|    std                  | 10.1        |
|    value_loss           | 0.0144      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 693         |
|    ep_rew_mean          | 7.11e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1890        |
|    time_elapsed         | 11392       |
|    total_timesteps      | 61931520    |
| train/                  |             |
|    approx_kl            | 0.019177299 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 18890       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 10.2        |
|    value_loss           | 0.0141      |
-----------------------------------------
Eval num_timesteps=62000000, episode_reward=9108.22 +/- 4033.67
Episode length: 814.00 +/- 567.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | 9.11e+03    |
| time/                   |             |
|    total_timesteps      | 62000000    |
| train/                  |             |
|    approx_kl            | 0.019258203 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.6       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.305      |
|    n_updates            | 18920       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 10.3        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 703         |
|    ep_rew_mean          | 7.15e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1900        |
|    time_elapsed         | 11454       |
|    total_timesteps      | 62259200    |
| train/                  |             |
|    approx_kl            | 0.021085478 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 18990       |
|    policy_gradient_loss | -0.0232     |
|    std                  | 10.4        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 723         |
|    ep_rew_mean          | 7.42e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1910        |
|    time_elapsed         | 11513       |
|    total_timesteps      | 62586880    |
| train/                  |             |
|    approx_kl            | 0.020046568 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.023      |
|    std                  | 10.6        |
|    value_loss           | 0.0142      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 611         |
|    ep_rew_mean          | 6.47e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 1920        |
|    time_elapsed         | 11572       |
|    total_timesteps      | 62914560    |
| train/                  |             |
|    approx_kl            | 0.020549124 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.1       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.309      |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.023      |
|    std                  | 10.7        |
|    value_loss           | 0.019       |
-----------------------------------------
Eval num_timesteps=63000000, episode_reward=13336.62 +/- 806.37
Episode length: 1392.00 +/- 200.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.39e+03    |
|    mean_reward          | 1.33e+04    |
| time/                   |             |
|    total_timesteps      | 63000000    |
| train/                  |             |
|    approx_kl            | 0.019209439 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19220       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 10.7        |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 788         |
|    ep_rew_mean          | 7.61e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 1930        |
|    time_elapsed         | 11637       |
|    total_timesteps      | 63242240    |
| train/                  |             |
|    approx_kl            | 0.021537405 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19290       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 10.8        |
|    value_loss           | 0.0115      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 701        |
|    ep_rew_mean          | 6.97e+03   |
| time/                   |            |
|    fps                  | 5434       |
|    iterations           | 1940       |
|    time_elapsed         | 11696      |
|    total_timesteps      | 63569920   |
| train/                  |            |
|    approx_kl            | 0.01809825 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -54.3      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.306     |
|    n_updates            | 19390      |
|    policy_gradient_loss | -0.021     |
|    std                  | 10.9       |
|    value_loss           | 0.0149     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 846         |
|    ep_rew_mean          | 7.64e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1950        |
|    time_elapsed         | 11755       |
|    total_timesteps      | 63897600    |
| train/                  |             |
|    approx_kl            | 0.020438815 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.308      |
|    n_updates            | 19490       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 11.1        |
|    value_loss           | 0.0151      |
-----------------------------------------
Eval num_timesteps=64000000, episode_reward=7695.55 +/- 4736.72
Episode length: 753.20 +/- 575.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 753         |
|    mean_reward          | 7.7e+03     |
| time/                   |             |
|    total_timesteps      | 64000000    |
| train/                  |             |
|    approx_kl            | 0.020566847 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.5       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19530       |
|    policy_gradient_loss | -0.0219     |
|    std                  | 11.2        |
|    value_loss           | 0.0199      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 575         |
|    ep_rew_mean          | 6.27e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 1960        |
|    time_elapsed         | 11817       |
|    total_timesteps      | 64225280    |
| train/                  |             |
|    approx_kl            | 0.019345123 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.308      |
|    n_updates            | 19590       |
|    policy_gradient_loss | -0.022      |
|    std                  | 11.3        |
|    value_loss           | 0.0168      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 673         |
|    ep_rew_mean          | 6.66e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1970        |
|    time_elapsed         | 11876       |
|    total_timesteps      | 64552960    |
| train/                  |             |
|    approx_kl            | 0.017334715 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.8       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.308      |
|    n_updates            | 19690       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 11.5        |
|    value_loss           | 0.0193      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 732         |
|    ep_rew_mean          | 7.04e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 1980        |
|    time_elapsed         | 11935       |
|    total_timesteps      | 64880640    |
| train/                  |             |
|    approx_kl            | 0.021297919 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 19790       |
|    policy_gradient_loss | -0.0242     |
|    std                  | 11.6        |
|    value_loss           | 0.0128      |
-----------------------------------------
Eval num_timesteps=65000000, episode_reward=12153.39 +/- 3530.11
Episode length: 1238.20 +/- 499.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.24e+03    |
|    mean_reward          | 1.22e+04    |
| time/                   |             |
|    total_timesteps      | 65000000    |
| train/                  |             |
|    approx_kl            | 0.018668631 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55         |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 19830       |
|    policy_gradient_loss | -0.023      |
|    std                  | 11.7        |
|    value_loss           | 0.0154      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 633         |
|    ep_rew_mean          | 6.69e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 1990        |
|    time_elapsed         | 11999       |
|    total_timesteps      | 65208320    |
| train/                  |             |
|    approx_kl            | 0.021783754 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.2       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 19890       |
|    policy_gradient_loss | -0.0235     |
|    std                  | 11.9        |
|    value_loss           | 0.0216      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 751         |
|    ep_rew_mean          | 7.51e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2000        |
|    time_elapsed         | 12058       |
|    total_timesteps      | 65536000    |
| train/                  |             |
|    approx_kl            | 0.018629707 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.4       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.315      |
|    n_updates            | 19990       |
|    policy_gradient_loss | -0.0219     |
|    std                  | 12.1        |
|    value_loss           | 0.0152      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 631         |
|    ep_rew_mean          | 6.57e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2010        |
|    time_elapsed         | 12117       |
|    total_timesteps      | 65863680    |
| train/                  |             |
|    approx_kl            | 0.021264557 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.315      |
|    n_updates            | 20090       |
|    policy_gradient_loss | -0.0235     |
|    std                  | 12.3        |
|    value_loss           | 0.0179      |
-----------------------------------------
Eval num_timesteps=66000000, episode_reward=10257.03 +/- 3811.29
Episode length: 957.00 +/- 511.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 957         |
|    mean_reward          | 1.03e+04    |
| time/                   |             |
|    total_timesteps      | 66000000    |
| train/                  |             |
|    approx_kl            | 0.018933829 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.315      |
|    n_updates            | 20140       |
|    policy_gradient_loss | -0.0219     |
|    std                  | 12.4        |
|    value_loss           | 0.019       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 707         |
|    ep_rew_mean          | 7.3e+03     |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2020        |
|    time_elapsed         | 12179       |
|    total_timesteps      | 66191360    |
| train/                  |             |
|    approx_kl            | 0.019431721 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.318      |
|    n_updates            | 20190       |
|    policy_gradient_loss | -0.023      |
|    std                  | 12.5        |
|    value_loss           | 0.0147      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 705         |
|    ep_rew_mean          | 6.75e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2030        |
|    time_elapsed         | 12238       |
|    total_timesteps      | 66519040    |
| train/                  |             |
|    approx_kl            | 0.020983618 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.318      |
|    n_updates            | 20290       |
|    policy_gradient_loss | -0.0237     |
|    std                  | 12.7        |
|    value_loss           | 0.0193      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 719         |
|    ep_rew_mean          | 7.35e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2040        |
|    time_elapsed         | 12297       |
|    total_timesteps      | 66846720    |
| train/                  |             |
|    approx_kl            | 0.018812586 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.1       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.317      |
|    n_updates            | 20390       |
|    policy_gradient_loss | -0.0218     |
|    std                  | 12.9        |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=67000000, episode_reward=9972.00 +/- 3662.79
Episode length: 886.40 +/- 441.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 886        |
|    mean_reward          | 9.97e+03   |
| time/                   |            |
|    total_timesteps      | 67000000   |
| train/                  |            |
|    approx_kl            | 0.01869031 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56.2      |
|    explained_variance   | 0.976      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20440      |
|    policy_gradient_loss | -0.0215    |
|    std                  | 13         |
|    value_loss           | 0.015      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 707         |
|    ep_rew_mean          | 7.03e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2050        |
|    time_elapsed         | 12359       |
|    total_timesteps      | 67174400    |
| train/                  |             |
|    approx_kl            | 0.023924092 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.2       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20490       |
|    policy_gradient_loss | -0.0243     |
|    std                  | 13.1        |
|    value_loss           | 0.0172      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 580        |
|    ep_rew_mean          | 6.09e+03   |
| time/                   |            |
|    fps                  | 5435       |
|    iterations           | 2060       |
|    time_elapsed         | 12418      |
|    total_timesteps      | 67502080   |
| train/                  |            |
|    approx_kl            | 0.01926855 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56.4      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20590      |
|    policy_gradient_loss | -0.0224    |
|    std                  | 13.3       |
|    value_loss           | 0.0181     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 694         |
|    ep_rew_mean          | 7.29e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2070        |
|    time_elapsed         | 12477       |
|    total_timesteps      | 67829760    |
| train/                  |             |
|    approx_kl            | 0.020111296 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20690       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 13.5        |
|    value_loss           | 0.0144      |
-----------------------------------------
Eval num_timesteps=68000000, episode_reward=11064.78 +/- 2687.19
Episode length: 1146.80 +/- 451.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.15e+03    |
|    mean_reward          | 1.11e+04    |
| time/                   |             |
|    total_timesteps      | 68000000    |
| train/                  |             |
|    approx_kl            | 0.023105722 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.321      |
|    n_updates            | 20750       |
|    policy_gradient_loss | -0.0238     |
|    std                  | 13.6        |
|    value_loss           | 0.0165      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 771         |
|    ep_rew_mean          | 7.47e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2080        |
|    time_elapsed         | 12540       |
|    total_timesteps      | 68157440    |
| train/                  |             |
|    approx_kl            | 0.023438549 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 20790       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 13.7        |
|    value_loss           | 0.018       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 784        |
|    ep_rew_mean          | 7.81e+03   |
| time/                   |            |
|    fps                  | 5435       |
|    iterations           | 2090       |
|    time_elapsed         | 12599      |
|    total_timesteps      | 68485120   |
| train/                  |            |
|    approx_kl            | 0.01948317 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.32      |
|    n_updates            | 20890      |
|    policy_gradient_loss | -0.0221    |
|    std                  | 14         |
|    value_loss           | 0.0139     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 626         |
|    ep_rew_mean          | 6.64e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2100        |
|    time_elapsed         | 12658       |
|    total_timesteps      | 68812800    |
| train/                  |             |
|    approx_kl            | 0.021548048 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57         |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20990       |
|    policy_gradient_loss | -0.022      |
|    std                  | 14.2        |
|    value_loss           | 0.0192      |
-----------------------------------------
Eval num_timesteps=69000000, episode_reward=10425.37 +/- 5162.84
Episode length: 1169.40 +/- 598.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.17e+03    |
|    mean_reward          | 1.04e+04    |
| time/                   |             |
|    total_timesteps      | 69000000    |
| train/                  |             |
|    approx_kl            | 0.020648502 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.2       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21050       |
|    policy_gradient_loss | -0.0224     |
|    std                  | 14.3        |
|    value_loss           | 0.0164      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 698        |
|    ep_rew_mean          | 7.48e+03   |
| time/                   |            |
|    fps                  | 5434       |
|    iterations           | 2110       |
|    time_elapsed         | 12722      |
|    total_timesteps      | 69140480   |
| train/                  |            |
|    approx_kl            | 0.02256054 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.324     |
|    n_updates            | 21090      |
|    policy_gradient_loss | -0.0232    |
|    std                  | 14.4       |
|    value_loss           | 0.0138     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 744         |
|    ep_rew_mean          | 7.29e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2120        |
|    time_elapsed         | 12781       |
|    total_timesteps      | 69468160    |
| train/                  |             |
|    approx_kl            | 0.020360183 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.4       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21190       |
|    policy_gradient_loss | -0.0221     |
|    std                  | 14.5        |
|    value_loss           | 0.0191      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 624         |
|    ep_rew_mean          | 6.79e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2130        |
|    time_elapsed         | 12840       |
|    total_timesteps      | 69795840    |
| train/                  |             |
|    approx_kl            | 0.018735934 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.5       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21290       |
|    policy_gradient_loss | -0.021      |
|    std                  | 14.8        |
|    value_loss           | 0.0181      |
-----------------------------------------
Eval num_timesteps=70000000, episode_reward=6870.62 +/- 4050.05
Episode length: 914.60 +/- 873.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 915         |
|    mean_reward          | 6.87e+03    |
| time/                   |             |
|    total_timesteps      | 70000000    |
| train/                  |             |
|    approx_kl            | 0.019008357 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 21360       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 14.9        |
|    value_loss           | 0.0173      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 668         |
|    ep_rew_mean          | 7.07e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2140        |
|    time_elapsed         | 12903       |
|    total_timesteps      | 70123520    |
| train/                  |             |
|    approx_kl            | 0.021900931 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.6       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21390       |
|    policy_gradient_loss | -0.024      |
|    std                  | 15          |
|    value_loss           | 0.0167      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 721         |
|    ep_rew_mean          | 7.07e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2150        |
|    time_elapsed         | 12962       |
|    total_timesteps      | 70451200    |
| train/                  |             |
|    approx_kl            | 0.020996243 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.8       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.326      |
|    n_updates            | 21490       |
|    policy_gradient_loss | -0.0228     |
|    std                  | 15.3        |
|    value_loss           | 0.0165      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 696        |
|    ep_rew_mean          | 7.21e+03   |
| time/                   |            |
|    fps                  | 5435       |
|    iterations           | 2160       |
|    time_elapsed         | 13021      |
|    total_timesteps      | 70778880   |
| train/                  |            |
|    approx_kl            | 0.02180898 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -58        |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.329     |
|    n_updates            | 21590      |
|    policy_gradient_loss | -0.0234    |
|    std                  | 15.6       |
|    value_loss           | 0.0158     |
----------------------------------------
Eval num_timesteps=71000000, episode_reward=9738.06 +/- 4421.17
Episode length: 1000.20 +/- 557.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | 9.74e+03    |
| time/                   |             |
|    total_timesteps      | 71000000    |
| train/                  |             |
|    approx_kl            | 0.020603754 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.2       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21660       |
|    policy_gradient_loss | -0.0228     |
|    std                  | 15.7        |
|    value_loss           | 0.0183      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 611        |
|    ep_rew_mean          | 6.58e+03   |
| time/                   |            |
|    fps                  | 5434       |
|    iterations           | 2170       |
|    time_elapsed         | 13083      |
|    total_timesteps      | 71106560   |
| train/                  |            |
|    approx_kl            | 0.02052362 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -58.2      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.326     |
|    n_updates            | 21690      |
|    policy_gradient_loss | -0.0223    |
|    std                  | 15.8       |
|    value_loss           | 0.0201     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 683         |
|    ep_rew_mean          | 7.41e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2180        |
|    time_elapsed         | 13142       |
|    total_timesteps      | 71434240    |
| train/                  |             |
|    approx_kl            | 0.020641902 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.331      |
|    n_updates            | 21790       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 16.1        |
|    value_loss           | 0.016       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 803         |
|    ep_rew_mean          | 7.18e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2190        |
|    time_elapsed         | 13201       |
|    total_timesteps      | 71761920    |
| train/                  |             |
|    approx_kl            | 0.022322312 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.331      |
|    n_updates            | 21890       |
|    policy_gradient_loss | -0.0232     |
|    std                  | 16.5        |
|    value_loss           | 0.0155      |
-----------------------------------------
Eval num_timesteps=72000000, episode_reward=9725.64 +/- 5555.24
Episode length: 920.80 +/- 557.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 921         |
|    mean_reward          | 9.73e+03    |
| time/                   |             |
|    total_timesteps      | 72000000    |
| train/                  |             |
|    approx_kl            | 0.022103414 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.332      |
|    n_updates            | 21970       |
|    policy_gradient_loss | -0.024      |
|    std                  | 16.6        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 719         |
|    ep_rew_mean          | 7.46e+03    |
| time/                   |             |
|    fps                  | 5434        |
|    iterations           | 2200        |
|    time_elapsed         | 13264       |
|    total_timesteps      | 72089600    |
| train/                  |             |
|    approx_kl            | 0.019780546 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.331      |
|    n_updates            | 21990       |
|    policy_gradient_loss | -0.0222     |
|    std                  | 16.7        |
|    value_loss           | 0.0162      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 727         |
|    ep_rew_mean          | 7.36e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2210        |
|    time_elapsed         | 13323       |
|    total_timesteps      | 72417280    |
| train/                  |             |
|    approx_kl            | 0.020229686 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.332      |
|    n_updates            | 22090       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 17          |
|    value_loss           | 0.0164      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 647         |
|    ep_rew_mean          | 6.73e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2220        |
|    time_elapsed         | 13382       |
|    total_timesteps      | 72744960    |
| train/                  |             |
|    approx_kl            | 0.022177294 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.334      |
|    n_updates            | 22190       |
|    policy_gradient_loss | -0.0236     |
|    std                  | 17.3        |
|    value_loss           | 0.0177      |
-----------------------------------------
Eval num_timesteps=73000000, episode_reward=9098.45 +/- 4791.97
Episode length: 844.40 +/- 503.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 844         |
|    mean_reward          | 9.1e+03     |
| time/                   |             |
|    total_timesteps      | 73000000    |
| train/                  |             |
|    approx_kl            | 0.020350795 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.3       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.331      |
|    n_updates            | 22270       |
|    policy_gradient_loss | -0.0214     |
|    std                  | 17.5        |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 730         |
|    ep_rew_mean          | 7.61e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2230        |
|    time_elapsed         | 13444       |
|    total_timesteps      | 73072640    |
| train/                  |             |
|    approx_kl            | 0.020431612 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22290       |
|    policy_gradient_loss | -0.0225     |
|    std                  | 17.5        |
|    value_loss           | 0.0157      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 660         |
|    ep_rew_mean          | 7.1e+03     |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2240        |
|    time_elapsed         | 13503       |
|    total_timesteps      | 73400320    |
| train/                  |             |
|    approx_kl            | 0.021515254 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.335      |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.0229     |
|    std                  | 17.9        |
|    value_loss           | 0.0185      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 646         |
|    ep_rew_mean          | 6.96e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2250        |
|    time_elapsed         | 13561       |
|    total_timesteps      | 73728000    |
| train/                  |             |
|    approx_kl            | 0.022505073 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.339      |
|    n_updates            | 22490       |
|    policy_gradient_loss | -0.0245     |
|    std                  | 18.1        |
|    value_loss           | 0.0172      |
-----------------------------------------
Eval num_timesteps=74000000, episode_reward=10255.41 +/- 3403.49
Episode length: 867.00 +/- 401.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 867         |
|    mean_reward          | 1.03e+04    |
| time/                   |             |
|    total_timesteps      | 74000000    |
| train/                  |             |
|    approx_kl            | 0.022148862 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.34       |
|    n_updates            | 22580       |
|    policy_gradient_loss | -0.0243     |
|    std                  | 18.4        |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 678         |
|    ep_rew_mean          | 6.89e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2260        |
|    time_elapsed         | 13624       |
|    total_timesteps      | 74055680    |
| train/                  |             |
|    approx_kl            | 0.023884565 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 22590       |
|    policy_gradient_loss | -0.0251     |
|    std                  | 18.4        |
|    value_loss           | 0.0164      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 713        |
|    ep_rew_mean          | 7.1e+03    |
| time/                   |            |
|    fps                  | 5436       |
|    iterations           | 2270       |
|    time_elapsed         | 13682      |
|    total_timesteps      | 74383360   |
| train/                  |            |
|    approx_kl            | 0.02273764 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.9      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.337     |
|    n_updates            | 22690      |
|    policy_gradient_loss | -0.0234    |
|    std                  | 18.6       |
|    value_loss           | 0.0159     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 687         |
|    ep_rew_mean          | 7.25e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2280        |
|    time_elapsed         | 13741       |
|    total_timesteps      | 74711040    |
| train/                  |             |
|    approx_kl            | 0.023753101 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.338      |
|    n_updates            | 22790       |
|    policy_gradient_loss | -0.0244     |
|    std                  | 18.9        |
|    value_loss           | 0.0152      |
-----------------------------------------
Eval num_timesteps=75000000, episode_reward=13350.63 +/- 1295.73
Episode length: 1326.80 +/- 359.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.33e+03   |
|    mean_reward          | 1.34e+04   |
| time/                   |            |
|    total_timesteps      | 75000000   |
| train/                  |            |
|    approx_kl            | 0.02278867 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -60.2      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.34      |
|    n_updates            | 22880      |
|    policy_gradient_loss | -0.0237    |
|    std                  | 19.1       |
|    value_loss           | 0.0141     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 806         |
|    ep_rew_mean          | 7.92e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2290        |
|    time_elapsed         | 13805       |
|    total_timesteps      | 75038720    |
| train/                  |             |
|    approx_kl            | 0.024957268 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.2       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 22890       |
|    policy_gradient_loss | -0.0248     |
|    std                  | 19.1        |
|    value_loss           | 0.0121      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 670         |
|    ep_rew_mean          | 7.22e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2300        |
|    time_elapsed         | 13864       |
|    total_timesteps      | 75366400    |
| train/                  |             |
|    approx_kl            | 0.020191304 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.339      |
|    n_updates            | 22990       |
|    policy_gradient_loss | -0.0218     |
|    std                  | 19.5        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 677         |
|    ep_rew_mean          | 7.29e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2310        |
|    time_elapsed         | 13923       |
|    total_timesteps      | 75694080    |
| train/                  |             |
|    approx_kl            | 0.023558812 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.342      |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0244     |
|    std                  | 19.7        |
|    value_loss           | 0.017       |
-----------------------------------------
Eval num_timesteps=76000000, episode_reward=13639.34 +/- 1376.87
Episode length: 1414.00 +/- 482.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.41e+03    |
|    mean_reward          | 1.36e+04    |
| time/                   |             |
|    total_timesteps      | 76000000    |
| train/                  |             |
|    approx_kl            | 0.021352366 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.6       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.337      |
|    n_updates            | 23190       |
|    policy_gradient_loss | -0.022      |
|    std                  | 19.9        |
|    value_loss           | 0.0186      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 667      |
|    ep_rew_mean     | 6.93e+03 |
| time/              |          |
|    fps             | 5435     |
|    iterations      | 2320     |
|    time_elapsed    | 13987    |
|    total_timesteps | 76021760 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 738         |
|    ep_rew_mean          | 7.76e+03    |
| time/                   |             |
|    fps                  | 5435        |
|    iterations           | 2330        |
|    time_elapsed         | 14046       |
|    total_timesteps      | 76349440    |
| train/                  |             |
|    approx_kl            | 0.026082113 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.349      |
|    n_updates            | 23290       |
|    policy_gradient_loss | -0.0268     |
|    std                  | 20.2        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 635         |
|    ep_rew_mean          | 6.8e+03     |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2340        |
|    time_elapsed         | 14104       |
|    total_timesteps      | 76677120    |
| train/                  |             |
|    approx_kl            | 0.018548466 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.334      |
|    n_updates            | 23390       |
|    policy_gradient_loss | -0.0177     |
|    std                  | 20.6        |
|    value_loss           | 0.0181      |
-----------------------------------------
Eval num_timesteps=77000000, episode_reward=10869.91 +/- 2583.46
Episode length: 915.80 +/- 321.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 916        |
|    mean_reward          | 1.09e+04   |
| time/                   |            |
|    total_timesteps      | 77000000   |
| train/                  |            |
|    approx_kl            | 0.02240768 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -61.1      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.344     |
|    n_updates            | 23490      |
|    policy_gradient_loss | -0.0233    |
|    std                  | 20.8       |
|    value_loss           | 0.0145     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 662      |
|    ep_rew_mean     | 7.13e+03 |
| time/              |          |
|    fps             | 5435     |
|    iterations      | 2350     |
|    time_elapsed    | 14167    |
|    total_timesteps | 77004800 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 704         |
|    ep_rew_mean          | 7.26e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2360        |
|    time_elapsed         | 14225       |
|    total_timesteps      | 77332480    |
| train/                  |             |
|    approx_kl            | 0.023462862 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.339      |
|    n_updates            | 23590       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 21.2        |
|    value_loss           | 0.0149      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 751         |
|    ep_rew_mean          | 7.68e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2370        |
|    time_elapsed         | 14284       |
|    total_timesteps      | 77660160    |
| train/                  |             |
|    approx_kl            | 0.022314081 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.4       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 23690       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 21.5        |
|    value_loss           | 0.0143      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 807         |
|    ep_rew_mean          | 8.21e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 2380        |
|    time_elapsed         | 14343       |
|    total_timesteps      | 77987840    |
| train/                  |             |
|    approx_kl            | 0.025162674 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.6       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.351      |
|    n_updates            | 23790       |
|    policy_gradient_loss | -0.0253     |
|    std                  | 21.9        |
|    value_loss           | 0.0129      |
-----------------------------------------
Eval num_timesteps=78000000, episode_reward=12053.74 +/- 984.72
Episode length: 1221.40 +/- 295.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.22e+03    |
|    mean_reward          | 1.21e+04    |
| time/                   |             |
|    total_timesteps      | 78000000    |
| train/                  |             |
|    approx_kl            | 0.025707766 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.7       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 23800       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 21.9        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 695         |
|    ep_rew_mean          | 7.29e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2390        |
|    time_elapsed         | 14406       |
|    total_timesteps      | 78315520    |
| train/                  |             |
|    approx_kl            | 0.021518506 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.344      |
|    n_updates            | 23890       |
|    policy_gradient_loss | -0.0217     |
|    std                  | 22.3        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 706         |
|    ep_rew_mean          | 7.46e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2400        |
|    time_elapsed         | 14465       |
|    total_timesteps      | 78643200    |
| train/                  |             |
|    approx_kl            | 0.024113888 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.351      |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.0251     |
|    std                  | 22.5        |
|    value_loss           | 0.0162      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 707         |
|    ep_rew_mean          | 7.59e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2410        |
|    time_elapsed         | 14524       |
|    total_timesteps      | 78970880    |
| train/                  |             |
|    approx_kl            | 0.022868557 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.1       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.35       |
|    n_updates            | 24090       |
|    policy_gradient_loss | -0.0237     |
|    std                  | 22.8        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=79000000, episode_reward=9757.16 +/- 3620.63
Episode length: 941.00 +/- 358.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 941         |
|    mean_reward          | 9.76e+03    |
| time/                   |             |
|    total_timesteps      | 79000000    |
| train/                  |             |
|    approx_kl            | 0.024503183 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 24100       |
|    policy_gradient_loss | -0.0237     |
|    std                  | 22.8        |
|    value_loss           | 0.019       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 622         |
|    ep_rew_mean          | 6.71e+03    |
| time/                   |             |
|    fps                  | 5436        |
|    iterations           | 2420        |
|    time_elapsed         | 14587       |
|    total_timesteps      | 79298560    |
| train/                  |             |
|    approx_kl            | 0.022842187 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.2       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 24190       |
|    policy_gradient_loss | -0.0232     |
|    std                  | 23.2        |
|    value_loss           | 0.0168      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 706        |
|    ep_rew_mean          | 7.46e+03   |
| time/                   |            |
|    fps                  | 5436       |
|    iterations           | 2430       |
|    time_elapsed         | 14646      |
|    total_timesteps      | 79626240   |
| train/                  |            |
|    approx_kl            | 0.02312371 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -62.3      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.348     |
|    n_updates            | 24290      |
|    policy_gradient_loss | -0.0227    |
|    std                  | 23.5       |
|    value_loss           | 0.0168     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 636         |
|    ep_rew_mean          | 6.84e+03    |
| time/                   |             |
|    fps                  | 5437        |
|    iterations           | 2440        |
|    time_elapsed         | 14705       |
|    total_timesteps      | 79953920    |
| train/                  |             |
|    approx_kl            | 0.023037624 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.348      |
|    n_updates            | 24390       |
|    policy_gradient_loss | -0.0231     |
|    std                  | 23.9        |
|    value_loss           | 0.0183      |
-----------------------------------------
Eval num_timesteps=80000000, episode_reward=9473.42 +/- 5122.08
Episode length: 1013.60 +/- 494.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.01e+03    |
|    mean_reward          | 9.47e+03    |
| time/                   |             |
|    total_timesteps      | 80000000    |
| train/                  |             |
|    approx_kl            | 0.023917403 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.348      |
|    n_updates            | 24410       |
|    policy_gradient_loss | -0.0223     |
|    std                  | 23.9        |
|    value_loss           | 0.0182      |
-----------------------------------------

============================================================
Training Complete!
============================================================
Total training time: 4:05:20
Total timesteps: 80,000,000
Timesteps per second: 5434.49
Run directory: /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/17-59-51
============================================================

