
=== Merged Config ===
 exp_name: rl-humanoid
seed: 42
resume_from: null
paths:
  log_root: null
vecnorm:
  enabled: true
  clip_obs: 10.0
  gamma: 0.99
  epsilon: 1.0e-08
env:
  name: HumanoidCircuit-v0
  make_kwargs:
    render_mode: null
    waypoints:
    - - 5.0
      - 0.0
    - - 5.0
      - 5.0
    - - 2.0
      - 5.0
    - - 2.0
      - 0.0
    waypoint_reach_threshold: 1.5
    stairs: []
    terrain_width: 15.0
    progress_reward_weight: 200.0
    waypoint_bonus: 150.0
    circuit_completion_bonus: 500.0
    height_reward_weight: 0.0
    forward_reward_weight: 1.0
    heading_reward_weight: 2.0
    balance_reward_weight: 0.5
    optimal_speed: 1.2
    speed_regulation_weight: 0.2
    ctrl_cost_weight: 0.1
    contact_cost_weight: 5.0e-07
    healthy_reward: 5.0
    terminate_when_unhealthy: true
    healthy_z_range:
    - 0.8
    - 3.0
  vec_env:
    n_envs: 8
    start_method: spawn
    monitor: true
algo:
  policy: MlpPolicy
  device: cuda
  policy_kwargs:
    net_arch:
    - 256
    - 256
  hyperparams:
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 16384
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.005
    vf_coef: 0.25
    max_grad_norm: 0.5
training:
  total_timesteps: 80000000
  log_interval: 10
  checkpoint_every_steps: 250000

Logging to /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/10-37-28
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Using cuda device

=== Starting Training ===
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.4        |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 7239        |
|    iterations           | 10          |
|    time_elapsed         | 45          |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.010297303 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0841     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0132     |
|    std                  | 1.01        |
|    value_loss           | 0.261       |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 54.2         |
|    ep_rew_mean          | 317          |
| time/                   |              |
|    fps                  | 7325         |
|    iterations           | 20           |
|    time_elapsed         | 89           |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0111259725 |
|    clip_fraction        | 0.0805       |
|    clip_range           | 0.2          |
|    entropy_loss         | -24.1        |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.118       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.013       |
|    std                  | 1            |
|    value_loss           | 0.114        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 65.5        |
|    ep_rew_mean          | 387         |
| time/                   |             |
|    fps                  | 7335        |
|    iterations           | 30          |
|    time_elapsed         | 134         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.009535918 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.127      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0128     |
|    std                  | 0.997       |
|    value_loss           | 0.0737      |
-----------------------------------------
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000000, episode_reward=757.33 +/- 176.54
Episode length: 90.60 +/- 11.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.6        |
|    mean_reward          | 757         |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.010435816 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0125     |
|    std                  | 0.995       |
|    value_loss           | 0.0701      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 76.7        |
|    ep_rew_mean          | 442         |
| time/                   |             |
|    fps                  | 7269        |
|    iterations           | 40          |
|    time_elapsed         | 180         |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.009916108 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0131     |
|    std                  | 0.991       |
|    value_loss           | 0.0651      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 79.5        |
|    ep_rew_mean          | 450         |
| time/                   |             |
|    fps                  | 7246        |
|    iterations           | 50          |
|    time_elapsed         | 226         |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.010745604 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.942       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0132     |
|    std                  | 0.989       |
|    value_loss           | 0.076       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 87.6      |
|    ep_rew_mean          | 536       |
| time/                   |           |
|    fps                  | 7252      |
|    iterations           | 60        |
|    time_elapsed         | 271       |
|    total_timesteps      | 1966080   |
| train/                  |           |
|    approx_kl            | 0.0111319 |
|    clip_fraction        | 0.0887    |
|    clip_range           | 0.2       |
|    entropy_loss         | -24       |
|    explained_variance   | 0.944     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.126    |
|    n_updates            | 590       |
|    policy_gradient_loss | -0.0133   |
|    std                  | 0.994     |
|    value_loss           | 0.0819    |
---------------------------------------
Eval num_timesteps=2000000, episode_reward=752.04 +/- 150.41
Episode length: 92.40 +/- 13.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.4        |
|    mean_reward          | 752         |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.010979189 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.124      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.0126     |
|    std                  | 0.993       |
|    value_loss           | 0.0842      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.9         |
|    ep_rew_mean          | 584          |
| time/                   |              |
|    fps                  | 7243         |
|    iterations           | 70           |
|    time_elapsed         | 316          |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0110485945 |
|    clip_fraction        | 0.0926       |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.9        |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.125       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.0137      |
|    std                  | 0.992        |
|    value_loss           | 0.0857       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.1        |
|    ep_rew_mean          | 631         |
| time/                   |             |
|    fps                  | 7223        |
|    iterations           | 80          |
|    time_elapsed         | 362         |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.011202518 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.122      |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0137     |
|    std                  | 0.992       |
|    value_loss           | 0.0967      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 679         |
| time/                   |             |
|    fps                  | 7234        |
|    iterations           | 90          |
|    time_elapsed         | 407         |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.011429442 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.124      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0133     |
|    std                  | 0.992       |
|    value_loss           | 0.0832      |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=759.09 +/- 300.63
Episode length: 87.80 +/- 27.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.8         |
|    mean_reward          | 759          |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0116781145 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.9        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.123       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.0136      |
|    std                  | 0.992        |
|    value_loss           | 0.0912       |
------------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 732         |
| time/                   |             |
|    fps                  | 7236        |
|    iterations           | 100         |
|    time_elapsed         | 452         |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.012336237 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.99        |
|    value_loss           | 0.0761      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 7242        |
|    iterations           | 110         |
|    time_elapsed         | 497         |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.012769887 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0143     |
|    std                  | 0.992       |
|    value_loss           | 0.0816      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 110          |
|    ep_rew_mean          | 810          |
| time/                   |              |
|    fps                  | 7247         |
|    iterations           | 120          |
|    time_elapsed         | 542          |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0125018675 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.127       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0142      |
|    std                  | 0.995        |
|    value_loss           | 0.0738       |
------------------------------------------
Eval num_timesteps=4000000, episode_reward=1035.87 +/- 293.50
Episode length: 111.00 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 1.04e+03    |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.013221728 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 0.996       |
|    value_loss           | 0.0711      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 920         |
| time/                   |             |
|    fps                  | 7245        |
|    iterations           | 130         |
|    time_elapsed         | 587         |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.012887841 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0147     |
|    std                  | 0.998       |
|    value_loss           | 0.0721      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 954         |
| time/                   |             |
|    fps                  | 7249        |
|    iterations           | 140         |
|    time_elapsed         | 632         |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.013577942 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 0.998       |
|    value_loss           | 0.0665      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 7253        |
|    iterations           | 150         |
|    time_elapsed         | 677         |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.014322734 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 1           |
|    value_loss           | 0.0733      |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=1214.06 +/- 372.58
Episode length: 133.80 +/- 27.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 1.21e+03    |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.013276277 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0155     |
|    std                  | 1           |
|    value_loss           | 0.0713      |
-----------------------------------------
New best mean reward!
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 126          |
|    ep_rew_mean          | 1.02e+03     |
| time/                   |              |
|    fps                  | 7252         |
|    iterations           | 160          |
|    time_elapsed         | 722          |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0141287185 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.132       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.0164      |
|    std                  | 1            |
|    value_loss           | 0.0712       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 144         |
|    ep_rew_mean          | 1.26e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 170         |
|    time_elapsed         | 767         |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.013903452 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1           |
|    value_loss           | 0.0671      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 1.25e+03    |
| time/                   |             |
|    fps                  | 7257        |
|    iterations           | 180         |
|    time_elapsed         | 812         |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.014012106 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0173     |
|    std                  | 1           |
|    value_loss           | 0.0641      |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=2258.72 +/- 363.51
Episode length: 198.60 +/- 19.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 2.26e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.014255364 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.134      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1           |
|    value_loss           | 0.0696      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 7255        |
|    iterations           | 190         |
|    time_elapsed         | 858         |
|    total_timesteps      | 6225920     |
| train/                  |             |
|    approx_kl            | 0.015678775 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.01        |
|    value_loss           | 0.0715      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 162          |
|    ep_rew_mean          | 1.56e+03     |
| time/                   |              |
|    fps                  | 7257         |
|    iterations           | 200          |
|    time_elapsed         | 902          |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0151181575 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.8        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.134       |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.0175      |
|    std                  | 1            |
|    value_loss           | 0.0714       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 176         |
|    ep_rew_mean          | 1.75e+03    |
| time/                   |             |
|    fps                  | 7260        |
|    iterations           | 210         |
|    time_elapsed         | 947         |
|    total_timesteps      | 6881280     |
| train/                  |             |
|    approx_kl            | 0.015222359 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.7       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0177     |
|    std                  | 1           |
|    value_loss           | 0.0722      |
-----------------------------------------
Eval num_timesteps=7000000, episode_reward=2035.33 +/- 350.01
Episode length: 185.80 +/- 26.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 2.04e+03   |
| time/                   |            |
|    total_timesteps      | 7000000    |
| train/                  |            |
|    approx_kl            | 0.01533648 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.134     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0177    |
|    std                  | 1          |
|    value_loss           | 0.0679     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 178         |
|    ep_rew_mean          | 1.81e+03    |
| time/                   |             |
|    fps                  | 7259        |
|    iterations           | 220         |
|    time_elapsed         | 993         |
|    total_timesteps      | 7208960     |
| train/                  |             |
|    approx_kl            | 0.015649045 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1           |
|    value_loss           | 0.0653      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 7261        |
|    iterations           | 230         |
|    time_elapsed         | 1037        |
|    total_timesteps      | 7536640     |
| train/                  |             |
|    approx_kl            | 0.015140012 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.01        |
|    value_loss           | 0.0565      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 7264        |
|    iterations           | 240         |
|    time_elapsed         | 1082        |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.015559951 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.02        |
|    value_loss           | 0.0564      |
-----------------------------------------
Eval num_timesteps=8000000, episode_reward=2768.87 +/- 429.91
Episode length: 249.60 +/- 10.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 2.77e+03    |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.016414374 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.02        |
|    value_loss           | 0.0475      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 7263        |
|    iterations           | 250         |
|    time_elapsed         | 1127        |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.015371178 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.143      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.018      |
|    std                  | 1.03        |
|    value_loss           | 0.0459      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 220         |
|    ep_rew_mean          | 2.42e+03    |
| time/                   |             |
|    fps                  | 7266        |
|    iterations           | 260         |
|    time_elapsed         | 1172        |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.016211044 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.143      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.03        |
|    value_loss           | 0.0425      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 228         |
|    ep_rew_mean          | 2.5e+03     |
| time/                   |             |
|    fps                  | 7268        |
|    iterations           | 270         |
|    time_elapsed         | 1217        |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.016020544 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.147      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.04        |
|    value_loss           | 0.0348      |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=2597.78 +/- 1233.31
Episode length: 248.20 +/- 81.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 2.6e+03     |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.015647864 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.2       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.04        |
|    value_loss           | 0.035       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 2.54e+03   |
| time/                   |            |
|    fps                  | 7266       |
|    iterations           | 280        |
|    time_elapsed         | 1262       |
|    total_timesteps      | 9175040    |
| train/                  |            |
|    approx_kl            | 0.01670146 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.146     |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0191    |
|    std                  | 1.04       |
|    value_loss           | 0.0391     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 245         |
|    ep_rew_mean          | 2.7e+03     |
| time/                   |             |
|    fps                  | 7269        |
|    iterations           | 290         |
|    time_elapsed         | 1307        |
|    total_timesteps      | 9502720     |
| train/                  |             |
|    approx_kl            | 0.015250227 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.146      |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 1.05        |
|    value_loss           | 0.0354      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 269       |
|    ep_rew_mean          | 2.82e+03  |
| time/                   |           |
|    fps                  | 7271      |
|    iterations           | 300       |
|    time_elapsed         | 1351      |
|    total_timesteps      | 9830400   |
| train/                  |           |
|    approx_kl            | 0.0162506 |
|    clip_fraction        | 0.133     |
|    clip_range           | 0.2       |
|    entropy_loss         | -24.4     |
|    explained_variance   | 0.976     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.148    |
|    n_updates            | 2990      |
|    policy_gradient_loss | -0.0183   |
|    std                  | 1.05      |
|    value_loss           | 0.0309    |
---------------------------------------
Eval num_timesteps=10000000, episode_reward=3638.33 +/- 95.20
Episode length: 318.80 +/- 17.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 319         |
|    mean_reward          | 3.64e+03    |
| time/                   |             |
|    total_timesteps      | 10000000    |
| train/                  |             |
|    approx_kl            | 0.016459256 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.151      |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.06        |
|    value_loss           | 0.0265      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 256        |
|    ep_rew_mean          | 2.79e+03   |
| time/                   |            |
|    fps                  | 7268       |
|    iterations           | 310        |
|    time_elapsed         | 1397       |
|    total_timesteps      | 10158080   |
| train/                  |            |
|    approx_kl            | 0.01727093 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.4      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.151     |
|    n_updates            | 3090       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 1.06       |
|    value_loss           | 0.0262     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 260         |
|    ep_rew_mean          | 2.89e+03    |
| time/                   |             |
|    fps                  | 7271        |
|    iterations           | 320         |
|    time_elapsed         | 1442        |
|    total_timesteps      | 10485760    |
| train/                  |             |
|    approx_kl            | 0.016089983 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.149      |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.07        |
|    value_loss           | 0.0278      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 270         |
|    ep_rew_mean          | 3.01e+03    |
| time/                   |             |
|    fps                  | 7273        |
|    iterations           | 330         |
|    time_elapsed         | 1486        |
|    total_timesteps      | 10813440    |
| train/                  |             |
|    approx_kl            | 0.015714442 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.5       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.07        |
|    value_loss           | 0.0221      |
-----------------------------------------
Eval num_timesteps=11000000, episode_reward=3653.84 +/- 359.51
Episode length: 309.80 +/- 33.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 310         |
|    mean_reward          | 3.65e+03    |
| time/                   |             |
|    total_timesteps      | 11000000    |
| train/                  |             |
|    approx_kl            | 0.016496548 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.08        |
|    value_loss           | 0.0252      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 276         |
|    ep_rew_mean          | 3.05e+03    |
| time/                   |             |
|    fps                  | 7271        |
|    iterations           | 340         |
|    time_elapsed         | 1532        |
|    total_timesteps      | 11141120    |
| train/                  |             |
|    approx_kl            | 0.016789854 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.08        |
|    value_loss           | 0.0216      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 290         |
|    ep_rew_mean          | 3.2e+03     |
| time/                   |             |
|    fps                  | 7273        |
|    iterations           | 350         |
|    time_elapsed         | 1576        |
|    total_timesteps      | 11468800    |
| train/                  |             |
|    approx_kl            | 0.015796563 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.09        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 295         |
|    ep_rew_mean          | 3.32e+03    |
| time/                   |             |
|    fps                  | 7275        |
|    iterations           | 360         |
|    time_elapsed         | 1621        |
|    total_timesteps      | 11796480    |
| train/                  |             |
|    approx_kl            | 0.016010638 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.1         |
|    value_loss           | 0.0185      |
-----------------------------------------
Eval num_timesteps=12000000, episode_reward=3778.39 +/- 170.00
Episode length: 309.40 +/- 24.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 3.78e+03    |
| time/                   |             |
|    total_timesteps      | 12000000    |
| train/                  |             |
|    approx_kl            | 0.015807036 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.153      |
|    n_updates            | 3660        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.1         |
|    value_loss           | 0.0183      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 292         |
|    ep_rew_mean          | 3.27e+03    |
| time/                   |             |
|    fps                  | 7273        |
|    iterations           | 370         |
|    time_elapsed         | 1666        |
|    total_timesteps      | 12124160    |
| train/                  |             |
|    approx_kl            | 0.015767384 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.154      |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0185     |
|    std                  | 1.11        |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 298         |
|    ep_rew_mean          | 3.29e+03    |
| time/                   |             |
|    fps                  | 7275        |
|    iterations           | 380         |
|    time_elapsed         | 1711        |
|    total_timesteps      | 12451840    |
| train/                  |             |
|    approx_kl            | 0.016612794 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.156      |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.12        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 301         |
|    ep_rew_mean          | 3.4e+03     |
| time/                   |             |
|    fps                  | 7277        |
|    iterations           | 390         |
|    time_elapsed         | 1756        |
|    total_timesteps      | 12779520    |
| train/                  |             |
|    approx_kl            | 0.016783983 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.157      |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.13        |
|    value_loss           | 0.0191      |
-----------------------------------------
Eval num_timesteps=13000000, episode_reward=3754.17 +/- 275.60
Episode length: 314.20 +/- 26.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 314         |
|    mean_reward          | 3.75e+03    |
| time/                   |             |
|    total_timesteps      | 13000000    |
| train/                  |             |
|    approx_kl            | 0.015915174 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.155      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 1.13        |
|    value_loss           | 0.0188      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 303        |
|    ep_rew_mean          | 3.41e+03   |
| time/                   |            |
|    fps                  | 7275       |
|    iterations           | 400        |
|    time_elapsed         | 1801       |
|    total_timesteps      | 13107200   |
| train/                  |            |
|    approx_kl            | 0.01602345 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.3      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.156     |
|    n_updates            | 3990       |
|    policy_gradient_loss | -0.0186    |
|    std                  | 1.14       |
|    value_loss           | 0.0173     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 3.42e+03    |
| time/                   |             |
|    fps                  | 7275        |
|    iterations           | 410         |
|    time_elapsed         | 1846        |
|    total_timesteps      | 13434880    |
| train/                  |             |
|    approx_kl            | 0.017406266 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.5       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.158      |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.15        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 323         |
|    ep_rew_mean          | 3.6e+03     |
| time/                   |             |
|    fps                  | 7276        |
|    iterations           | 420         |
|    time_elapsed         | 1891        |
|    total_timesteps      | 13762560    |
| train/                  |             |
|    approx_kl            | 0.017087549 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.16        |
|    value_loss           | 0.0166      |
-----------------------------------------
Eval num_timesteps=14000000, episode_reward=3364.71 +/- 1458.91
Episode length: 279.60 +/- 111.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 3.36e+03    |
| time/                   |             |
|    total_timesteps      | 14000000    |
| train/                  |             |
|    approx_kl            | 0.016577803 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.17        |
|    value_loss           | 0.0213      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 320         |
|    ep_rew_mean          | 3.45e+03    |
| time/                   |             |
|    fps                  | 7275        |
|    iterations           | 430         |
|    time_elapsed         | 1936        |
|    total_timesteps      | 14090240    |
| train/                  |             |
|    approx_kl            | 0.016664261 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.162      |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.18        |
|    value_loss           | 0.0149      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 321         |
|    ep_rew_mean          | 3.58e+03    |
| time/                   |             |
|    fps                  | 7277        |
|    iterations           | 440         |
|    time_elapsed         | 1981        |
|    total_timesteps      | 14417920    |
| train/                  |             |
|    approx_kl            | 0.017029481 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.19        |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 312         |
|    ep_rew_mean          | 3.61e+03    |
| time/                   |             |
|    fps                  | 7279        |
|    iterations           | 450         |
|    time_elapsed         | 2025        |
|    total_timesteps      | 14745600    |
| train/                  |             |
|    approx_kl            | 0.016093351 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.2         |
|    value_loss           | 0.0143      |
-----------------------------------------
Eval num_timesteps=15000000, episode_reward=2914.93 +/- 1298.84
Episode length: 232.00 +/- 91.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 2.91e+03   |
| time/                   |            |
|    total_timesteps      | 15000000   |
| train/                  |            |
|    approx_kl            | 0.01701479 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.164     |
|    n_updates            | 4570       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.21       |
|    value_loss           | 0.0181     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 322         |
|    ep_rew_mean          | 3.74e+03    |
| time/                   |             |
|    fps                  | 7278        |
|    iterations           | 460         |
|    time_elapsed         | 2070        |
|    total_timesteps      | 15073280    |
| train/                  |             |
|    approx_kl            | 0.017158924 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 1.21        |
|    value_loss           | 0.0168      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.84e+03    |
| time/                   |             |
|    fps                  | 7280        |
|    iterations           | 470         |
|    time_elapsed         | 2115        |
|    total_timesteps      | 15400960    |
| train/                  |             |
|    approx_kl            | 0.016675778 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.165      |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.22        |
|    value_loss           | 0.013       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 328         |
|    ep_rew_mean          | 3.75e+03    |
| time/                   |             |
|    fps                  | 7282        |
|    iterations           | 480         |
|    time_elapsed         | 2159        |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.017265387 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.166      |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0201     |
|    std                  | 1.24        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=16000000, episode_reward=3522.63 +/- 347.14
Episode length: 273.80 +/- 34.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 3.52e+03    |
| time/                   |             |
|    total_timesteps      | 16000000    |
| train/                  |             |
|    approx_kl            | 0.016347557 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.166      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.25        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.93e+03    |
| time/                   |             |
|    fps                  | 7281        |
|    iterations           | 490         |
|    time_elapsed         | 2205        |
|    total_timesteps      | 16056320    |
| train/                  |             |
|    approx_kl            | 0.016210757 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.25        |
|    value_loss           | 0.015       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 326         |
|    ep_rew_mean          | 3.8e+03     |
| time/                   |             |
|    fps                  | 7282        |
|    iterations           | 500         |
|    time_elapsed         | 2249        |
|    total_timesteps      | 16384000    |
| train/                  |             |
|    approx_kl            | 0.017417781 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.164      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.27        |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 341         |
|    ep_rew_mean          | 3.87e+03    |
| time/                   |             |
|    fps                  | 7284        |
|    iterations           | 510         |
|    time_elapsed         | 2294        |
|    total_timesteps      | 16711680    |
| train/                  |             |
|    approx_kl            | 0.017543036 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.28        |
|    value_loss           | 0.0148      |
-----------------------------------------
Eval num_timesteps=17000000, episode_reward=4136.39 +/- 353.14
Episode length: 332.20 +/- 39.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 332         |
|    mean_reward          | 4.14e+03    |
| time/                   |             |
|    total_timesteps      | 17000000    |
| train/                  |             |
|    approx_kl            | 0.017526733 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.29        |
|    value_loss           | 0.014       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 4.27e+03    |
| time/                   |             |
|    fps                  | 7282        |
|    iterations           | 520         |
|    time_elapsed         | 2339        |
|    total_timesteps      | 17039360    |
| train/                  |             |
|    approx_kl            | 0.018134179 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.29        |
|    value_loss           | 0.015       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 334         |
|    ep_rew_mean          | 3.9e+03     |
| time/                   |             |
|    fps                  | 7284        |
|    iterations           | 530         |
|    time_elapsed         | 2384        |
|    total_timesteps      | 17367040    |
| train/                  |             |
|    approx_kl            | 0.017296806 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.2       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.167      |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.3         |
|    value_loss           | 0.017       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 3.95e+03    |
| time/                   |             |
|    fps                  | 7285        |
|    iterations           | 540         |
|    time_elapsed         | 2428        |
|    total_timesteps      | 17694720    |
| train/                  |             |
|    approx_kl            | 0.018097993 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.17       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.32        |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=18000000, episode_reward=3926.36 +/- 1749.42
Episode length: 323.20 +/- 126.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 3.93e+03    |
| time/                   |             |
|    total_timesteps      | 18000000    |
| train/                  |             |
|    approx_kl            | 0.016725343 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.171      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.34        |
|    value_loss           | 0.0153      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | 4.14e+03 |
| time/              |          |
|    fps             | 7284     |
|    iterations      | 550      |
|    time_elapsed    | 2474     |
|    total_timesteps | 18022400 |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 334        |
|    ep_rew_mean          | 4.05e+03   |
| time/                   |            |
|    fps                  | 7285       |
|    iterations           | 560        |
|    time_elapsed         | 2518       |
|    total_timesteps      | 18350080   |
| train/                  |            |
|    approx_kl            | 0.01740751 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.8      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.172     |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.35       |
|    value_loss           | 0.0154     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 342        |
|    ep_rew_mean          | 4.17e+03   |
| time/                   |            |
|    fps                  | 7287       |
|    iterations           | 570        |
|    time_elapsed         | 2563       |
|    total_timesteps      | 18677760   |
| train/                  |            |
|    approx_kl            | 0.01653136 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.9      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.174     |
|    n_updates            | 5690       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.37       |
|    value_loss           | 0.0114     |
----------------------------------------
Eval num_timesteps=19000000, episode_reward=4540.70 +/- 507.54
Episode length: 361.20 +/- 27.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 361         |
|    mean_reward          | 4.54e+03    |
| time/                   |             |
|    total_timesteps      | 19000000    |
| train/                  |             |
|    approx_kl            | 0.017512705 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.39        |
|    value_loss           | 0.0138      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | 4.2e+03  |
| time/              |          |
|    fps             | 7285     |
|    iterations      | 580      |
|    time_elapsed    | 2608     |
|    total_timesteps | 19005440 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.24e+03    |
| time/                   |             |
|    fps                  | 7286        |
|    iterations           | 590         |
|    time_elapsed         | 2653        |
|    total_timesteps      | 19333120    |
| train/                  |             |
|    approx_kl            | 0.017574277 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.4         |
|    value_loss           | 0.0148      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 342         |
|    ep_rew_mean          | 4.26e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 600         |
|    time_elapsed         | 2697        |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.016366895 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.42        |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 331         |
|    ep_rew_mean          | 4.12e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 610         |
|    time_elapsed         | 2742        |
|    total_timesteps      | 19988480    |
| train/                  |             |
|    approx_kl            | 0.018768266 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.177      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.44        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=20000000, episode_reward=5171.40 +/- 50.91
Episode length: 408.20 +/- 12.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 408        |
|    mean_reward          | 5.17e+03   |
| time/                   |            |
|    total_timesteps      | 20000000   |
| train/                  |            |
|    approx_kl            | 0.01691229 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.175     |
|    n_updates            | 6100       |
|    policy_gradient_loss | -0.0185    |
|    std                  | 1.44       |
|    value_loss           | 0.0137     |
----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 341        |
|    ep_rew_mean          | 4.17e+03   |
| time/                   |            |
|    fps                  | 7287       |
|    iterations           | 620        |
|    time_elapsed         | 2787       |
|    total_timesteps      | 20316160   |
| train/                  |            |
|    approx_kl            | 0.01759119 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.177     |
|    n_updates            | 6190       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 1.46       |
|    value_loss           | 0.0112     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 351         |
|    ep_rew_mean          | 4.39e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 630         |
|    time_elapsed         | 2832        |
|    total_timesteps      | 20643840    |
| train/                  |             |
|    approx_kl            | 0.017498557 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.18       |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0217     |
|    std                  | 1.48        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 341         |
|    ep_rew_mean          | 4.28e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 640         |
|    time_elapsed         | 2876        |
|    total_timesteps      | 20971520    |
| train/                  |             |
|    approx_kl            | 0.016870512 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.5         |
|    value_loss           | 0.0143      |
-----------------------------------------
Eval num_timesteps=21000000, episode_reward=5221.29 +/- 221.45
Episode length: 394.80 +/- 19.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 395        |
|    mean_reward          | 5.22e+03   |
| time/                   |            |
|    total_timesteps      | 21000000   |
| train/                  |            |
|    approx_kl            | 0.01738807 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.2      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.179     |
|    n_updates            | 6400       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.5        |
|    value_loss           | 0.0136     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 336         |
|    ep_rew_mean          | 4.23e+03    |
| time/                   |             |
|    fps                  | 7287        |
|    iterations           | 650         |
|    time_elapsed         | 2922        |
|    total_timesteps      | 21299200    |
| train/                  |             |
|    approx_kl            | 0.018084588 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.179      |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.52        |
|    value_loss           | 0.0115      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 355         |
|    ep_rew_mean          | 4.57e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 660         |
|    time_elapsed         | 2967        |
|    total_timesteps      | 21626880    |
| train/                  |             |
|    approx_kl            | 0.018397799 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.54        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 354         |
|    ep_rew_mean          | 4.44e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 670         |
|    time_elapsed         | 3011        |
|    total_timesteps      | 21954560    |
| train/                  |             |
|    approx_kl            | 0.018538786 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.183      |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.56        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=22000000, episode_reward=5182.98 +/- 150.62
Episode length: 387.80 +/- 21.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 388        |
|    mean_reward          | 5.18e+03   |
| time/                   |            |
|    total_timesteps      | 22000000   |
| train/                  |            |
|    approx_kl            | 0.02647432 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -29.7      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.181     |
|    n_updates            | 6710       |
|    policy_gradient_loss | -0.0197    |
|    std                  | 1.56       |
|    value_loss           | 0.011      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | 4.5e+03     |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 680         |
|    time_elapsed         | 3057        |
|    total_timesteps      | 22282240    |
| train/                  |             |
|    approx_kl            | 0.016482454 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.179      |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.58        |
|    value_loss           | 0.0128      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 351         |
|    ep_rew_mean          | 4.44e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 690         |
|    time_elapsed         | 3101        |
|    total_timesteps      | 22609920    |
| train/                  |             |
|    approx_kl            | 0.016682032 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.6         |
|    value_loss           | 0.0124      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 342        |
|    ep_rew_mean          | 4.37e+03   |
| time/                   |            |
|    fps                  | 7289       |
|    iterations           | 700        |
|    time_elapsed         | 3146       |
|    total_timesteps      | 22937600   |
| train/                  |            |
|    approx_kl            | 0.01667092 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.2      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.179     |
|    n_updates            | 6990       |
|    policy_gradient_loss | -0.0179    |
|    std                  | 1.62       |
|    value_loss           | 0.0116     |
----------------------------------------
Eval num_timesteps=23000000, episode_reward=5101.62 +/- 331.57
Episode length: 396.00 +/- 22.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 396       |
|    mean_reward          | 5.1e+03   |
| time/                   |           |
|    total_timesteps      | 23000000  |
| train/                  |           |
|    approx_kl            | 0.0187371 |
|    clip_fraction        | 0.16      |
|    clip_range           | 0.2       |
|    entropy_loss         | -30.2     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.18     |
|    n_updates            | 7010      |
|    policy_gradient_loss | -0.0188   |
|    std                  | 1.63      |
|    value_loss           | 0.0152    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 354         |
|    ep_rew_mean          | 4.47e+03    |
| time/                   |             |
|    fps                  | 7287        |
|    iterations           | 710         |
|    time_elapsed         | 3192        |
|    total_timesteps      | 23265280    |
| train/                  |             |
|    approx_kl            | 0.018485758 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.4       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.64        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 344         |
|    ep_rew_mean          | 4.38e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 720         |
|    time_elapsed         | 3237        |
|    total_timesteps      | 23592960    |
| train/                  |             |
|    approx_kl            | 0.016560469 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.6       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.184      |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.019      |
|    std                  | 1.66        |
|    value_loss           | 0.0118      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 367         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 730         |
|    time_elapsed         | 3281        |
|    total_timesteps      | 23920640    |
| train/                  |             |
|    approx_kl            | 0.016537063 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.186      |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.68        |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=24000000, episode_reward=5118.38 +/- 341.23
Episode length: 378.20 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 5.12e+03    |
| time/                   |             |
|    total_timesteps      | 24000000    |
| train/                  |             |
|    approx_kl            | 0.018093333 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.185      |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.68        |
|    value_loss           | 0.0144      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 346        |
|    ep_rew_mean          | 4.37e+03   |
| time/                   |            |
|    fps                  | 7287       |
|    iterations           | 740        |
|    time_elapsed         | 3327       |
|    total_timesteps      | 24248320   |
| train/                  |            |
|    approx_kl            | 0.01740398 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.9      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.188     |
|    n_updates            | 7390       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.7        |
|    value_loss           | 0.0118     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | 4.56e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 750         |
|    time_elapsed         | 3371        |
|    total_timesteps      | 24576000    |
| train/                  |             |
|    approx_kl            | 0.019105121 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.189      |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.73        |
|    value_loss           | 0.0118      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 351        |
|    ep_rew_mean          | 4.36e+03   |
| time/                   |            |
|    fps                  | 7289       |
|    iterations           | 760        |
|    time_elapsed         | 3416       |
|    total_timesteps      | 24903680   |
| train/                  |            |
|    approx_kl            | 0.01786679 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -31.3      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.189     |
|    n_updates            | 7590       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 1.75       |
|    value_loss           | 0.0108     |
----------------------------------------
Eval num_timesteps=25000000, episode_reward=4701.50 +/- 386.89
Episode length: 399.40 +/- 43.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 399         |
|    mean_reward          | 4.7e+03     |
| time/                   |             |
|    total_timesteps      | 25000000    |
| train/                  |             |
|    approx_kl            | 0.016979128 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.76        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 349         |
|    ep_rew_mean          | 4.38e+03    |
| time/                   |             |
|    fps                  | 7285        |
|    iterations           | 770         |
|    time_elapsed         | 3463        |
|    total_timesteps      | 25231360    |
| train/                  |             |
|    approx_kl            | 0.016883038 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.5       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.188      |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.77        |
|    value_loss           | 0.013       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.3e+03     |
| time/                   |             |
|    fps                  | 7286        |
|    iterations           | 780         |
|    time_elapsed         | 3507        |
|    total_timesteps      | 25559040    |
| train/                  |             |
|    approx_kl            | 0.017381586 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.8         |
|    value_loss           | 0.0175      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 360         |
|    ep_rew_mean          | 4.58e+03    |
| time/                   |             |
|    fps                  | 7287        |
|    iterations           | 790         |
|    time_elapsed         | 3552        |
|    total_timesteps      | 25886720    |
| train/                  |             |
|    approx_kl            | 0.017933274 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.194      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.83        |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=26000000, episode_reward=5248.21 +/- 295.28
Episode length: 410.20 +/- 19.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 410        |
|    mean_reward          | 5.25e+03   |
| time/                   |            |
|    total_timesteps      | 26000000   |
| train/                  |            |
|    approx_kl            | 0.01710907 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32        |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.192     |
|    n_updates            | 7930       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 1.84       |
|    value_loss           | 0.0129     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 365         |
|    ep_rew_mean          | 4.66e+03    |
| time/                   |             |
|    fps                  | 7286        |
|    iterations           | 800         |
|    time_elapsed         | 3597        |
|    total_timesteps      | 26214400    |
| train/                  |             |
|    approx_kl            | 0.018508002 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.193      |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.85        |
|    value_loss           | 0.011       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.76e+03    |
| time/                   |             |
|    fps                  | 7287        |
|    iterations           | 810         |
|    time_elapsed         | 3642        |
|    total_timesteps      | 26542080    |
| train/                  |             |
|    approx_kl            | 0.016981024 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.88        |
|    value_loss           | 0.0097      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | 4.67e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 820         |
|    time_elapsed         | 3686        |
|    total_timesteps      | 26869760    |
| train/                  |             |
|    approx_kl            | 0.018075708 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.193      |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1.91        |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=27000000, episode_reward=5281.48 +/- 240.72
Episode length: 416.00 +/- 23.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 416         |
|    mean_reward          | 5.28e+03    |
| time/                   |             |
|    total_timesteps      | 27000000    |
| train/                  |             |
|    approx_kl            | 0.018653538 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8230        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 1.92        |
|    value_loss           | 0.0149      |
-----------------------------------------
New best mean reward!
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 370       |
|    ep_rew_mean          | 4.73e+03  |
| time/                   |           |
|    fps                  | 7287      |
|    iterations           | 830       |
|    time_elapsed         | 3732      |
|    total_timesteps      | 27197440  |
| train/                  |           |
|    approx_kl            | 0.0187455 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.2       |
|    entropy_loss         | -32.7     |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.195    |
|    n_updates            | 8290      |
|    policy_gradient_loss | -0.0198   |
|    std                  | 1.94      |
|    value_loss           | 0.0125    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | 4.74e+03    |
| time/                   |             |
|    fps                  | 7287        |
|    iterations           | 840         |
|    time_elapsed         | 3776        |
|    total_timesteps      | 27525120    |
| train/                  |             |
|    approx_kl            | 0.018117836 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.96        |
|    value_loss           | 0.0124      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | 4.47e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 850         |
|    time_elapsed         | 3821        |
|    total_timesteps      | 27852800    |
| train/                  |             |
|    approx_kl            | 0.017775416 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.198      |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2           |
|    value_loss           | 0.0124      |
-----------------------------------------
Eval num_timesteps=28000000, episode_reward=4450.61 +/- 1480.44
Episode length: 336.60 +/- 93.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 337         |
|    mean_reward          | 4.45e+03    |
| time/                   |             |
|    total_timesteps      | 28000000    |
| train/                  |             |
|    approx_kl            | 0.016826686 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.199      |
|    n_updates            | 8540        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.01        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 4.71e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 860         |
|    time_elapsed         | 3866        |
|    total_timesteps      | 28180480    |
| train/                  |             |
|    approx_kl            | 0.017207533 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.3       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.197      |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 2.02        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 362         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 870         |
|    time_elapsed         | 3911        |
|    total_timesteps      | 28508160    |
| train/                  |             |
|    approx_kl            | 0.019004185 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.05        |
|    value_loss           | 0.0103      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 382        |
|    ep_rew_mean          | 4.93e+03   |
| time/                   |            |
|    fps                  | 7289       |
|    iterations           | 880        |
|    time_elapsed         | 3955       |
|    total_timesteps      | 28835840   |
| train/                  |            |
|    approx_kl            | 0.01691171 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.6      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.201     |
|    n_updates            | 8790       |
|    policy_gradient_loss | -0.02      |
|    std                  | 2.07       |
|    value_loss           | 0.0113     |
----------------------------------------
Eval num_timesteps=29000000, episode_reward=4603.59 +/- 1212.76
Episode length: 387.40 +/- 31.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 387         |
|    mean_reward          | 4.6e+03     |
| time/                   |             |
|    total_timesteps      | 29000000    |
| train/                  |             |
|    approx_kl            | 0.017333295 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8850        |
|    policy_gradient_loss | -0.02       |
|    std                  | 2.1         |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 360         |
|    ep_rew_mean          | 4.66e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 890         |
|    time_elapsed         | 4001        |
|    total_timesteps      | 29163520    |
| train/                  |             |
|    approx_kl            | 0.015304524 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0186     |
|    std                  | 2.1         |
|    value_loss           | 0.0104      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 366        |
|    ep_rew_mean          | 4.64e+03   |
| time/                   |            |
|    fps                  | 7289       |
|    iterations           | 900        |
|    time_elapsed         | 4045       |
|    total_timesteps      | 29491200   |
| train/                  |            |
|    approx_kl            | 0.01898979 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.1      |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.204     |
|    n_updates            | 8990       |
|    policy_gradient_loss | -0.0205    |
|    std                  | 2.13       |
|    value_loss           | 0.0131     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 344         |
|    ep_rew_mean          | 4.33e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 910         |
|    time_elapsed         | 4090        |
|    total_timesteps      | 29818880    |
| train/                  |             |
|    approx_kl            | 0.017770976 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.203      |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.17        |
|    value_loss           | 0.0134      |
-----------------------------------------
Eval num_timesteps=30000000, episode_reward=5423.54 +/- 296.65
Episode length: 406.20 +/- 46.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 406         |
|    mean_reward          | 5.42e+03    |
| time/                   |             |
|    total_timesteps      | 30000000    |
| train/                  |             |
|    approx_kl            | 0.017347744 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.4       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.205      |
|    n_updates            | 9150        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 2.19        |
|    value_loss           | 0.013       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 362         |
|    ep_rew_mean          | 4.7e+03     |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 920         |
|    time_elapsed         | 4136        |
|    total_timesteps      | 30146560    |
| train/                  |             |
|    approx_kl            | 0.018128488 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.202      |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 2.2         |
|    value_loss           | 0.0169      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 367        |
|    ep_rew_mean          | 4.62e+03   |
| time/                   |            |
|    fps                  | 7289       |
|    iterations           | 930        |
|    time_elapsed         | 4180       |
|    total_timesteps      | 30474240   |
| train/                  |            |
|    approx_kl            | 0.01797387 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.205     |
|    n_updates            | 9290       |
|    policy_gradient_loss | -0.0196    |
|    std                  | 2.23       |
|    value_loss           | 0.0136     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 940         |
|    time_elapsed         | 4225        |
|    total_timesteps      | 30801920    |
| train/                  |             |
|    approx_kl            | 0.018678594 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.207      |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.26        |
|    value_loss           | 0.0125      |
-----------------------------------------
Eval num_timesteps=31000000, episode_reward=5087.22 +/- 286.76
Episode length: 377.60 +/- 16.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 378         |
|    mean_reward          | 5.09e+03    |
| time/                   |             |
|    total_timesteps      | 31000000    |
| train/                  |             |
|    approx_kl            | 0.018121213 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -35         |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.207      |
|    n_updates            | 9460        |
|    policy_gradient_loss | -0.0203     |
|    std                  | 2.28        |
|    value_loss           | 0.0156      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 357         |
|    ep_rew_mean          | 4.58e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 950         |
|    time_elapsed         | 4270        |
|    total_timesteps      | 31129600    |
| train/                  |             |
|    approx_kl            | 0.017271493 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9490        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 2.29        |
|    value_loss           | 0.0116      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | 4.61e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 960         |
|    time_elapsed         | 4315        |
|    total_timesteps      | 31457280    |
| train/                  |             |
|    approx_kl            | 0.018320246 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.33        |
|    value_loss           | 0.0164      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 364         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 970         |
|    time_elapsed         | 4360        |
|    total_timesteps      | 31784960    |
| train/                  |             |
|    approx_kl            | 0.018801183 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.36        |
|    value_loss           | 0.013       |
-----------------------------------------
Eval num_timesteps=32000000, episode_reward=4834.53 +/- 1282.49
Episode length: 381.00 +/- 50.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 381         |
|    mean_reward          | 4.83e+03    |
| time/                   |             |
|    total_timesteps      | 32000000    |
| train/                  |             |
|    approx_kl            | 0.017243747 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.6       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 2.38        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.78e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 980         |
|    time_elapsed         | 4405        |
|    total_timesteps      | 32112640    |
| train/                  |             |
|    approx_kl            | 0.018085113 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 2.39        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 345         |
|    ep_rew_mean          | 4.36e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 990         |
|    time_elapsed         | 4450        |
|    total_timesteps      | 32440320    |
| train/                  |             |
|    approx_kl            | 0.018949928 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.213      |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.43        |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 372         |
|    ep_rew_mean          | 4.68e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1000        |
|    time_elapsed         | 4495        |
|    total_timesteps      | 32768000    |
| train/                  |             |
|    approx_kl            | 0.017659022 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36         |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.213      |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 2.46        |
|    value_loss           | 0.0132      |
-----------------------------------------
Eval num_timesteps=33000000, episode_reward=5378.75 +/- 173.01
Episode length: 404.20 +/- 10.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 5.38e+03    |
| time/                   |             |
|    total_timesteps      | 33000000    |
| train/                  |             |
|    approx_kl            | 0.016885985 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.48        |
|    value_loss           | 0.00991     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 380         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1010        |
|    time_elapsed         | 4540        |
|    total_timesteps      | 33095680    |
| train/                  |             |
|    approx_kl            | 0.017528385 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.1       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 2.49        |
|    value_loss           | 0.00823     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 368         |
|    ep_rew_mean          | 4.63e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1020        |
|    time_elapsed         | 4585        |
|    total_timesteps      | 33423360    |
| train/                  |             |
|    approx_kl            | 0.017909208 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.3       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.214      |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 2.53        |
|    value_loss           | 0.0131      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 371         |
|    ep_rew_mean          | 4.64e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1030        |
|    time_elapsed         | 4630        |
|    total_timesteps      | 33751040    |
| train/                  |             |
|    approx_kl            | 0.018018413 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.5       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.56        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=34000000, episode_reward=5476.07 +/- 187.16
Episode length: 407.40 +/- 29.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 407        |
|    mean_reward          | 5.48e+03   |
| time/                   |            |
|    total_timesteps      | 34000000   |
| train/                  |            |
|    approx_kl            | 0.01725314 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 10370      |
|    policy_gradient_loss | -0.0193    |
|    std                  | 2.59       |
|    value_loss           | 0.0115     |
----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 371        |
|    ep_rew_mean          | 4.72e+03   |
| time/                   |            |
|    fps                  | 7288       |
|    iterations           | 1040       |
|    time_elapsed         | 4675       |
|    total_timesteps      | 34078720   |
| train/                  |            |
|    approx_kl            | 0.01804078 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.7      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 10390      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 2.6        |
|    value_loss           | 0.0134     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1050        |
|    time_elapsed         | 4720        |
|    total_timesteps      | 34406400    |
| train/                  |             |
|    approx_kl            | 0.018689679 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0213     |
|    std                  | 2.64        |
|    value_loss           | 0.00877     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1060        |
|    time_elapsed         | 4765        |
|    total_timesteps      | 34734080    |
| train/                  |             |
|    approx_kl            | 0.017338932 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.1       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.67        |
|    value_loss           | 0.00907     |
-----------------------------------------
Eval num_timesteps=35000000, episode_reward=5341.75 +/- 139.59
Episode length: 399.60 +/- 21.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 5.34e+03    |
| time/                   |             |
|    total_timesteps      | 35000000    |
| train/                  |             |
|    approx_kl            | 0.018085858 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.71        |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1070        |
|    time_elapsed         | 4810        |
|    total_timesteps      | 35061760    |
| train/                  |             |
|    approx_kl            | 0.018646136 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 2.71        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.78e+03    |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1080        |
|    time_elapsed         | 4855        |
|    total_timesteps      | 35389440    |
| train/                  |             |
|    approx_kl            | 0.018206667 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.5       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.221      |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.75        |
|    value_loss           | 0.0158      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1090        |
|    time_elapsed         | 4899        |
|    total_timesteps      | 35717120    |
| train/                  |             |
|    approx_kl            | 0.018956661 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.222      |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 2.79        |
|    value_loss           | 0.0123      |
-----------------------------------------
Eval num_timesteps=36000000, episode_reward=5528.24 +/- 299.52
Episode length: 402.00 +/- 17.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 5.53e+03    |
| time/                   |             |
|    total_timesteps      | 36000000    |
| train/                  |             |
|    approx_kl            | 0.017256018 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.224      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.83        |
|    value_loss           | 0.00983     |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 371         |
|    ep_rew_mean          | 4.7e+03     |
| time/                   |             |
|    fps                  | 7288        |
|    iterations           | 1100        |
|    time_elapsed         | 4945        |
|    total_timesteps      | 36044800    |
| train/                  |             |
|    approx_kl            | 0.017237555 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.9       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.221      |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 2.83        |
|    value_loss           | 0.0109      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 385         |
|    ep_rew_mean          | 4.93e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1110        |
|    time_elapsed         | 4989        |
|    total_timesteps      | 36372480    |
| train/                  |             |
|    approx_kl            | 0.018586453 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.1       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.223      |
|    n_updates            | 11090       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.87        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 4.94e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1120        |
|    time_elapsed         | 5034        |
|    total_timesteps      | 36700160    |
| train/                  |             |
|    approx_kl            | 0.018611297 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 2.91        |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=37000000, episode_reward=5560.07 +/- 84.52
Episode length: 408.00 +/- 7.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 5.56e+03    |
| time/                   |             |
|    total_timesteps      | 37000000    |
| train/                  |             |
|    approx_kl            | 0.018078716 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.226      |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.96        |
|    value_loss           | 0.0105      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | 4.93e+03 |
| time/              |          |
|    fps             | 7288     |
|    iterations      | 1130     |
|    time_elapsed    | 5080     |
|    total_timesteps | 37027840 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1140        |
|    time_elapsed         | 5124        |
|    total_timesteps      | 37355520    |
| train/                  |             |
|    approx_kl            | 0.016109794 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.225      |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 3.01        |
|    value_loss           | 0.0106      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 378         |
|    ep_rew_mean          | 4.8e+03     |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1150        |
|    time_elapsed         | 5168        |
|    total_timesteps      | 37683200    |
| train/                  |             |
|    approx_kl            | 0.017058615 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 3.06        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=38000000, episode_reward=5538.03 +/- 139.79
Episode length: 413.20 +/- 17.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 5.54e+03    |
| time/                   |             |
|    total_timesteps      | 38000000    |
| train/                  |             |
|    approx_kl            | 0.018478842 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.2       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.228      |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 3.11        |
|    value_loss           | 0.0128      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 381      |
|    ep_rew_mean     | 4.87e+03 |
| time/              |          |
|    fps             | 7289     |
|    iterations      | 1160     |
|    time_elapsed    | 5214     |
|    total_timesteps | 38010880 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 377         |
|    ep_rew_mean          | 4.8e+03     |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1170        |
|    time_elapsed         | 5259        |
|    total_timesteps      | 38338560    |
| train/                  |             |
|    approx_kl            | 0.016657647 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 3.16        |
|    value_loss           | 0.00937     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 355         |
|    ep_rew_mean          | 4.55e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1180        |
|    time_elapsed         | 5303        |
|    total_timesteps      | 38666240    |
| train/                  |             |
|    approx_kl            | 0.018213741 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.232      |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 3.21        |
|    value_loss           | 0.0111      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.75e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1190        |
|    time_elapsed         | 5348        |
|    total_timesteps      | 38993920    |
| train/                  |             |
|    approx_kl            | 0.016899161 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 3.26        |
|    value_loss           | 0.0138      |
-----------------------------------------
Eval num_timesteps=39000000, episode_reward=5289.10 +/- 780.35
Episode length: 391.40 +/- 63.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 391         |
|    mean_reward          | 5.29e+03    |
| time/                   |             |
|    total_timesteps      | 39000000    |
| train/                  |             |
|    approx_kl            | 0.017097887 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.9       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.23       |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 3.27        |
|    value_loss           | 0.0138      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 374         |
|    ep_rew_mean          | 4.73e+03    |
| time/                   |             |
|    fps                  | 7289        |
|    iterations           | 1200        |
|    time_elapsed         | 5394        |
|    total_timesteps      | 39321600    |
| train/                  |             |
|    approx_kl            | 0.017440893 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.231      |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 3.32        |
|    value_loss           | 0.0114      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | 4.98e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1210        |
|    time_elapsed         | 5438        |
|    total_timesteps      | 39649280    |
| train/                  |             |
|    approx_kl            | 0.017910779 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.37        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 374         |
|    ep_rew_mean          | 4.79e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1220        |
|    time_elapsed         | 5483        |
|    total_timesteps      | 39976960    |
| train/                  |             |
|    approx_kl            | 0.017491309 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 3.41        |
|    value_loss           | 0.00968     |
-----------------------------------------
Eval num_timesteps=40000000, episode_reward=5630.62 +/- 47.55
Episode length: 403.80 +/- 7.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 404         |
|    mean_reward          | 5.63e+03    |
| time/                   |             |
|    total_timesteps      | 40000000    |
| train/                  |             |
|    approx_kl            | 0.018493395 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.5       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.235      |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 3.42        |
|    value_loss           | 0.0116      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.03e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1230        |
|    time_elapsed         | 5528        |
|    total_timesteps      | 40304640    |
| train/                  |             |
|    approx_kl            | 0.017220616 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.6       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 3.46        |
|    value_loss           | 0.0104      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.99e+03   |
| time/                   |            |
|    fps                  | 7290       |
|    iterations           | 1240       |
|    time_elapsed         | 5572       |
|    total_timesteps      | 40632320   |
| train/                  |            |
|    approx_kl            | 0.01731256 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.239     |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.0206    |
|    std                  | 3.53       |
|    value_loss           | 0.00928    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 393        |
|    ep_rew_mean          | 5.08e+03   |
| time/                   |            |
|    fps                  | 7291       |
|    iterations           | 1250       |
|    time_elapsed         | 5617       |
|    total_timesteps      | 40960000   |
| train/                  |            |
|    approx_kl            | 0.01681754 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -41.1      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.239     |
|    n_updates            | 12490      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 3.58       |
|    value_loss           | 0.0098     |
----------------------------------------
Eval num_timesteps=41000000, episode_reward=5581.22 +/- 244.68
Episode length: 418.40 +/- 17.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 5.58e+03    |
| time/                   |             |
|    total_timesteps      | 41000000    |
| train/                  |             |
|    approx_kl            | 0.015942145 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.2       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.236      |
|    n_updates            | 12510       |
|    policy_gradient_loss | -0.018      |
|    std                  | 3.59        |
|    value_loss           | 0.00904     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 4.81e+03    |
| time/                   |             |
|    fps                  | 7290        |
|    iterations           | 1260        |
|    time_elapsed         | 5663        |
|    total_timesteps      | 41287680    |
| train/                  |             |
|    approx_kl            | 0.017004274 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.242      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 3.63        |
|    value_loss           | 0.0108      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 389        |
|    ep_rew_mean          | 4.96e+03   |
| time/                   |            |
|    fps                  | 7291       |
|    iterations           | 1270       |
|    time_elapsed         | 5707       |
|    total_timesteps      | 41615360   |
| train/                  |            |
|    approx_kl            | 0.01736335 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -41.5      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.24      |
|    n_updates            | 12690      |
|    policy_gradient_loss | -0.0192    |
|    std                  | 3.67       |
|    value_loss           | 0.0117     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.76e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1280        |
|    time_elapsed         | 5751        |
|    total_timesteps      | 41943040    |
| train/                  |             |
|    approx_kl            | 0.017719254 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.24       |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 3.72        |
|    value_loss           | 0.0126      |
-----------------------------------------
Eval num_timesteps=42000000, episode_reward=5592.67 +/- 264.78
Episode length: 408.40 +/- 14.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 408         |
|    mean_reward          | 5.59e+03    |
| time/                   |             |
|    total_timesteps      | 42000000    |
| train/                  |             |
|    approx_kl            | 0.017680012 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.239      |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 3.74        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | 5.02e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1290        |
|    time_elapsed         | 5797        |
|    total_timesteps      | 42270720    |
| train/                  |             |
|    approx_kl            | 0.017689805 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 3.79        |
|    value_loss           | 0.00899     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 376         |
|    ep_rew_mean          | 4.79e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1300        |
|    time_elapsed         | 5842        |
|    total_timesteps      | 42598400    |
| train/                  |             |
|    approx_kl            | 0.017659217 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.1       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.244      |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.02       |
|    std                  | 3.84        |
|    value_loss           | 0.00927     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 5.01e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1310        |
|    time_elapsed         | 5886        |
|    total_timesteps      | 42926080    |
| train/                  |             |
|    approx_kl            | 0.018011194 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.3       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13090       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.91        |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=43000000, episode_reward=5572.71 +/- 229.80
Episode length: 424.80 +/- 19.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 425        |
|    mean_reward          | 5.57e+03   |
| time/                   |            |
|    total_timesteps      | 43000000   |
| train/                  |            |
|    approx_kl            | 0.01777501 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.246     |
|    n_updates            | 13120      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 3.93       |
|    value_loss           | 0.0122     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 390        |
|    ep_rew_mean          | 5e+03      |
| time/                   |            |
|    fps                  | 7291       |
|    iterations           | 1320       |
|    time_elapsed         | 5932       |
|    total_timesteps      | 43253760   |
| train/                  |            |
|    approx_kl            | 0.01682643 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.6      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.245     |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.0192    |
|    std                  | 3.97       |
|    value_loss           | 0.00955    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 5e+03       |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1330        |
|    time_elapsed         | 5976        |
|    total_timesteps      | 43581440    |
| train/                  |             |
|    approx_kl            | 0.017707694 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.7       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.03        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1340        |
|    time_elapsed         | 6021        |
|    total_timesteps      | 43909120    |
| train/                  |             |
|    approx_kl            | 0.017714273 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.247      |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.1         |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=44000000, episode_reward=5444.22 +/- 137.16
Episode length: 399.80 +/- 17.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 400         |
|    mean_reward          | 5.44e+03    |
| time/                   |             |
|    total_timesteps      | 44000000    |
| train/                  |             |
|    approx_kl            | 0.017493758 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.251      |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 4.12        |
|    value_loss           | 0.00774     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.31e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1350        |
|    time_elapsed         | 6066        |
|    total_timesteps      | 44236800    |
| train/                  |             |
|    approx_kl            | 0.017894806 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.2       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.25       |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 4.18        |
|    value_loss           | 0.00802     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 379        |
|    ep_rew_mean          | 4.88e+03   |
| time/                   |            |
|    fps                  | 7291       |
|    iterations           | 1360       |
|    time_elapsed         | 6111       |
|    total_timesteps      | 44564480   |
| train/                  |            |
|    approx_kl            | 0.01755324 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -43.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.248     |
|    n_updates            | 13590      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 4.25       |
|    value_loss           | 0.0118     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 370        |
|    ep_rew_mean          | 4.84e+03   |
| time/                   |            |
|    fps                  | 7292       |
|    iterations           | 1370       |
|    time_elapsed         | 6156       |
|    total_timesteps      | 44892160   |
| train/                  |            |
|    approx_kl            | 0.01805049 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -43.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.252     |
|    n_updates            | 13690      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 4.3        |
|    value_loss           | 0.0129     |
----------------------------------------
Eval num_timesteps=45000000, episode_reward=4615.85 +/- 2134.32
Episode length: 353.40 +/- 150.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 353         |
|    mean_reward          | 4.62e+03    |
| time/                   |             |
|    total_timesteps      | 45000000    |
| train/                  |             |
|    approx_kl            | 0.017828993 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.252      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.34        |
|    value_loss           | 0.00832     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.91e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1380        |
|    time_elapsed         | 6201        |
|    total_timesteps      | 45219840    |
| train/                  |             |
|    approx_kl            | 0.016782328 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.249      |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 4.37        |
|    value_loss           | 0.0127      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 5.3e+03     |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1390        |
|    time_elapsed         | 6246        |
|    total_timesteps      | 45547520    |
| train/                  |             |
|    approx_kl            | 0.017340746 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.256      |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 4.44        |
|    value_loss           | 0.00838     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1400        |
|    time_elapsed         | 6290        |
|    total_timesteps      | 45875200    |
| train/                  |             |
|    approx_kl            | 0.017470203 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.253      |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.52        |
|    value_loss           | 0.00876     |
-----------------------------------------
Eval num_timesteps=46000000, episode_reward=5396.74 +/- 234.91
Episode length: 405.40 +/- 12.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 405         |
|    mean_reward          | 5.4e+03     |
| time/                   |             |
|    total_timesteps      | 46000000    |
| train/                  |             |
|    approx_kl            | 0.017440865 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 14030       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 4.55        |
|    value_loss           | 0.0128      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.09e+03    |
| time/                   |             |
|    fps                  | 7291        |
|    iterations           | 1410        |
|    time_elapsed         | 6336        |
|    total_timesteps      | 46202880    |
| train/                  |             |
|    approx_kl            | 0.018322617 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.254      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 4.59        |
|    value_loss           | 0.00963     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.89e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1420        |
|    time_elapsed         | 6380        |
|    total_timesteps      | 46530560    |
| train/                  |             |
|    approx_kl            | 0.018709254 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 4.68        |
|    value_loss           | 0.00952     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1430        |
|    time_elapsed         | 6425        |
|    total_timesteps      | 46858240    |
| train/                  |             |
|    approx_kl            | 0.018500438 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.9       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.256      |
|    n_updates            | 14290       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.76        |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=47000000, episode_reward=5567.11 +/- 127.24
Episode length: 442.20 +/- 55.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 442         |
|    mean_reward          | 5.57e+03    |
| time/                   |             |
|    total_timesteps      | 47000000    |
| train/                  |             |
|    approx_kl            | 0.017972529 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.257      |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 4.79        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1440        |
|    time_elapsed         | 6470        |
|    total_timesteps      | 47185920    |
| train/                  |             |
|    approx_kl            | 0.018135302 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.1       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 4.83        |
|    value_loss           | 0.0111      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 377        |
|    ep_rew_mean          | 4.78e+03   |
| time/                   |            |
|    fps                  | 7292       |
|    iterations           | 1450       |
|    time_elapsed         | 6515       |
|    total_timesteps      | 47513600   |
| train/                  |            |
|    approx_kl            | 0.01905592 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.4      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.26      |
|    n_updates            | 14490      |
|    policy_gradient_loss | -0.02      |
|    std                  | 4.92       |
|    value_loss           | 0.0141     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.87e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1460        |
|    time_elapsed         | 6559        |
|    total_timesteps      | 47841280    |
| train/                  |             |
|    approx_kl            | 0.018095752 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.5       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 14590       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 4.99        |
|    value_loss           | 0.0142      |
-----------------------------------------
Eval num_timesteps=48000000, episode_reward=5079.11 +/- 1345.23
Episode length: 395.80 +/- 58.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 396         |
|    mean_reward          | 5.08e+03    |
| time/                   |             |
|    total_timesteps      | 48000000    |
| train/                  |             |
|    approx_kl            | 0.018362785 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.7       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.262      |
|    n_updates            | 14640       |
|    policy_gradient_loss | -0.02       |
|    std                  | 5.03        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 384         |
|    ep_rew_mean          | 4.89e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1470        |
|    time_elapsed         | 6605        |
|    total_timesteps      | 48168960    |
| train/                  |             |
|    approx_kl            | 0.018041953 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 14690       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 5.07        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1480        |
|    time_elapsed         | 6649        |
|    total_timesteps      | 48496640    |
| train/                  |             |
|    approx_kl            | 0.018324332 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -46         |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 5.17        |
|    value_loss           | 0.0155      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.85e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1490        |
|    time_elapsed         | 6694        |
|    total_timesteps      | 48824320    |
| train/                  |             |
|    approx_kl            | 0.017617572 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.2       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.264      |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 5.24        |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=49000000, episode_reward=5687.60 +/- 90.06
Episode length: 432.40 +/- 9.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 5.69e+03    |
| time/                   |             |
|    total_timesteps      | 49000000    |
| train/                  |             |
|    approx_kl            | 0.017861772 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.263      |
|    n_updates            | 14950       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 5.31        |
|    value_loss           | 0.0147      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.9e+03    |
| time/                   |            |
|    fps                  | 7292       |
|    iterations           | 1500       |
|    time_elapsed         | 6740       |
|    total_timesteps      | 49152000   |
| train/                  |            |
|    approx_kl            | 0.01769726 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -46.4      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.264     |
|    n_updates            | 14990      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 5.35       |
|    value_loss           | 0.0163     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 386        |
|    ep_rew_mean          | 4.93e+03   |
| time/                   |            |
|    fps                  | 7292       |
|    iterations           | 1510       |
|    time_elapsed         | 6784       |
|    total_timesteps      | 49479680   |
| train/                  |            |
|    approx_kl            | 0.01776395 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -46.7      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.265     |
|    n_updates            | 15090      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 5.45       |
|    value_loss           | 0.0164     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 403         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1520        |
|    time_elapsed         | 6829        |
|    total_timesteps      | 49807360    |
| train/                  |             |
|    approx_kl            | 0.017881848 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.266      |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 5.53        |
|    value_loss           | 0.0117      |
-----------------------------------------
Eval num_timesteps=50000000, episode_reward=5845.80 +/- 95.41
Episode length: 447.40 +/- 14.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 447         |
|    mean_reward          | 5.85e+03    |
| time/                   |             |
|    total_timesteps      | 50000000    |
| train/                  |             |
|    approx_kl            | 0.017441723 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15250       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.58        |
|    value_loss           | 0.013       |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | 4.87e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1530        |
|    time_elapsed         | 6874        |
|    total_timesteps      | 50135040    |
| train/                  |             |
|    approx_kl            | 0.018137576 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.266      |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.019      |
|    std                  | 5.61        |
|    value_loss           | 0.0112      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1540        |
|    time_elapsed         | 6919        |
|    total_timesteps      | 50462720    |
| train/                  |             |
|    approx_kl            | 0.018313557 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 5.7         |
|    value_loss           | 0.0138      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 380         |
|    ep_rew_mean          | 4.92e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1550        |
|    time_elapsed         | 6963        |
|    total_timesteps      | 50790400    |
| train/                  |             |
|    approx_kl            | 0.019468626 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.3       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.269      |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 5.79        |
|    value_loss           | 0.0131      |
-----------------------------------------
Eval num_timesteps=51000000, episode_reward=5617.98 +/- 47.78
Episode length: 412.60 +/- 12.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 413         |
|    mean_reward          | 5.62e+03    |
| time/                   |             |
|    total_timesteps      | 51000000    |
| train/                  |             |
|    approx_kl            | 0.017993655 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.4       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15560       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 5.84        |
|    value_loss           | 0.0153      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 386         |
|    ep_rew_mean          | 4.99e+03    |
| time/                   |             |
|    fps                  | 7292        |
|    iterations           | 1560        |
|    time_elapsed         | 7009        |
|    total_timesteps      | 51118080    |
| train/                  |             |
|    approx_kl            | 0.017883794 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.269      |
|    n_updates            | 15590       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.86        |
|    value_loss           | 0.00916     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1570        |
|    time_elapsed         | 7053        |
|    total_timesteps      | 51445760    |
| train/                  |             |
|    approx_kl            | 0.017446391 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.7       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.271      |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 5.96        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1580        |
|    time_elapsed         | 7098        |
|    total_timesteps      | 51773440    |
| train/                  |             |
|    approx_kl            | 0.017643353 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.9       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 6.06        |
|    value_loss           | 0.012       |
-----------------------------------------
Eval num_timesteps=52000000, episode_reward=5660.47 +/- 105.45
Episode length: 422.20 +/- 17.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 422         |
|    mean_reward          | 5.66e+03    |
| time/                   |             |
|    total_timesteps      | 52000000    |
| train/                  |             |
|    approx_kl            | 0.018578522 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48         |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15860       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 6.12        |
|    value_loss           | 0.0129      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1590        |
|    time_elapsed         | 7143        |
|    total_timesteps      | 52101120    |
| train/                  |             |
|    approx_kl            | 0.017974555 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 6.16        |
|    value_loss           | 0.0119      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 389         |
|    ep_rew_mean          | 4.97e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1600        |
|    time_elapsed         | 7188        |
|    total_timesteps      | 52428800    |
| train/                  |             |
|    approx_kl            | 0.018748138 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 15990       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 6.27        |
|    value_loss           | 0.0117      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 388        |
|    ep_rew_mean          | 5.08e+03   |
| time/                   |            |
|    fps                  | 7294       |
|    iterations           | 1610       |
|    time_elapsed         | 7232       |
|    total_timesteps      | 52756480   |
| train/                  |            |
|    approx_kl            | 0.01735156 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.4      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.274     |
|    n_updates            | 16090      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 6.33       |
|    value_loss           | 0.0118     |
----------------------------------------
Eval num_timesteps=53000000, episode_reward=4965.02 +/- 1404.86
Episode length: 410.60 +/- 50.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 411         |
|    mean_reward          | 4.97e+03    |
| time/                   |             |
|    total_timesteps      | 53000000    |
| train/                  |             |
|    approx_kl            | 0.018463604 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.6       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 16170       |
|    policy_gradient_loss | -0.02       |
|    std                  | 6.4         |
|    value_loss           | 0.0132      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.18e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1620        |
|    time_elapsed         | 7278        |
|    total_timesteps      | 53084160    |
| train/                  |             |
|    approx_kl            | 0.016733533 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.276      |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 6.43        |
|    value_loss           | 0.00969     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 396        |
|    ep_rew_mean          | 5.16e+03   |
| time/                   |            |
|    fps                  | 7293       |
|    iterations           | 1630       |
|    time_elapsed         | 7322       |
|    total_timesteps      | 53411840   |
| train/                  |            |
|    approx_kl            | 0.01919121 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -48.8      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.275     |
|    n_updates            | 16290      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 6.53       |
|    value_loss           | 0.0121     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1640        |
|    time_elapsed         | 7367        |
|    total_timesteps      | 53739520    |
| train/                  |             |
|    approx_kl            | 0.016685616 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -49         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 16390       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 6.62        |
|    value_loss           | 0.0115      |
-----------------------------------------
Eval num_timesteps=54000000, episode_reward=3374.84 +/- 2167.86
Episode length: 312.80 +/- 143.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | 3.37e+03    |
| time/                   |             |
|    total_timesteps      | 54000000    |
| train/                  |             |
|    approx_kl            | 0.018962035 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.1       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.277      |
|    n_updates            | 16470       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 6.71        |
|    value_loss           | 0.0139      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 392        |
|    ep_rew_mean          | 5.04e+03   |
| time/                   |            |
|    fps                  | 7293       |
|    iterations           | 1650       |
|    time_elapsed         | 7413       |
|    total_timesteps      | 54067200   |
| train/                  |            |
|    approx_kl            | 0.01685045 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -49.2      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.278     |
|    n_updates            | 16490      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 6.75       |
|    value_loss           | 0.0116     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1660        |
|    time_elapsed         | 7457        |
|    total_timesteps      | 54394880    |
| train/                  |             |
|    approx_kl            | 0.017557856 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 16590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.81        |
|    value_loss           | 0.0104      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1670        |
|    time_elapsed         | 7502        |
|    total_timesteps      | 54722560    |
| train/                  |             |
|    approx_kl            | 0.017943181 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.5       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 16690       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 6.91        |
|    value_loss           | 0.00998     |
-----------------------------------------
Eval num_timesteps=55000000, episode_reward=5133.60 +/- 1271.56
Episode length: 407.20 +/- 37.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 407         |
|    mean_reward          | 5.13e+03    |
| time/                   |             |
|    total_timesteps      | 55000000    |
| train/                  |             |
|    approx_kl            | 0.017853606 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.7       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.28       |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 7.01        |
|    value_loss           | 0.0141      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 391         |
|    ep_rew_mean          | 4.96e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1680        |
|    time_elapsed         | 7548        |
|    total_timesteps      | 55050240    |
| train/                  |             |
|    approx_kl            | 0.017672388 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.7       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 16790       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.02        |
|    value_loss           | 0.0126      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 409         |
|    ep_rew_mean          | 5.34e+03    |
| time/                   |             |
|    fps                  | 7293        |
|    iterations           | 1690        |
|    time_elapsed         | 7592        |
|    total_timesteps      | 55377920    |
| train/                  |             |
|    approx_kl            | 0.018253844 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.284      |
|    n_updates            | 16890       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.13        |
|    value_loss           | 0.01        |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 391       |
|    ep_rew_mean          | 5.15e+03  |
| time/                   |           |
|    fps                  | 7294      |
|    iterations           | 1700      |
|    time_elapsed         | 7636      |
|    total_timesteps      | 55705600  |
| train/                  |           |
|    approx_kl            | 0.0173188 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -50.1     |
|    explained_variance   | 0.982     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.281    |
|    n_updates            | 16990     |
|    policy_gradient_loss | -0.0187   |
|    std                  | 7.28      |
|    value_loss           | 0.0117    |
---------------------------------------
Eval num_timesteps=56000000, episode_reward=5597.75 +/- 275.99
Episode length: 427.20 +/- 8.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 427         |
|    mean_reward          | 5.6e+03     |
| time/                   |             |
|    total_timesteps      | 56000000    |
| train/                  |             |
|    approx_kl            | 0.017918652 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.3       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 17080       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 7.4         |
|    value_loss           | 0.0142      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 405        |
|    ep_rew_mean          | 5.24e+03   |
| time/                   |            |
|    fps                  | 7293       |
|    iterations           | 1710       |
|    time_elapsed         | 7682       |
|    total_timesteps      | 56033280   |
| train/                  |            |
|    approx_kl            | 0.01761654 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.4      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.284     |
|    n_updates            | 17090      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 7.41       |
|    value_loss           | 0.0113     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 394         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1720        |
|    time_elapsed         | 7726        |
|    total_timesteps      | 56360960    |
| train/                  |             |
|    approx_kl            | 0.018609986 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.6       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 17190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.53        |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 384         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1730        |
|    time_elapsed         | 7771        |
|    total_timesteps      | 56688640    |
| train/                  |             |
|    approx_kl            | 0.016934318 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.284      |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 7.68        |
|    value_loss           | 0.0102      |
-----------------------------------------
Eval num_timesteps=57000000, episode_reward=5596.90 +/- 390.05
Episode length: 429.00 +/- 23.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 429         |
|    mean_reward          | 5.6e+03     |
| time/                   |             |
|    total_timesteps      | 57000000    |
| train/                  |             |
|    approx_kl            | 0.018105108 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 17390       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 7.82        |
|    value_loss           | 0.0121      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 7293     |
|    iterations      | 1740     |
|    time_elapsed    | 7816     |
|    total_timesteps | 57016320 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 382         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1750        |
|    time_elapsed         | 7861        |
|    total_timesteps      | 57344000    |
| train/                  |             |
|    approx_kl            | 0.018229883 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.289      |
|    n_updates            | 17490       |
|    policy_gradient_loss | -0.02       |
|    std                  | 7.97        |
|    value_loss           | 0.0141      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1760        |
|    time_elapsed         | 7905        |
|    total_timesteps      | 57671680    |
| train/                  |             |
|    approx_kl            | 0.018015455 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.5       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.292      |
|    n_updates            | 17590       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 8.12        |
|    value_loss           | 0.00955     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 397         |
|    ep_rew_mean          | 5.07e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1770        |
|    time_elapsed         | 7950        |
|    total_timesteps      | 57999360    |
| train/                  |             |
|    approx_kl            | 0.019724282 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.29       |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 8.23        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=58000000, episode_reward=5862.12 +/- 248.00
Episode length: 452.60 +/- 34.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 453         |
|    mean_reward          | 5.86e+03    |
| time/                   |             |
|    total_timesteps      | 58000000    |
| train/                  |             |
|    approx_kl            | 0.017835245 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.7       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 17700       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 8.23        |
|    value_loss           | 0.0115      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | 5.24e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1780        |
|    time_elapsed         | 7996        |
|    total_timesteps      | 58327040    |
| train/                  |             |
|    approx_kl            | 0.016612656 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.8       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.292      |
|    n_updates            | 17790       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 8.35        |
|    value_loss           | 0.0099      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1790        |
|    time_elapsed         | 8040        |
|    total_timesteps      | 58654720    |
| train/                  |             |
|    approx_kl            | 0.018090686 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52         |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 17890       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 8.48        |
|    value_loss           | 0.0117      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.17e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1800        |
|    time_elapsed         | 8085        |
|    total_timesteps      | 58982400    |
| train/                  |             |
|    approx_kl            | 0.017477863 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.1       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 17990       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 8.55        |
|    value_loss           | 0.00846     |
-----------------------------------------
Eval num_timesteps=59000000, episode_reward=5106.09 +/- 849.26
Episode length: 393.00 +/- 84.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 393        |
|    mean_reward          | 5.11e+03   |
| time/                   |            |
|    total_timesteps      | 59000000   |
| train/                  |            |
|    approx_kl            | 0.01763868 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.1      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.292     |
|    n_updates            | 18000      |
|    policy_gradient_loss | -0.019     |
|    std                  | 8.57       |
|    value_loss           | 0.0101     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 398         |
|    ep_rew_mean          | 5.18e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1810        |
|    time_elapsed         | 8130        |
|    total_timesteps      | 59310080    |
| train/                  |             |
|    approx_kl            | 0.018013297 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 18090       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 8.7         |
|    value_loss           | 0.00978     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1820        |
|    time_elapsed         | 8175        |
|    total_timesteps      | 59637760    |
| train/                  |             |
|    approx_kl            | 0.017770989 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.6       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 8.89        |
|    value_loss           | 0.00936     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1830        |
|    time_elapsed         | 8219        |
|    total_timesteps      | 59965440    |
| train/                  |             |
|    approx_kl            | 0.017772157 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.7       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 9.02        |
|    value_loss           | 0.0106      |
-----------------------------------------
Eval num_timesteps=60000000, episode_reward=5854.82 +/- 227.84
Episode length: 432.40 +/- 32.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 432         |
|    mean_reward          | 5.85e+03    |
| time/                   |             |
|    total_timesteps      | 60000000    |
| train/                  |             |
|    approx_kl            | 0.017018843 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18310       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 9.05        |
|    value_loss           | 0.0113      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 416        |
|    ep_rew_mean          | 5.38e+03   |
| time/                   |            |
|    fps                  | 7294       |
|    iterations           | 1840       |
|    time_elapsed         | 8265       |
|    total_timesteps      | 60293120   |
| train/                  |            |
|    approx_kl            | 0.01808159 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.9      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 18390      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 9.14       |
|    value_loss           | 0.0106     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 391        |
|    ep_rew_mean          | 5.02e+03   |
| time/                   |            |
|    fps                  | 7294       |
|    iterations           | 1850       |
|    time_elapsed         | 8310       |
|    total_timesteps      | 60620800   |
| train/                  |            |
|    approx_kl            | 0.01669118 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.1      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.296     |
|    n_updates            | 18490      |
|    policy_gradient_loss | -0.019     |
|    std                  | 9.26       |
|    value_loss           | 0.0131     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1860        |
|    time_elapsed         | 8354        |
|    total_timesteps      | 60948480    |
| train/                  |             |
|    approx_kl            | 0.017434653 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.2       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 9.37        |
|    value_loss           | 0.014       |
-----------------------------------------
Eval num_timesteps=61000000, episode_reward=6083.20 +/- 67.89
Episode length: 456.20 +/- 18.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 456         |
|    mean_reward          | 6.08e+03    |
| time/                   |             |
|    total_timesteps      | 61000000    |
| train/                  |             |
|    approx_kl            | 0.019613704 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.3        |
|    n_updates            | 18610       |
|    policy_gradient_loss | -0.0211     |
|    std                  | 9.39        |
|    value_loss           | 0.0151      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1870        |
|    time_elapsed         | 8400        |
|    total_timesteps      | 61276160    |
| train/                  |             |
|    approx_kl            | 0.017199576 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18690       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 9.51        |
|    value_loss           | 0.0129      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | 5.07e+03   |
| time/                   |            |
|    fps                  | 7294       |
|    iterations           | 1880       |
|    time_elapsed         | 8444       |
|    total_timesteps      | 61603840   |
| train/                  |            |
|    approx_kl            | 0.01882249 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.6      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.303     |
|    n_updates            | 18790      |
|    policy_gradient_loss | -0.0205    |
|    std                  | 9.66       |
|    value_loss           | 0.0121     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 407        |
|    ep_rew_mean          | 5.24e+03   |
| time/                   |            |
|    fps                  | 7295       |
|    iterations           | 1890       |
|    time_elapsed         | 8489       |
|    total_timesteps      | 61931520   |
| train/                  |            |
|    approx_kl            | 0.01830792 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -53.8      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.303     |
|    n_updates            | 18890      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 9.77       |
|    value_loss           | 0.0103     |
----------------------------------------
Eval num_timesteps=62000000, episode_reward=5361.54 +/- 1388.00
Episode length: 415.40 +/- 50.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 415         |
|    mean_reward          | 5.36e+03    |
| time/                   |             |
|    total_timesteps      | 62000000    |
| train/                  |             |
|    approx_kl            | 0.016916446 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.8       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18920       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 9.83        |
|    value_loss           | 0.0136      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | 5.34e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1900        |
|    time_elapsed         | 8535        |
|    total_timesteps      | 62259200    |
| train/                  |             |
|    approx_kl            | 0.016939014 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54         |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 18990       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 9.96        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 435         |
|    ep_rew_mean          | 5.58e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1910        |
|    time_elapsed         | 8579        |
|    total_timesteps      | 62586880    |
| train/                  |             |
|    approx_kl            | 0.017122135 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.302      |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 10.1        |
|    value_loss           | 0.0123      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 410         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1920        |
|    time_elapsed         | 8624        |
|    total_timesteps      | 62914560    |
| train/                  |             |
|    approx_kl            | 0.016799834 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.3       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 10.3        |
|    value_loss           | 0.00928     |
-----------------------------------------
Eval num_timesteps=63000000, episode_reward=6014.45 +/- 178.49
Episode length: 447.40 +/- 23.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 447         |
|    mean_reward          | 6.01e+03    |
| time/                   |             |
|    total_timesteps      | 63000000    |
| train/                  |             |
|    approx_kl            | 0.017309876 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.4       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 19220       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 10.3        |
|    value_loss           | 0.012       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 407         |
|    ep_rew_mean          | 5.27e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1930        |
|    time_elapsed         | 8670        |
|    total_timesteps      | 63242240    |
| train/                  |             |
|    approx_kl            | 0.016878678 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.5       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 19290       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 10.5        |
|    value_loss           | 0.0135      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | 5.51e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1940        |
|    time_elapsed         | 8714        |
|    total_timesteps      | 63569920    |
| train/                  |             |
|    approx_kl            | 0.016572013 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.7       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19390       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 10.6        |
|    value_loss           | 0.0091      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 404         |
|    ep_rew_mean          | 5.23e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1950        |
|    time_elapsed         | 8758        |
|    total_timesteps      | 63897600    |
| train/                  |             |
|    approx_kl            | 0.017150791 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.8       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 19490       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 10.8        |
|    value_loss           | 0.011       |
-----------------------------------------
Eval num_timesteps=64000000, episode_reward=6095.46 +/- 343.32
Episode length: 461.40 +/- 20.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 461         |
|    mean_reward          | 6.1e+03     |
| time/                   |             |
|    total_timesteps      | 64000000    |
| train/                  |             |
|    approx_kl            | 0.017232552 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.9       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19530       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 10.9        |
|    value_loss           | 0.0142      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 5.39e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1960        |
|    time_elapsed         | 8804        |
|    total_timesteps      | 64225280    |
| train/                  |             |
|    approx_kl            | 0.019352729 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.305      |
|    n_updates            | 19590       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 11          |
|    value_loss           | 0.0161      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.16e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1970        |
|    time_elapsed         | 8849        |
|    total_timesteps      | 64552960    |
| train/                  |             |
|    approx_kl            | 0.019251324 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.1       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19690       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 11.1        |
|    value_loss           | 0.0108      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 5.39e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 1980        |
|    time_elapsed         | 8893        |
|    total_timesteps      | 64880640    |
| train/                  |             |
|    approx_kl            | 0.017061004 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19790       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 11.3        |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=65000000, episode_reward=5247.23 +/- 1420.60
Episode length: 428.00 +/- 57.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 428         |
|    mean_reward          | 5.25e+03    |
| time/                   |             |
|    total_timesteps      | 65000000    |
| train/                  |             |
|    approx_kl            | 0.016918452 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.4       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19830       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 11.4        |
|    value_loss           | 0.0125      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | 5.2e+03     |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 1990        |
|    time_elapsed         | 8939        |
|    total_timesteps      | 65208320    |
| train/                  |             |
|    approx_kl            | 0.016969634 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.5       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.311      |
|    n_updates            | 19890       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 11.5        |
|    value_loss           | 0.00981     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 396         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2000        |
|    time_elapsed         | 8983        |
|    total_timesteps      | 65536000    |
| train/                  |             |
|    approx_kl            | 0.017929006 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.7       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.31       |
|    n_updates            | 19990       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 11.6        |
|    value_loss           | 0.0159      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 419         |
|    ep_rew_mean          | 5.44e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2010        |
|    time_elapsed         | 9028        |
|    total_timesteps      | 65863680    |
| train/                  |             |
|    approx_kl            | 0.017809657 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.9       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.313      |
|    n_updates            | 20090       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 11.9        |
|    value_loss           | 0.00959     |
-----------------------------------------
Eval num_timesteps=66000000, episode_reward=6048.83 +/- 272.01
Episode length: 471.20 +/- 23.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 471        |
|    mean_reward          | 6.05e+03   |
| time/                   |            |
|    total_timesteps      | 66000000   |
| train/                  |            |
|    approx_kl            | 0.01788121 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -56        |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.312     |
|    n_updates            | 20140      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 12         |
|    value_loss           | 0.0105     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.03e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2020        |
|    time_elapsed         | 9074        |
|    total_timesteps      | 66191360    |
| train/                  |             |
|    approx_kl            | 0.018064544 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.312      |
|    n_updates            | 20190       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 12.2        |
|    value_loss           | 0.0179      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 405         |
|    ep_rew_mean          | 5.26e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2030        |
|    time_elapsed         | 9118        |
|    total_timesteps      | 66519040    |
| train/                  |             |
|    approx_kl            | 0.016911138 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.3       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.311      |
|    n_updates            | 20290       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 12.4        |
|    value_loss           | 0.0136      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 398       |
|    ep_rew_mean          | 5.12e+03  |
| time/                   |           |
|    fps                  | 7295      |
|    iterations           | 2040      |
|    time_elapsed         | 9163      |
|    total_timesteps      | 66846720  |
| train/                  |           |
|    approx_kl            | 0.0167242 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.2       |
|    entropy_loss         | -56.5     |
|    explained_variance   | 0.983     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.314    |
|    n_updates            | 20390     |
|    policy_gradient_loss | -0.0183   |
|    std                  | 12.6      |
|    value_loss           | 0.0118    |
---------------------------------------
Eval num_timesteps=67000000, episode_reward=5057.54 +/- 2307.26
Episode length: 394.80 +/- 163.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 395         |
|    mean_reward          | 5.06e+03    |
| time/                   |             |
|    total_timesteps      | 67000000    |
| train/                  |             |
|    approx_kl            | 0.017972177 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.7       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 20440       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 12.8        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.12e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2050        |
|    time_elapsed         | 9208        |
|    total_timesteps      | 67174400    |
| train/                  |             |
|    approx_kl            | 0.017804846 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.315      |
|    n_updates            | 20490       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 12.9        |
|    value_loss           | 0.0158      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 413         |
|    ep_rew_mean          | 5.27e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2060        |
|    time_elapsed         | 9253        |
|    total_timesteps      | 67502080    |
| train/                  |             |
|    approx_kl            | 0.015889017 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57         |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.317      |
|    n_updates            | 20590       |
|    policy_gradient_loss | -0.019      |
|    std                  | 13.1        |
|    value_loss           | 0.00986     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | 5.11e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2070        |
|    time_elapsed         | 9297        |
|    total_timesteps      | 67829760    |
| train/                  |             |
|    approx_kl            | 0.018182486 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.2       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20690       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 13.4        |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=68000000, episode_reward=5318.83 +/- 2256.37
Episode length: 409.20 +/- 129.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 409        |
|    mean_reward          | 5.32e+03   |
| time/                   |            |
|    total_timesteps      | 68000000   |
| train/                  |            |
|    approx_kl            | 0.01706379 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.3      |
|    explained_variance   | 0.98       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.317     |
|    n_updates            | 20750      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 13.5       |
|    value_loss           | 0.014      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 385        |
|    ep_rew_mean          | 4.84e+03   |
| time/                   |            |
|    fps                  | 7294       |
|    iterations           | 2080       |
|    time_elapsed         | 9343       |
|    total_timesteps      | 68157440   |
| train/                  |            |
|    approx_kl            | 0.01683509 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.4      |
|    explained_variance   | 0.981      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20790      |
|    policy_gradient_loss | -0.0194    |
|    std                  | 13.5       |
|    value_loss           | 0.012      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 409         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2090        |
|    time_elapsed         | 9387        |
|    total_timesteps      | 68485120    |
| train/                  |             |
|    approx_kl            | 0.017089585 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.6       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.319      |
|    n_updates            | 20890       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 13.7        |
|    value_loss           | 0.0154      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | 4.93e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2100        |
|    time_elapsed         | 9432        |
|    total_timesteps      | 68812800    |
| train/                  |             |
|    approx_kl            | 0.016643738 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.8       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.321      |
|    n_updates            | 20990       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 14          |
|    value_loss           | 0.0158      |
-----------------------------------------
Eval num_timesteps=69000000, episode_reward=6308.69 +/- 234.45
Episode length: 478.60 +/- 18.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 479         |
|    mean_reward          | 6.31e+03    |
| time/                   |             |
|    total_timesteps      | 69000000    |
| train/                  |             |
|    approx_kl            | 0.017288234 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.9       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 21050       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 14.1        |
|    value_loss           | 0.0162      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 405         |
|    ep_rew_mean          | 5.11e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2110        |
|    time_elapsed         | 9477        |
|    total_timesteps      | 69140480    |
| train/                  |             |
|    approx_kl            | 0.017783184 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58         |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 21090       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 14.2        |
|    value_loss           | 0.0144      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 406         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2120        |
|    time_elapsed         | 9522        |
|    total_timesteps      | 69468160    |
| train/                  |             |
|    approx_kl            | 0.017607756 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.2       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 21190       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 14.5        |
|    value_loss           | 0.0154      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 412        |
|    ep_rew_mean          | 5.31e+03   |
| time/                   |            |
|    fps                  | 7295       |
|    iterations           | 2130       |
|    time_elapsed         | 9566       |
|    total_timesteps      | 69795840   |
| train/                  |            |
|    approx_kl            | 0.01652394 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -58.3      |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.306     |
|    n_updates            | 21290      |
|    policy_gradient_loss | -0.0135    |
|    std                  | 14.8       |
|    value_loss           | 0.0452     |
----------------------------------------
Eval num_timesteps=70000000, episode_reward=6436.27 +/- 422.78
Episode length: 480.00 +/- 20.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 480         |
|    mean_reward          | 6.44e+03    |
| time/                   |             |
|    total_timesteps      | 70000000    |
| train/                  |             |
|    approx_kl            | 0.018589355 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21360       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 15          |
|    value_loss           | 0.0177      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 407         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2140        |
|    time_elapsed         | 9612        |
|    total_timesteps      | 70123520    |
| train/                  |             |
|    approx_kl            | 0.018637206 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 21390       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 15.1        |
|    value_loss           | 0.0163      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 416         |
|    ep_rew_mean          | 5.3e+03     |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2150        |
|    time_elapsed         | 9657        |
|    total_timesteps      | 70451200    |
| train/                  |             |
|    approx_kl            | 0.017067645 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.7       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21490       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 15.3        |
|    value_loss           | 0.0171      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 426         |
|    ep_rew_mean          | 5.42e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2160        |
|    time_elapsed         | 9701        |
|    total_timesteps      | 70778880    |
| train/                  |             |
|    approx_kl            | 0.016991043 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59         |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21590       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 15.6        |
|    value_loss           | 0.0161      |
-----------------------------------------
Eval num_timesteps=71000000, episode_reward=9218.86 +/- 5119.55
Episode length: 502.60 +/- 25.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 503         |
|    mean_reward          | 9.22e+03    |
| time/                   |             |
|    total_timesteps      | 71000000    |
| train/                  |             |
|    approx_kl            | 0.015736392 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.1       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.298      |
|    n_updates            | 21660       |
|    policy_gradient_loss | -0.0101     |
|    std                  | 15.8        |
|    value_loss           | 0.0718      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.1e+03     |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2170        |
|    time_elapsed         | 9747        |
|    total_timesteps      | 71106560    |
| train/                  |             |
|    approx_kl            | 0.017583922 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.319      |
|    n_updates            | 21690       |
|    policy_gradient_loss | -0.017      |
|    std                  | 15.8        |
|    value_loss           | 0.0274      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 435         |
|    ep_rew_mean          | 5.53e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2180        |
|    time_elapsed         | 9792        |
|    total_timesteps      | 71434240    |
| train/                  |             |
|    approx_kl            | 0.016599052 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.4       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.326      |
|    n_updates            | 21790       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 16.1        |
|    value_loss           | 0.0237      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 402         |
|    ep_rew_mean          | 5.08e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2190        |
|    time_elapsed         | 9836        |
|    total_timesteps      | 71761920    |
| train/                  |             |
|    approx_kl            | 0.017078526 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.6       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21890       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 16.5        |
|    value_loss           | 0.0287      |
-----------------------------------------
Eval num_timesteps=72000000, episode_reward=6320.91 +/- 283.41
Episode length: 491.40 +/- 21.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 491         |
|    mean_reward          | 6.32e+03    |
| time/                   |             |
|    total_timesteps      | 72000000    |
| train/                  |             |
|    approx_kl            | 0.016642593 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.8       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21970       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 16.8        |
|    value_loss           | 0.0286      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 421         |
|    ep_rew_mean          | 5.29e+03    |
| time/                   |             |
|    fps                  | 7294        |
|    iterations           | 2200        |
|    time_elapsed         | 9882        |
|    total_timesteps      | 72089600    |
| train/                  |             |
|    approx_kl            | 0.016896073 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.8       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.328      |
|    n_updates            | 21990       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 16.9        |
|    value_loss           | 0.027       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 5.26e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2210        |
|    time_elapsed         | 9926        |
|    total_timesteps      | 72417280    |
| train/                  |             |
|    approx_kl            | 0.016335081 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.1       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 22090       |
|    policy_gradient_loss | -0.0171     |
|    std                  | 17.1        |
|    value_loss           | 0.0311      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 415         |
|    ep_rew_mean          | 5.14e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2220        |
|    time_elapsed         | 9971        |
|    total_timesteps      | 72744960    |
| train/                  |             |
|    approx_kl            | 0.016996188 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.3       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 22190       |
|    policy_gradient_loss | -0.0175     |
|    std                  | 17.5        |
|    value_loss           | 0.037       |
-----------------------------------------
Eval num_timesteps=73000000, episode_reward=6585.04 +/- 275.96
Episode length: 506.60 +/- 26.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 507         |
|    mean_reward          | 6.59e+03    |
| time/                   |             |
|    total_timesteps      | 73000000    |
| train/                  |             |
|    approx_kl            | 0.015411224 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.5       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 22270       |
|    policy_gradient_loss | -0.017      |
|    std                  | 17.8        |
|    value_loss           | 0.0439      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | 5.26e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2230        |
|    time_elapsed         | 10016       |
|    total_timesteps      | 73072640    |
| train/                  |             |
|    approx_kl            | 0.016300062 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.5       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 22290       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 17.8        |
|    value_loss           | 0.0375      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 392         |
|    ep_rew_mean          | 5.15e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2240        |
|    time_elapsed         | 10061       |
|    total_timesteps      | 73400320    |
| train/                  |             |
|    approx_kl            | 0.014669858 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.7       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.301      |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.0127     |
|    std                  | 18.1        |
|    value_loss           | 0.157       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 415         |
|    ep_rew_mean          | 5.22e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2250        |
|    time_elapsed         | 10105       |
|    total_timesteps      | 73728000    |
| train/                  |             |
|    approx_kl            | 0.017487153 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.9       |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.326      |
|    n_updates            | 22490       |
|    policy_gradient_loss | -0.0173     |
|    std                  | 18.5        |
|    value_loss           | 0.0472      |
-----------------------------------------
Eval num_timesteps=74000000, episode_reward=12069.05 +/- 7035.26
Episode length: 523.60 +/- 67.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 524         |
|    mean_reward          | 1.21e+04    |
| time/                   |             |
|    total_timesteps      | 74000000    |
| train/                  |             |
|    approx_kl            | 0.013688758 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.1       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.312      |
|    n_updates            | 22580       |
|    policy_gradient_loss | -0.0111     |
|    std                  | 18.9        |
|    value_loss           | 0.0911      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 423         |
|    ep_rew_mean          | 5.75e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2260        |
|    time_elapsed         | 10151       |
|    total_timesteps      | 74055680    |
| train/                  |             |
|    approx_kl            | 0.017313749 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.1       |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 22590       |
|    policy_gradient_loss | -0.0162     |
|    std                  | 18.9        |
|    value_loss           | 0.0543      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 393         |
|    ep_rew_mean          | 5.05e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2270        |
|    time_elapsed         | 10195       |
|    total_timesteps      | 74383360    |
| train/                  |             |
|    approx_kl            | 0.015251733 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.3       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.309      |
|    n_updates            | 22690       |
|    policy_gradient_loss | -0.0115     |
|    std                  | 19.3        |
|    value_loss           | 0.105       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 411         |
|    ep_rew_mean          | 5.14e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2280        |
|    time_elapsed         | 10240       |
|    total_timesteps      | 74711040    |
| train/                  |             |
|    approx_kl            | 0.017634634 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.5       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.33       |
|    n_updates            | 22790       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 19.7        |
|    value_loss           | 0.0516      |
-----------------------------------------
Eval num_timesteps=75000000, episode_reward=9344.59 +/- 3522.75
Episode length: 526.40 +/- 54.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 526         |
|    mean_reward          | 9.34e+03    |
| time/                   |             |
|    total_timesteps      | 75000000    |
| train/                  |             |
|    approx_kl            | 0.015231509 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.7       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.3        |
|    n_updates            | 22880       |
|    policy_gradient_loss | -0.0113     |
|    std                  | 20          |
|    value_loss           | 0.148       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 397        |
|    ep_rew_mean          | 4.95e+03   |
| time/                   |            |
|    fps                  | 7295       |
|    iterations           | 2290       |
|    time_elapsed         | 10285      |
|    total_timesteps      | 75038720   |
| train/                  |            |
|    approx_kl            | 0.01690935 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -61.7      |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.337     |
|    n_updates            | 22890      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 20.1       |
|    value_loss           | 0.032      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 408        |
|    ep_rew_mean          | 5.21e+03   |
| time/                   |            |
|    fps                  | 7295       |
|    iterations           | 2300       |
|    time_elapsed         | 10330      |
|    total_timesteps      | 75366400   |
| train/                  |            |
|    approx_kl            | 0.01745407 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -61.8      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.327     |
|    n_updates            | 22990      |
|    policy_gradient_loss | -0.0162    |
|    std                  | 20.4       |
|    value_loss           | 0.0572     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 407         |
|    ep_rew_mean          | 5.04e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2310        |
|    time_elapsed         | 10374       |
|    total_timesteps      | 75694080    |
| train/                  |             |
|    approx_kl            | 0.017912729 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62         |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.335      |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 20.7        |
|    value_loss           | 0.0407      |
-----------------------------------------
Eval num_timesteps=76000000, episode_reward=11845.32 +/- 6124.77
Episode length: 544.60 +/- 24.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 545         |
|    mean_reward          | 1.18e+04    |
| time/                   |             |
|    total_timesteps      | 76000000    |
| train/                  |             |
|    approx_kl            | 0.014247037 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.2       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 23190       |
|    policy_gradient_loss | -0.0105     |
|    std                  | 21.2        |
|    value_loss           | 0.277       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 406      |
|    ep_rew_mean     | 5.07e+03 |
| time/              |          |
|    fps             | 7295     |
|    iterations      | 2320     |
|    time_elapsed    | 10420    |
|    total_timesteps | 76021760 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 4.88e+03    |
| time/                   |             |
|    fps                  | 7295        |
|    iterations           | 2330        |
|    time_elapsed         | 10464       |
|    total_timesteps      | 76349440    |
| train/                  |             |
|    approx_kl            | 0.017476879 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.5       |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.326      |
|    n_updates            | 23290       |
|    policy_gradient_loss | -0.017      |
|    std                  | 21.7        |
|    value_loss           | 0.0826      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 5.37e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2340        |
|    time_elapsed         | 10509       |
|    total_timesteps      | 76677120    |
| train/                  |             |
|    approx_kl            | 0.013262241 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.8       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.308      |
|    n_updates            | 23390       |
|    policy_gradient_loss | -0.0124     |
|    std                  | 22.2        |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=77000000, episode_reward=10051.22 +/- 3320.06
Episode length: 524.80 +/- 31.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | 1.01e+04    |
| time/                   |             |
|    total_timesteps      | 77000000    |
| train/                  |             |
|    approx_kl            | 0.014657654 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -63         |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.288      |
|    n_updates            | 23490       |
|    policy_gradient_loss | -0.0118     |
|    std                  | 22.6        |
|    value_loss           | 0.21        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | 5.16e+03 |
| time/              |          |
|    fps             | 7295     |
|    iterations      | 2350     |
|    time_elapsed    | 10554    |
|    total_timesteps | 77004800 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 422         |
|    ep_rew_mean          | 5.19e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2360        |
|    time_elapsed         | 10599       |
|    total_timesteps      | 77332480    |
| train/                  |             |
|    approx_kl            | 0.016767662 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.3       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 23590       |
|    policy_gradient_loss | -0.0176     |
|    std                  | 23.1        |
|    value_loss           | 0.0976      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 414         |
|    ep_rew_mean          | 5.79e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2370        |
|    time_elapsed         | 10643       |
|    total_timesteps      | 77660160    |
| train/                  |             |
|    approx_kl            | 0.018952206 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.6       |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 23690       |
|    policy_gradient_loss | -0.0215     |
|    std                  | 23.8        |
|    value_loss           | 0.0458      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 409         |
|    ep_rew_mean          | 5.54e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2380        |
|    time_elapsed         | 10687       |
|    total_timesteps      | 77987840    |
| train/                  |             |
|    approx_kl            | 0.016908197 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.8       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.334      |
|    n_updates            | 23790       |
|    policy_gradient_loss | -0.017      |
|    std                  | 24.3        |
|    value_loss           | 0.0835      |
-----------------------------------------
Eval num_timesteps=78000000, episode_reward=8754.57 +/- 3040.22
Episode length: 553.00 +/- 32.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 553         |
|    mean_reward          | 8.75e+03    |
| time/                   |             |
|    total_timesteps      | 78000000    |
| train/                  |             |
|    approx_kl            | 0.012025915 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.9       |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.321      |
|    n_updates            | 23800       |
|    policy_gradient_loss | -0.0125     |
|    std                  | 24.3        |
|    value_loss           | 0.138       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 400         |
|    ep_rew_mean          | 4.87e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2390        |
|    time_elapsed         | 10733       |
|    total_timesteps      | 78315520    |
| train/                  |             |
|    approx_kl            | 0.015584409 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.1       |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 23890       |
|    policy_gradient_loss | -0.016      |
|    std                  | 24.6        |
|    value_loss           | 0.121       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 399         |
|    ep_rew_mean          | 4.99e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2400        |
|    time_elapsed         | 10777       |
|    total_timesteps      | 78643200    |
| train/                  |             |
|    approx_kl            | 0.017241366 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.3       |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.322      |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.0157     |
|    std                  | 25.1        |
|    value_loss           | 0.125       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 405         |
|    ep_rew_mean          | 5.15e+03    |
| time/                   |             |
|    fps                  | 7297        |
|    iterations           | 2410        |
|    time_elapsed         | 10822       |
|    total_timesteps      | 78970880    |
| train/                  |             |
|    approx_kl            | 0.014919192 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.6       |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.314      |
|    n_updates            | 24090       |
|    policy_gradient_loss | -0.0142     |
|    std                  | 25.8        |
|    value_loss           | 0.188       |
-----------------------------------------
Eval num_timesteps=79000000, episode_reward=9030.81 +/- 6011.64
Episode length: 508.80 +/- 71.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 509         |
|    mean_reward          | 9.03e+03    |
| time/                   |             |
|    total_timesteps      | 79000000    |
| train/                  |             |
|    approx_kl            | 0.015159723 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.6       |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.332      |
|    n_updates            | 24100       |
|    policy_gradient_loss | -0.0157     |
|    std                  | 25.9        |
|    value_loss           | 0.112       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 382         |
|    ep_rew_mean          | 4.83e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2420        |
|    time_elapsed         | 10867       |
|    total_timesteps      | 79298560    |
| train/                  |             |
|    approx_kl            | 0.017090421 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.8       |
|    explained_variance   | 0.874       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.324      |
|    n_updates            | 24190       |
|    policy_gradient_loss | -0.0155     |
|    std                  | 26.4        |
|    value_loss           | 0.17        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 387         |
|    ep_rew_mean          | 4.86e+03    |
| time/                   |             |
|    fps                  | 7296        |
|    iterations           | 2430        |
|    time_elapsed         | 10912       |
|    total_timesteps      | 79626240    |
| train/                  |             |
|    approx_kl            | 0.016057437 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -65         |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 24290       |
|    policy_gradient_loss | -0.0164     |
|    std                  | 27          |
|    value_loss           | 0.134       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 370        |
|    ep_rew_mean          | 5.39e+03   |
| time/                   |            |
|    fps                  | 7297       |
|    iterations           | 2440       |
|    time_elapsed         | 10956      |
|    total_timesteps      | 79953920   |
| train/                  |            |
|    approx_kl            | 0.01681019 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -65.3      |
|    explained_variance   | 0.895      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.334     |
|    n_updates            | 24390      |
|    policy_gradient_loss | -0.0178    |
|    std                  | 27.6       |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=80000000, episode_reward=13861.98 +/- 14006.84
Episode length: 414.20 +/- 235.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 414         |
|    mean_reward          | 1.39e+04    |
| time/                   |             |
|    total_timesteps      | 80000000    |
| train/                  |             |
|    approx_kl            | 0.018323433 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -65.3       |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 24410       |
|    policy_gradient_loss | -0.019      |
|    std                  | 27.6        |
|    value_loss           | 0.103       |
-----------------------------------------
New best mean reward!

============================================================
Training Complete!
============================================================
Total training time: 3:02:46
Total timesteps: 80,000,000
Timesteps per second: 7294.71
Run directory: /home/fspinto/projects/rl-humanoid/outputs/2025-12-23/10-37-28
============================================================

