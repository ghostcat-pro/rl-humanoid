
=== Merged Config ===
 exp_name: rl-humanoid
seed: 42
resume_from: null
paths:
  log_root: null
vecnorm:
  enabled: true
  clip_obs: 10.0
  gamma: 0.99
  epsilon: 1.0e-08
env:
  name: HumanoidCircuit-v0
  make_kwargs:
    render_mode: null
    waypoints:
    - - 8.0
      - 0.0
    - - 15.0
      - 0.0
    - - 20.0
      - 3.0
    waypoint_reach_threshold: 1.5
    stairs:
    - - 9.0
      - 5
      - 0.1
      - 0.8
    terrain_width: 15.0
    progress_reward_weight: 200.0
    waypoint_bonus: 150.0
    circuit_completion_bonus: 500.0
    height_reward_weight: 2.0
    forward_reward_weight: 1.0
    heading_reward_weight: 2.0
    balance_reward_weight: 0.5
    optimal_speed: 1.0
    speed_regulation_weight: 0.2
    ctrl_cost_weight: 0.1
    contact_cost_weight: 5.0e-07
    healthy_reward: 5.0
    terminate_when_unhealthy: true
    healthy_z_range:
    - 1.0
    - 2.0
    check_healthy_z_relative: true
  vec_env:
    n_envs: 8
    start_method: spawn
    monitor: true
algo:
  policy: MlpPolicy
  device: cuda
  policy_kwargs:
    net_arch:
    - 256
    - 256
  hyperparams:
    learning_rate: 0.0003
    n_steps: 4096
    batch_size: 16384
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.005
    vf_coef: 0.25
    max_grad_norm: 0.5
training:
  total_timesteps: 80000000
  log_interval: 10
  checkpoint_every_steps: 250000

Logging to /home/fspinto/projects/rl-humanoid/outputs/2025-12-24/16-29-37
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
Using cuda device

=== Starting Training ===
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.5        |
|    ep_rew_mean          | 204         |
| time/                   |             |
|    fps                  | 5486        |
|    iterations           | 10          |
|    time_elapsed         | 59          |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.013581425 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0841     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0143     |
|    std                  | 0.999       |
|    value_loss           | 0.269       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 54.1        |
|    ep_rew_mean          | 282         |
| time/                   |             |
|    fps                  | 5542        |
|    iterations           | 20          |
|    time_elapsed         | 118         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.010479006 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.929       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.126      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0138     |
|    std                  | 0.994       |
|    value_loss           | 0.086       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 59.9        |
|    ep_rew_mean          | 326         |
| time/                   |             |
|    fps                  | 5572        |
|    iterations           | 30          |
|    time_elapsed         | 176         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.009384461 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0127     |
|    std                  | 0.989       |
|    value_loss           | 0.0597      |
-----------------------------------------
/home/fspinto/projects/rl-humanoid/.venv/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000000, episode_reward=426.85 +/- 15.68
Episode length: 72.40 +/- 3.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.4        |
|    mean_reward          | 427         |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.010774551 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0139     |
|    std                  | 0.988       |
|    value_loss           | 0.0595      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 63.9       |
|    ep_rew_mean          | 357        |
| time/                   |            |
|    fps                  | 5585       |
|    iterations           | 40         |
|    time_elapsed         | 234        |
|    total_timesteps      | 1310720    |
| train/                  |            |
|    approx_kl            | 0.01163305 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.8      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0141    |
|    std                  | 0.986      |
|    value_loss           | 0.0498     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 69          |
|    ep_rew_mean          | 406         |
| time/                   |             |
|    fps                  | 5596        |
|    iterations           | 50          |
|    time_elapsed         | 292         |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.011208727 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0138     |
|    std                  | 0.98        |
|    value_loss           | 0.0399      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 77.7         |
|    ep_rew_mean          | 461          |
| time/                   |              |
|    fps                  | 5602         |
|    iterations           | 60           |
|    time_elapsed         | 350          |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0115998825 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.6        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.131       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.0137      |
|    std                  | 0.976        |
|    value_loss           | 0.0528       |
------------------------------------------
Eval num_timesteps=2000000, episode_reward=697.63 +/- 146.43
Episode length: 98.00 +/- 6.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98         |
|    mean_reward          | 698        |
| time/                   |            |
|    total_timesteps      | 2000000    |
| train/                  |            |
|    approx_kl            | 0.01198823 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.6      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.133     |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0143    |
|    std                  | 0.975      |
|    value_loss           | 0.0486     |
----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 86.9       |
|    ep_rew_mean          | 523        |
| time/                   |            |
|    fps                  | 5601       |
|    iterations           | 70         |
|    time_elapsed         | 409        |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.01156688 |
|    clip_fraction        | 0.0983     |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.129     |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.014     |
|    std                  | 0.971      |
|    value_loss           | 0.0631     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.1        |
|    ep_rew_mean          | 558         |
| time/                   |             |
|    fps                  | 5598        |
|    iterations           | 80          |
|    time_elapsed         | 468         |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.012358362 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.971       |
|    value_loss           | 0.0602      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96          |
|    ep_rew_mean          | 629         |
| time/                   |             |
|    fps                  | 5598        |
|    iterations           | 90          |
|    time_elapsed         | 526         |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.011932401 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.129      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0145     |
|    std                  | 0.972       |
|    value_loss           | 0.0613      |
-----------------------------------------
Eval num_timesteps=3000000, episode_reward=669.77 +/- 172.78
Episode length: 91.00 +/- 7.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91          |
|    mean_reward          | 670         |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.011875549 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.128      |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.0142     |
|    std                  | 0.974       |
|    value_loss           | 0.0647      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.7        |
|    ep_rew_mean          | 663         |
| time/                   |             |
|    fps                  | 5594        |
|    iterations           | 100         |
|    time_elapsed         | 585         |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.013804473 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.13       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0144     |
|    std                  | 0.976       |
|    value_loss           | 0.0617      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 702         |
| time/                   |             |
|    fps                  | 5594        |
|    iterations           | 110         |
|    time_elapsed         | 644         |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.012326572 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0146     |
|    std                  | 0.973       |
|    value_loss           | 0.0524      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 737         |
| time/                   |             |
|    fps                  | 5591        |
|    iterations           | 120         |
|    time_elapsed         | 703         |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.012676991 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 0.971       |
|    value_loss           | 0.049       |
-----------------------------------------
Eval num_timesteps=4000000, episode_reward=837.35 +/- 153.48
Episode length: 102.00 +/- 8.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 837         |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.013506838 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.131      |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 0.971       |
|    value_loss           | 0.0534      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 800         |
| time/                   |             |
|    fps                  | 5585        |
|    iterations           | 130         |
|    time_elapsed         | 762         |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.013423002 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.132      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0154     |
|    std                  | 0.974       |
|    value_loss           | 0.0545      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 786         |
| time/                   |             |
|    fps                  | 5583        |
|    iterations           | 140         |
|    time_elapsed         | 821         |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.013816403 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0152     |
|    std                  | 0.979       |
|    value_loss           | 0.0492      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 865         |
| time/                   |             |
|    fps                  | 5582        |
|    iterations           | 150         |
|    time_elapsed         | 880         |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.013585769 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0158     |
|    std                  | 0.979       |
|    value_loss           | 0.0503      |
-----------------------------------------
Eval num_timesteps=5000000, episode_reward=930.29 +/- 274.51
Episode length: 107.80 +/- 20.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 930         |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.013819355 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.0157     |
|    std                  | 0.982       |
|    value_loss           | 0.0492      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 946         |
| time/                   |             |
|    fps                  | 5576        |
|    iterations           | 160         |
|    time_elapsed         | 940         |
|    total_timesteps      | 5242880     |
| train/                  |             |
|    approx_kl            | 0.013848477 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.133      |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0165     |
|    std                  | 0.982       |
|    value_loss           | 0.0527      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 5575        |
|    iterations           | 170         |
|    time_elapsed         | 999         |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.014862953 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.136      |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0174     |
|    std                  | 0.985       |
|    value_loss           | 0.0552      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 133         |
|    ep_rew_mean          | 1.1e+03     |
| time/                   |             |
|    fps                  | 5575        |
|    iterations           | 180         |
|    time_elapsed         | 1057        |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.015810046 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 0.985       |
|    value_loss           | 0.0558      |
-----------------------------------------
Eval num_timesteps=6000000, episode_reward=1311.32 +/- 367.81
Episode length: 145.80 +/- 21.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 1.31e+03    |
| time/                   |             |
|    total_timesteps      | 6000000     |
| train/                  |             |
|    approx_kl            | 0.015644066 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.134      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.018      |
|    std                  | 0.986       |
|    value_loss           | 0.0643      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | 1.28e+03   |
| time/                   |            |
|    fps                  | 5573       |
|    iterations           | 190        |
|    time_elapsed         | 1117       |
|    total_timesteps      | 6225920    |
| train/                  |            |
|    approx_kl            | 0.01615917 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.5      |
|    explained_variance   | 0.959      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.137     |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0189    |
|    std                  | 0.985      |
|    value_loss           | 0.0616     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | 1.47e+03     |
| time/                   |              |
|    fps                  | 5573         |
|    iterations           | 200          |
|    time_elapsed         | 1175         |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0154204015 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    entropy_loss         | -23.4        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.133       |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.0182      |
|    std                  | 0.984        |
|    value_loss           | 0.0724       |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 171       |
|    ep_rew_mean          | 1.57e+03  |
| time/                   |           |
|    fps                  | 5573      |
|    iterations           | 210       |
|    time_elapsed         | 1234      |
|    total_timesteps      | 6881280   |
| train/                  |           |
|    approx_kl            | 0.0165725 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -23.4     |
|    explained_variance   | 0.959     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.137    |
|    n_updates            | 2090      |
|    policy_gradient_loss | -0.0193   |
|    std                  | 0.983     |
|    value_loss           | 0.0578    |
---------------------------------------
Eval num_timesteps=7000000, episode_reward=2071.26 +/- 271.52
Episode length: 198.40 +/- 19.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 2.07e+03    |
| time/                   |             |
|    total_timesteps      | 7000000     |
| train/                  |             |
|    approx_kl            | 0.016080875 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.134      |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 0.982       |
|    value_loss           | 0.0693      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 175         |
|    ep_rew_mean          | 1.69e+03    |
| time/                   |             |
|    fps                  | 5571        |
|    iterations           | 220         |
|    time_elapsed         | 1293        |
|    total_timesteps      | 7208960     |
| train/                  |             |
|    approx_kl            | 0.015156586 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0184     |
|    std                  | 0.983       |
|    value_loss           | 0.0608      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 1.85e+03    |
| time/                   |             |
|    fps                  | 5571        |
|    iterations           | 230         |
|    time_elapsed         | 1352        |
|    total_timesteps      | 7536640     |
| train/                  |             |
|    approx_kl            | 0.016225591 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 0.988       |
|    value_loss           | 0.0613      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 1.98e+03   |
| time/                   |            |
|    fps                  | 5572       |
|    iterations           | 240        |
|    time_elapsed         | 1411       |
|    total_timesteps      | 7864320    |
| train/                  |            |
|    approx_kl            | 0.01630918 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.4      |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.137     |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.019     |
|    std                  | 0.99       |
|    value_loss           | 0.063      |
----------------------------------------
Eval num_timesteps=8000000, episode_reward=3273.14 +/- 422.68
Episode length: 268.20 +/- 25.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 3.27e+03    |
| time/                   |             |
|    total_timesteps      | 8000000     |
| train/                  |             |
|    approx_kl            | 0.017386848 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.4       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.136      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.02       |
|    std                  | 0.992       |
|    value_loss           | 0.0601      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 2.25e+03    |
| time/                   |             |
|    fps                  | 5569        |
|    iterations           | 250         |
|    time_elapsed         | 1470        |
|    total_timesteps      | 8192000     |
| train/                  |             |
|    approx_kl            | 0.016845211 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 0.994       |
|    value_loss           | 0.0563      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 213         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 5570        |
|    iterations           | 260         |
|    time_elapsed         | 1529        |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.016787868 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.135      |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 0.998       |
|    value_loss           | 0.0635      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 228         |
|    ep_rew_mean          | 2.54e+03    |
| time/                   |             |
|    fps                  | 5572        |
|    iterations           | 270         |
|    time_elapsed         | 1587        |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.017885346 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.137      |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1           |
|    value_loss           | 0.0644      |
-----------------------------------------
Eval num_timesteps=9000000, episode_reward=4791.69 +/- 1144.42
Episode length: 355.00 +/- 69.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 355         |
|    mean_reward          | 4.79e+03    |
| time/                   |             |
|    total_timesteps      | 9000000     |
| train/                  |             |
|    approx_kl            | 0.016771179 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.5       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.138      |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.0193     |
|    std                  | 1           |
|    value_loss           | 0.0605      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 235         |
|    ep_rew_mean          | 2.58e+03    |
| time/                   |             |
|    fps                  | 5568        |
|    iterations           | 280         |
|    time_elapsed         | 1647        |
|    total_timesteps      | 9175040     |
| train/                  |             |
|    approx_kl            | 0.016511261 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.6       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.01        |
|    value_loss           | 0.0614      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 259         |
|    ep_rew_mean          | 3e+03       |
| time/                   |             |
|    fps                  | 5569        |
|    iterations           | 290         |
|    time_elapsed         | 1706        |
|    total_timesteps      | 9502720     |
| train/                  |             |
|    approx_kl            | 0.017086849 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.7       |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.141      |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.02        |
|    value_loss           | 0.0592      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 256        |
|    ep_rew_mean          | 3.09e+03   |
| time/                   |            |
|    fps                  | 5570       |
|    iterations           | 300        |
|    time_elapsed         | 1764       |
|    total_timesteps      | 9830400    |
| train/                  |            |
|    approx_kl            | 0.01637606 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.7      |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.137     |
|    n_updates            | 2990       |
|    policy_gradient_loss | -0.0188    |
|    std                  | 1.02       |
|    value_loss           | 0.0687     |
----------------------------------------
Eval num_timesteps=10000000, episode_reward=6195.35 +/- 249.34
Episode length: 439.20 +/- 19.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 439        |
|    mean_reward          | 6.2e+03    |
| time/                   |            |
|    total_timesteps      | 10000000   |
| train/                  |            |
|    approx_kl            | 0.01677856 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -23.8      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.141     |
|    n_updates            | 3050       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 1.02       |
|    value_loss           | 0.0568     |
----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 279         |
|    ep_rew_mean          | 3.45e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 310         |
|    time_elapsed         | 1824        |
|    total_timesteps      | 10158080    |
| train/                  |             |
|    approx_kl            | 0.017168343 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.8       |
|    explained_variance   | 0.94        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.139      |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.03        |
|    value_loss           | 0.068       |
-----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 283      |
|    ep_rew_mean          | 3.54e+03 |
| time/                   |          |
|    fps                  | 5567     |
|    iterations           | 320      |
|    time_elapsed         | 1883     |
|    total_timesteps      | 10485760 |
| train/                  |          |
|    approx_kl            | 0.017393 |
|    clip_fraction        | 0.149    |
|    clip_range           | 0.2      |
|    entropy_loss         | -23.8    |
|    explained_variance   | 0.944    |
|    learning_rate        | 0.0003   |
|    loss                 | -0.139   |
|    n_updates            | 3190     |
|    policy_gradient_loss | -0.0194  |
|    std                  | 1.03     |
|    value_loss           | 0.0583   |
--------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 302         |
|    ep_rew_mean          | 3.81e+03    |
| time/                   |             |
|    fps                  | 5568        |
|    iterations           | 330         |
|    time_elapsed         | 1941        |
|    total_timesteps      | 10813440    |
| train/                  |             |
|    approx_kl            | 0.017165959 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.939       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.14       |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.04        |
|    value_loss           | 0.0599      |
-----------------------------------------
Eval num_timesteps=11000000, episode_reward=5975.99 +/- 104.66
Episode length: 417.60 +/- 15.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 418         |
|    mean_reward          | 5.98e+03    |
| time/                   |             |
|    total_timesteps      | 11000000    |
| train/                  |             |
|    approx_kl            | 0.017669035 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -23.9       |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.145      |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.04        |
|    value_loss           | 0.0468      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 315         |
|    ep_rew_mean          | 4.02e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 340         |
|    time_elapsed         | 2001        |
|    total_timesteps      | 11141120    |
| train/                  |             |
|    approx_kl            | 0.017978363 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24         |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.144      |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.04        |
|    value_loss           | 0.053       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 329         |
|    ep_rew_mean          | 4.3e+03     |
| time/                   |             |
|    fps                  | 5567        |
|    iterations           | 350         |
|    time_elapsed         | 2059        |
|    total_timesteps      | 11468800    |
| train/                  |             |
|    approx_kl            | 0.017153792 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.1       |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.146      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 1.05        |
|    value_loss           | 0.0448      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 329        |
|    ep_rew_mean          | 4.27e+03   |
| time/                   |            |
|    fps                  | 5568       |
|    iterations           | 360        |
|    time_elapsed         | 2118       |
|    total_timesteps      | 11796480   |
| train/                  |            |
|    approx_kl            | 0.01729025 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.946      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.147     |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 1.06       |
|    value_loss           | 0.0436     |
----------------------------------------
Eval num_timesteps=12000000, episode_reward=5951.63 +/- 398.42
Episode length: 405.00 +/- 22.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 405        |
|    mean_reward          | 5.95e+03   |
| time/                   |            |
|    total_timesteps      | 12000000   |
| train/                  |            |
|    approx_kl            | 0.01641857 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.2      |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.147     |
|    n_updates            | 3660       |
|    policy_gradient_loss | -0.0194    |
|    std                  | 1.07       |
|    value_loss           | 0.0413     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 297         |
|    ep_rew_mean          | 3.92e+03    |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 370         |
|    time_elapsed         | 2178        |
|    total_timesteps      | 12124160    |
| train/                  |             |
|    approx_kl            | 0.016930088 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.3       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.147      |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.07        |
|    value_loss           | 0.0391      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 318         |
|    ep_rew_mean          | 4.23e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 380         |
|    time_elapsed         | 2236        |
|    total_timesteps      | 12451840    |
| train/                  |             |
|    approx_kl            | 0.017506657 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.4       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.15       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.08        |
|    value_loss           | 0.0349      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 346         |
|    ep_rew_mean          | 4.78e+03    |
| time/                   |             |
|    fps                  | 5567        |
|    iterations           | 390         |
|    time_elapsed         | 2295        |
|    total_timesteps      | 12779520    |
| train/                  |             |
|    approx_kl            | 0.016803037 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.6       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.149      |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.09        |
|    value_loss           | 0.0323      |
-----------------------------------------
Eval num_timesteps=13000000, episode_reward=3968.67 +/- 2406.10
Episode length: 291.40 +/- 133.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 291         |
|    mean_reward          | 3.97e+03    |
| time/                   |             |
|    total_timesteps      | 13000000    |
| train/                  |             |
|    approx_kl            | 0.018436018 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.1         |
|    value_loss           | 0.0328      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 343         |
|    ep_rew_mean          | 4.8e+03     |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 400         |
|    time_elapsed         | 2355        |
|    total_timesteps      | 13107200    |
| train/                  |             |
|    approx_kl            | 0.018020507 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -24.7       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.152      |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.1         |
|    value_loss           | 0.0326      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 347        |
|    ep_rew_mean          | 4.85e+03   |
| time/                   |            |
|    fps                  | 5566       |
|    iterations           | 410        |
|    time_elapsed         | 2413       |
|    total_timesteps      | 13434880   |
| train/                  |            |
|    approx_kl            | 0.01778049 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -24.9      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.154     |
|    n_updates            | 4090       |
|    policy_gradient_loss | -0.02      |
|    std                  | 1.12       |
|    value_loss           | 0.0258     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 332        |
|    ep_rew_mean          | 4.58e+03   |
| time/                   |            |
|    fps                  | 5567       |
|    iterations           | 420        |
|    time_elapsed         | 2471       |
|    total_timesteps      | 13762560   |
| train/                  |            |
|    approx_kl            | 0.01927424 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25        |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.155     |
|    n_updates            | 4190       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 1.13       |
|    value_loss           | 0.0275     |
----------------------------------------
Eval num_timesteps=14000000, episode_reward=5936.12 +/- 258.61
Episode length: 384.00 +/- 20.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 5.94e+03    |
| time/                   |             |
|    total_timesteps      | 14000000    |
| train/                  |             |
|    approx_kl            | 0.017760515 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.2       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.155      |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.14        |
|    value_loss           | 0.0259      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 353        |
|    ep_rew_mean          | 5.04e+03   |
| time/                   |            |
|    fps                  | 5565       |
|    iterations           | 430        |
|    time_elapsed         | 2531       |
|    total_timesteps      | 14090240   |
| train/                  |            |
|    approx_kl            | 0.01711032 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.2      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.155     |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 1.14       |
|    value_loss           | 0.0245     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 338         |
|    ep_rew_mean          | 4.77e+03    |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 440         |
|    time_elapsed         | 2590        |
|    total_timesteps      | 14417920    |
| train/                  |             |
|    approx_kl            | 0.017397773 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.3       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.158      |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.15        |
|    value_loss           | 0.0245      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 333        |
|    ep_rew_mean          | 4.78e+03   |
| time/                   |            |
|    fps                  | 5566       |
|    iterations           | 450        |
|    time_elapsed         | 2648       |
|    total_timesteps      | 14745600   |
| train/                  |            |
|    approx_kl            | 0.01857185 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.4      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.158     |
|    n_updates            | 4490       |
|    policy_gradient_loss | -0.0201    |
|    std                  | 1.16       |
|    value_loss           | 0.0232     |
----------------------------------------
Eval num_timesteps=15000000, episode_reward=5836.30 +/- 130.22
Episode length: 372.80 +/- 12.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 373         |
|    mean_reward          | 5.84e+03    |
| time/                   |             |
|    total_timesteps      | 15000000    |
| train/                  |             |
|    approx_kl            | 0.018757861 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.6       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.159      |
|    n_updates            | 4570        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 1.17        |
|    value_loss           | 0.0256      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 340        |
|    ep_rew_mean          | 4.87e+03   |
| time/                   |            |
|    fps                  | 5564       |
|    iterations           | 460        |
|    time_elapsed         | 2708       |
|    total_timesteps      | 15073280   |
| train/                  |            |
|    approx_kl            | 0.01837307 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -25.6      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.159     |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.0207    |
|    std                  | 1.17       |
|    value_loss           | 0.0256     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 347         |
|    ep_rew_mean          | 5.09e+03    |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 470         |
|    time_elapsed         | 2767        |
|    total_timesteps      | 15400960    |
| train/                  |             |
|    approx_kl            | 0.017849274 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.7       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.157      |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 1.18        |
|    value_loss           | 0.0241      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 350         |
|    ep_rew_mean          | 5.18e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 480         |
|    time_elapsed         | 2825        |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.018668966 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -25.9       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.2         |
|    value_loss           | 0.0182      |
-----------------------------------------
Eval num_timesteps=16000000, episode_reward=5968.69 +/- 227.50
Episode length: 384.40 +/- 9.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 384        |
|    mean_reward          | 5.97e+03   |
| time/                   |            |
|    total_timesteps      | 16000000   |
| train/                  |            |
|    approx_kl            | 0.01780583 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -26.1      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.16      |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.21       |
|    value_loss           | 0.0235     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 349         |
|    ep_rew_mean          | 5.15e+03    |
| time/                   |             |
|    fps                  | 5564        |
|    iterations           | 490         |
|    time_elapsed         | 2885        |
|    total_timesteps      | 16056320    |
| train/                  |             |
|    approx_kl            | 0.017775685 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.1       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.16       |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.22        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 311         |
|    ep_rew_mean          | 4.45e+03    |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 500         |
|    time_elapsed         | 2944        |
|    total_timesteps      | 16384000    |
| train/                  |             |
|    approx_kl            | 0.017759971 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.2       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.162      |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.23        |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 349         |
|    ep_rew_mean          | 5.17e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 510         |
|    time_elapsed         | 3002        |
|    total_timesteps      | 16711680    |
| train/                  |             |
|    approx_kl            | 0.018245235 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.4       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.164      |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.24        |
|    value_loss           | 0.0155      |
-----------------------------------------
Eval num_timesteps=17000000, episode_reward=5987.15 +/- 350.87
Episode length: 383.60 +/- 26.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 384         |
|    mean_reward          | 5.99e+03    |
| time/                   |             |
|    total_timesteps      | 17000000    |
| train/                  |             |
|    approx_kl            | 0.017230341 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.161      |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.25        |
|    value_loss           | 0.0214      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 327         |
|    ep_rew_mean          | 4.77e+03    |
| time/                   |             |
|    fps                  | 5564        |
|    iterations           | 520         |
|    time_elapsed         | 3062        |
|    total_timesteps      | 17039360    |
| train/                  |             |
|    approx_kl            | 0.017900895 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.5       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.163      |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.26        |
|    value_loss           | 0.019       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 361       |
|    ep_rew_mean          | 5.37e+03  |
| time/                   |           |
|    fps                  | 5565      |
|    iterations           | 530       |
|    time_elapsed         | 3120      |
|    total_timesteps      | 17367040  |
| train/                  |           |
|    approx_kl            | 0.0168055 |
|    clip_fraction        | 0.155     |
|    clip_range           | 0.2       |
|    entropy_loss         | -26.7     |
|    explained_variance   | 0.968     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.164    |
|    n_updates            | 5290      |
|    policy_gradient_loss | -0.0196   |
|    std                  | 1.27      |
|    value_loss           | 0.0196    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 364         |
|    ep_rew_mean          | 5.4e+03     |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 540         |
|    time_elapsed         | 3178        |
|    total_timesteps      | 17694720    |
| train/                  |             |
|    approx_kl            | 0.020058235 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -26.9       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.167      |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 1.29        |
|    value_loss           | 0.0177      |
-----------------------------------------
Eval num_timesteps=18000000, episode_reward=5032.52 +/- 2272.92
Episode length: 350.80 +/- 128.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 351         |
|    mean_reward          | 5.03e+03    |
| time/                   |             |
|    total_timesteps      | 18000000    |
| train/                  |             |
|    approx_kl            | 0.018201526 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.165      |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.3         |
|    value_loss           | 0.0219      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 5564     |
|    iterations      | 550      |
|    time_elapsed    | 3238     |
|    total_timesteps | 18022400 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | 5.54e+03    |
| time/                   |             |
|    fps                  | 5565        |
|    iterations           | 560         |
|    time_elapsed         | 3296        |
|    total_timesteps      | 18350080    |
| train/                  |             |
|    approx_kl            | 0.017758768 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.1       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.168      |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.31        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 350         |
|    ep_rew_mean          | 5.13e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 570         |
|    time_elapsed         | 3355        |
|    total_timesteps      | 18677760    |
| train/                  |             |
|    approx_kl            | 0.017564893 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.3       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.169      |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.33        |
|    value_loss           | 0.0139      |
-----------------------------------------
Eval num_timesteps=19000000, episode_reward=6333.23 +/- 285.10
Episode length: 392.00 +/- 16.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 392         |
|    mean_reward          | 6.33e+03    |
| time/                   |             |
|    total_timesteps      | 19000000    |
| train/                  |             |
|    approx_kl            | 0.017552696 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.5       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.167      |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.0196     |
|    std                  | 1.34        |
|    value_loss           | 0.0167      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | 5.37e+03 |
| time/              |          |
|    fps             | 5564     |
|    iterations      | 580      |
|    time_elapsed    | 3415     |
|    total_timesteps | 19005440 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 367         |
|    ep_rew_mean          | 5.5e+03     |
| time/                   |             |
|    fps                  | 5564        |
|    iterations           | 590         |
|    time_elapsed         | 3474        |
|    total_timesteps      | 19333120    |
| train/                  |             |
|    approx_kl            | 0.019000035 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.6       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.17       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.36        |
|    value_loss           | 0.0185      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 372        |
|    ep_rew_mean          | 5.44e+03   |
| time/                   |            |
|    fps                  | 5565       |
|    iterations           | 600        |
|    time_elapsed         | 3532       |
|    total_timesteps      | 19660800   |
| train/                  |            |
|    approx_kl            | 0.01762158 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -27.7      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.171     |
|    n_updates            | 5990       |
|    policy_gradient_loss | -0.0201    |
|    std                  | 1.37       |
|    value_loss           | 0.0208     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 415         |
|    ep_rew_mean          | 6.14e+03    |
| time/                   |             |
|    fps                  | 5566        |
|    iterations           | 610         |
|    time_elapsed         | 3590        |
|    total_timesteps      | 19988480    |
| train/                  |             |
|    approx_kl            | 0.018592719 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.9       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.172      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0199     |
|    std                  | 1.39        |
|    value_loss           | 0.0207      |
-----------------------------------------
Eval num_timesteps=20000000, episode_reward=8534.55 +/- 2200.84
Episode length: 588.20 +/- 210.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 588         |
|    mean_reward          | 8.53e+03    |
| time/                   |             |
|    total_timesteps      | 20000000    |
| train/                  |             |
|    approx_kl            | 0.018533126 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -27.9       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.175      |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.39        |
|    value_loss           | 0.0136      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 382        |
|    ep_rew_mean          | 5.64e+03   |
| time/                   |            |
|    fps                  | 5563       |
|    iterations           | 620        |
|    time_elapsed         | 3651       |
|    total_timesteps      | 20316160   |
| train/                  |            |
|    approx_kl            | 0.01836621 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -28.1      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.173     |
|    n_updates            | 6190       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 1.4        |
|    value_loss           | 0.0171     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | 5.61e+03    |
| time/                   |             |
|    fps                  | 5563        |
|    iterations           | 630         |
|    time_elapsed         | 3710        |
|    total_timesteps      | 20643840    |
| train/                  |             |
|    approx_kl            | 0.018818503 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.2       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.42        |
|    value_loss           | 0.0188      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 385         |
|    ep_rew_mean          | 5.69e+03    |
| time/                   |             |
|    fps                  | 5563        |
|    iterations           | 640         |
|    time_elapsed         | 3769        |
|    total_timesteps      | 20971520    |
| train/                  |             |
|    approx_kl            | 0.018524498 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.3       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.43        |
|    value_loss           | 0.0192      |
-----------------------------------------
Eval num_timesteps=21000000, episode_reward=7029.52 +/- 1201.90
Episode length: 481.40 +/- 126.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 481         |
|    mean_reward          | 7.03e+03    |
| time/                   |             |
|    total_timesteps      | 21000000    |
| train/                  |             |
|    approx_kl            | 0.018145103 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.3       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.174      |
|    n_updates            | 6400        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 1.43        |
|    value_loss           | 0.0203      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 403         |
|    ep_rew_mean          | 5.78e+03    |
| time/                   |             |
|    fps                  | 5561        |
|    iterations           | 650         |
|    time_elapsed         | 3829        |
|    total_timesteps      | 21299200    |
| train/                  |             |
|    approx_kl            | 0.018668707 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.4       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.175      |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.44        |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 425         |
|    ep_rew_mean          | 6.17e+03    |
| time/                   |             |
|    fps                  | 5562        |
|    iterations           | 660         |
|    time_elapsed         | 3888        |
|    total_timesteps      | 21626880    |
| train/                  |             |
|    approx_kl            | 0.018499363 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.6       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.177      |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0212     |
|    std                  | 1.46        |
|    value_loss           | 0.0186      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 418         |
|    ep_rew_mean          | 6.09e+03    |
| time/                   |             |
|    fps                  | 5563        |
|    iterations           | 670         |
|    time_elapsed         | 3946        |
|    total_timesteps      | 21954560    |
| train/                  |             |
|    approx_kl            | 0.019306026 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.8       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 1.48        |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=22000000, episode_reward=7063.40 +/- 642.75
Episode length: 444.00 +/- 45.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 444         |
|    mean_reward          | 7.06e+03    |
| time/                   |             |
|    total_timesteps      | 22000000    |
| train/                  |             |
|    approx_kl            | 0.020057723 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -28.9       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 6710        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 1.49        |
|    value_loss           | 0.0233      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 445         |
|    ep_rew_mean          | 6.48e+03    |
| time/                   |             |
|    fps                  | 5561        |
|    iterations           | 680         |
|    time_elapsed         | 4006        |
|    total_timesteps      | 22282240    |
| train/                  |             |
|    approx_kl            | 0.018196227 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.1       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.178      |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.51        |
|    value_loss           | 0.0232      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 448         |
|    ep_rew_mean          | 6.4e+03     |
| time/                   |             |
|    fps                  | 5562        |
|    iterations           | 690         |
|    time_elapsed         | 4064        |
|    total_timesteps      | 22609920    |
| train/                  |             |
|    approx_kl            | 0.019021472 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.2       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.53        |
|    value_loss           | 0.0216      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 447         |
|    ep_rew_mean          | 6.6e+03     |
| time/                   |             |
|    fps                  | 5562        |
|    iterations           | 700         |
|    time_elapsed         | 4123        |
|    total_timesteps      | 22937600    |
| train/                  |             |
|    approx_kl            | 0.020391274 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.4       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.18       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0214     |
|    std                  | 1.55        |
|    value_loss           | 0.022       |
-----------------------------------------
Eval num_timesteps=23000000, episode_reward=9330.04 +/- 2572.69
Episode length: 626.80 +/- 193.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 627         |
|    mean_reward          | 9.33e+03    |
| time/                   |             |
|    total_timesteps      | 23000000    |
| train/                  |             |
|    approx_kl            | 0.018777234 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.4       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 7010        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 1.55        |
|    value_loss           | 0.0191      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 459         |
|    ep_rew_mean          | 6.7e+03     |
| time/                   |             |
|    fps                  | 5560        |
|    iterations           | 710         |
|    time_elapsed         | 4184        |
|    total_timesteps      | 23265280    |
| train/                  |             |
|    approx_kl            | 0.020188276 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.6       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0212     |
|    std                  | 1.57        |
|    value_loss           | 0.0236      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 464         |
|    ep_rew_mean          | 6.63e+03    |
| time/                   |             |
|    fps                  | 5560        |
|    iterations           | 720         |
|    time_elapsed         | 4242        |
|    total_timesteps      | 23592960    |
| train/                  |             |
|    approx_kl            | 0.019879451 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -29.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.182      |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.0217     |
|    std                  | 1.58        |
|    value_loss           | 0.0202      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 498         |
|    ep_rew_mean          | 6.95e+03    |
| time/                   |             |
|    fps                  | 5561        |
|    iterations           | 730         |
|    time_elapsed         | 4300        |
|    total_timesteps      | 23920640    |
| train/                  |             |
|    approx_kl            | 0.019125437 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.182      |
|    n_updates            | 7290        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.6         |
|    value_loss           | 0.0203      |
-----------------------------------------
Eval num_timesteps=24000000, episode_reward=6334.63 +/- 3242.89
Episode length: 466.20 +/- 287.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 466        |
|    mean_reward          | 6.33e+03   |
| time/                   |            |
|    total_timesteps      | 24000000   |
| train/                  |            |
|    approx_kl            | 0.01937014 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30        |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.184     |
|    n_updates            | 7320       |
|    policy_gradient_loss | -0.021     |
|    std                  | 1.61       |
|    value_loss           | 0.0208     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 461        |
|    ep_rew_mean          | 6.48e+03   |
| time/                   |            |
|    fps                  | 5560       |
|    iterations           | 740        |
|    time_elapsed         | 4361       |
|    total_timesteps      | 24248320   |
| train/                  |            |
|    approx_kl            | 0.02003709 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.1      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.183     |
|    n_updates            | 7390       |
|    policy_gradient_loss | -0.021     |
|    std                  | 1.62       |
|    value_loss           | 0.0224     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 461         |
|    ep_rew_mean          | 6.44e+03    |
| time/                   |             |
|    fps                  | 5560        |
|    iterations           | 750         |
|    time_elapsed         | 4419        |
|    total_timesteps      | 24576000    |
| train/                  |             |
|    approx_kl            | 0.019286914 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.3       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.184      |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.0212     |
|    std                  | 1.64        |
|    value_loss           | 0.0203      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 472         |
|    ep_rew_mean          | 6.58e+03    |
| time/                   |             |
|    fps                  | 5561        |
|    iterations           | 760         |
|    time_elapsed         | 4478        |
|    total_timesteps      | 24903680    |
| train/                  |             |
|    approx_kl            | 0.020297954 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.184      |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.66        |
|    value_loss           | 0.0239      |
-----------------------------------------
Eval num_timesteps=25000000, episode_reward=8101.01 +/- 2374.55
Episode length: 560.60 +/- 220.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 561         |
|    mean_reward          | 8.1e+03     |
| time/                   |             |
|    total_timesteps      | 25000000    |
| train/                  |             |
|    approx_kl            | 0.019031342 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.5       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.181      |
|    n_updates            | 7620        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.67        |
|    value_loss           | 0.0227      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 463        |
|    ep_rew_mean          | 6.43e+03   |
| time/                   |            |
|    fps                  | 5559       |
|    iterations           | 770        |
|    time_elapsed         | 4538       |
|    total_timesteps      | 25231360   |
| train/                  |            |
|    approx_kl            | 0.02057297 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -30.6      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.188     |
|    n_updates            | 7690       |
|    policy_gradient_loss | -0.0219    |
|    std                  | 1.68       |
|    value_loss           | 0.0229     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 523         |
|    ep_rew_mean          | 7.21e+03    |
| time/                   |             |
|    fps                  | 5559        |
|    iterations           | 780         |
|    time_elapsed         | 4597        |
|    total_timesteps      | 25559040    |
| train/                  |             |
|    approx_kl            | 0.020500505 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -30.8       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.186      |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.7         |
|    value_loss           | 0.0203      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | 7.24e+03    |
| time/                   |             |
|    fps                  | 5560        |
|    iterations           | 790         |
|    time_elapsed         | 4655        |
|    total_timesteps      | 25886720    |
| train/                  |             |
|    approx_kl            | 0.019874288 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.187      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.73        |
|    value_loss           | 0.0214      |
-----------------------------------------
Eval num_timesteps=26000000, episode_reward=7676.33 +/- 1671.63
Episode length: 559.20 +/- 179.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 559         |
|    mean_reward          | 7.68e+03    |
| time/                   |             |
|    total_timesteps      | 26000000    |
| train/                  |             |
|    approx_kl            | 0.020055989 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 7930        |
|    policy_gradient_loss | -0.0216     |
|    std                  | 1.74        |
|    value_loss           | 0.0207      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 461         |
|    ep_rew_mean          | 6.58e+03    |
| time/                   |             |
|    fps                  | 5557        |
|    iterations           | 800         |
|    time_elapsed         | 4716        |
|    total_timesteps      | 26214400    |
| train/                  |             |
|    approx_kl            | 0.017568901 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.3       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.188      |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.76        |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 527         |
|    ep_rew_mean          | 7.43e+03    |
| time/                   |             |
|    fps                  | 5558        |
|    iterations           | 810         |
|    time_elapsed         | 4775        |
|    total_timesteps      | 26542080    |
| train/                  |             |
|    approx_kl            | 0.019741552 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.4       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.191      |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.0215     |
|    std                  | 1.78        |
|    value_loss           | 0.0196      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 498         |
|    ep_rew_mean          | 6.89e+03    |
| time/                   |             |
|    fps                  | 5558        |
|    iterations           | 820         |
|    time_elapsed         | 4834        |
|    total_timesteps      | 26869760    |
| train/                  |             |
|    approx_kl            | 0.020129263 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.7       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.19       |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.81        |
|    value_loss           | 0.0191      |
-----------------------------------------
Eval num_timesteps=27000000, episode_reward=10181.25 +/- 1719.84
Episode length: 707.40 +/- 141.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 707         |
|    mean_reward          | 1.02e+04    |
| time/                   |             |
|    total_timesteps      | 27000000    |
| train/                  |             |
|    approx_kl            | 0.019794509 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.7       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 8230        |
|    policy_gradient_loss | -0.0214     |
|    std                  | 1.82        |
|    value_loss           | 0.0214      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 558         |
|    ep_rew_mean          | 7.9e+03     |
| time/                   |             |
|    fps                  | 5555        |
|    iterations           | 830         |
|    time_elapsed         | 4895        |
|    total_timesteps      | 27197440    |
| train/                  |             |
|    approx_kl            | 0.019637022 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -31.9       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 1.83        |
|    value_loss           | 0.0198      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 514         |
|    ep_rew_mean          | 7.1e+03     |
| time/                   |             |
|    fps                  | 5556        |
|    iterations           | 840         |
|    time_elapsed         | 4954        |
|    total_timesteps      | 27525120    |
| train/                  |             |
|    approx_kl            | 0.018628065 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.1       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.192      |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.87        |
|    value_loss           | 0.0194      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 523         |
|    ep_rew_mean          | 7.36e+03    |
| time/                   |             |
|    fps                  | 5556        |
|    iterations           | 850         |
|    time_elapsed         | 5012        |
|    total_timesteps      | 27852800    |
| train/                  |             |
|    approx_kl            | 0.020335898 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.3       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.191      |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.0206     |
|    std                  | 1.9         |
|    value_loss           | 0.025       |
-----------------------------------------
Eval num_timesteps=28000000, episode_reward=7110.93 +/- 700.77
Episode length: 476.20 +/- 71.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 7.11e+03    |
| time/                   |             |
|    total_timesteps      | 28000000    |
| train/                  |             |
|    approx_kl            | 0.018462416 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.5       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.195      |
|    n_updates            | 8540        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.92        |
|    value_loss           | 0.0235      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 547        |
|    ep_rew_mean          | 7.77e+03   |
| time/                   |            |
|    fps                  | 5555       |
|    iterations           | 860        |
|    time_elapsed         | 5072       |
|    total_timesteps      | 28180480   |
| train/                  |            |
|    approx_kl            | 0.01815655 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -32.6      |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.196     |
|    n_updates            | 8590       |
|    policy_gradient_loss | -0.0204    |
|    std                  | 1.93       |
|    value_loss           | 0.0188     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 502         |
|    ep_rew_mean          | 7.02e+03    |
| time/                   |             |
|    fps                  | 5556        |
|    iterations           | 870         |
|    time_elapsed         | 5130        |
|    total_timesteps      | 28508160    |
| train/                  |             |
|    approx_kl            | 0.017283598 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -32.8       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.196      |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.96        |
|    value_loss           | 0.0218      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 528         |
|    ep_rew_mean          | 7.36e+03    |
| time/                   |             |
|    fps                  | 5556        |
|    iterations           | 880         |
|    time_elapsed         | 5189        |
|    total_timesteps      | 28835840    |
| train/                  |             |
|    approx_kl            | 0.020011993 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33         |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.198      |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.0212     |
|    std                  | 1.99        |
|    value_loss           | 0.0206      |
-----------------------------------------
Eval num_timesteps=29000000, episode_reward=9206.27 +/- 1718.04
Episode length: 664.40 +/- 132.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 664         |
|    mean_reward          | 9.21e+03    |
| time/                   |             |
|    total_timesteps      | 29000000    |
| train/                  |             |
|    approx_kl            | 0.019032247 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.1       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.197      |
|    n_updates            | 8850        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2           |
|    value_loss           | 0.0241      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 520        |
|    ep_rew_mean          | 7.5e+03    |
| time/                   |            |
|    fps                  | 5554       |
|    iterations           | 890        |
|    time_elapsed         | 5250       |
|    total_timesteps      | 29163520   |
| train/                  |            |
|    approx_kl            | 0.02134893 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -33.2      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.198     |
|    n_updates            | 8890       |
|    policy_gradient_loss | -0.0213    |
|    std                  | 2.01       |
|    value_loss           | 0.0199     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 569         |
|    ep_rew_mean          | 8.17e+03    |
| time/                   |             |
|    fps                  | 5555        |
|    iterations           | 900         |
|    time_elapsed         | 5308        |
|    total_timesteps      | 29491200    |
| train/                  |             |
|    approx_kl            | 0.021151379 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.4       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.2        |
|    n_updates            | 8990        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 2.05        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 520         |
|    ep_rew_mean          | 7.61e+03    |
| time/                   |             |
|    fps                  | 5555        |
|    iterations           | 910         |
|    time_elapsed         | 5367        |
|    total_timesteps      | 29818880    |
| train/                  |             |
|    approx_kl            | 0.019873708 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.202      |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 2.08        |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=30000000, episode_reward=6312.14 +/- 2633.13
Episode length: 464.40 +/- 225.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 6.31e+03    |
| time/                   |             |
|    total_timesteps      | 30000000    |
| train/                  |             |
|    approx_kl            | 0.019523572 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 9150        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 2.09        |
|    value_loss           | 0.0223      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | 7.26e+03    |
| time/                   |             |
|    fps                  | 5554        |
|    iterations           | 920         |
|    time_elapsed         | 5427        |
|    total_timesteps      | 30146560    |
| train/                  |             |
|    approx_kl            | 0.020233907 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -33.9       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.201      |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 2.11        |
|    value_loss           | 0.0201      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 548        |
|    ep_rew_mean          | 7.96e+03   |
| time/                   |            |
|    fps                  | 5555       |
|    iterations           | 930        |
|    time_elapsed         | 5485       |
|    total_timesteps      | 30474240   |
| train/                  |            |
|    approx_kl            | 0.01944618 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -34        |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.201     |
|    n_updates            | 9290       |
|    policy_gradient_loss | -0.0201    |
|    std                  | 2.13       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 544         |
|    ep_rew_mean          | 8e+03       |
| time/                   |             |
|    fps                  | 5555        |
|    iterations           | 940         |
|    time_elapsed         | 5544        |
|    total_timesteps      | 30801920    |
| train/                  |             |
|    approx_kl            | 0.020338262 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.2       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.204      |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.0209     |
|    std                  | 2.16        |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=31000000, episode_reward=9589.21 +/- 1563.62
Episode length: 685.40 +/- 136.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 685         |
|    mean_reward          | 9.59e+03    |
| time/                   |             |
|    total_timesteps      | 31000000    |
| train/                  |             |
|    approx_kl            | 0.019296978 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.4       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.203      |
|    n_updates            | 9460        |
|    policy_gradient_loss | -0.0204     |
|    std                  | 2.19        |
|    value_loss           | 0.0208      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 499       |
|    ep_rew_mean          | 7.21e+03  |
| time/                   |           |
|    fps                  | 5553      |
|    iterations           | 950       |
|    time_elapsed         | 5605      |
|    total_timesteps      | 31129600  |
| train/                  |           |
|    approx_kl            | 0.0197419 |
|    clip_fraction        | 0.159     |
|    clip_range           | 0.2       |
|    entropy_loss         | -34.4     |
|    explained_variance   | 0.977     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.203    |
|    n_updates            | 9490      |
|    policy_gradient_loss | -0.0204   |
|    std                  | 2.19      |
|    value_loss           | 0.0164    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 515        |
|    ep_rew_mean          | 7.51e+03   |
| time/                   |            |
|    fps                  | 5553       |
|    iterations           | 960        |
|    time_elapsed         | 5664       |
|    total_timesteps      | 31457280   |
| train/                  |            |
|    approx_kl            | 0.01876432 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -34.7      |
|    explained_variance   | 0.969      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.205     |
|    n_updates            | 9590       |
|    policy_gradient_loss | -0.0203    |
|    std                  | 2.23       |
|    value_loss           | 0.0197     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | 7.54e+03    |
| time/                   |             |
|    fps                  | 5554        |
|    iterations           | 970         |
|    time_elapsed         | 5722        |
|    total_timesteps      | 31784960    |
| train/                  |             |
|    approx_kl            | 0.018823277 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -34.9       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.208      |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.0202     |
|    std                  | 2.27        |
|    value_loss           | 0.0157      |
-----------------------------------------
Eval num_timesteps=32000000, episode_reward=9715.82 +/- 2587.74
Episode length: 640.60 +/- 192.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 641         |
|    mean_reward          | 9.72e+03    |
| time/                   |             |
|    total_timesteps      | 32000000    |
| train/                  |             |
|    approx_kl            | 0.019211527 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.1       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.205      |
|    n_updates            | 9760        |
|    policy_gradient_loss | -0.02       |
|    std                  | 2.3         |
|    value_loss           | 0.0203      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 508        |
|    ep_rew_mean          | 7.56e+03   |
| time/                   |            |
|    fps                  | 5552       |
|    iterations           | 980        |
|    time_elapsed         | 5783       |
|    total_timesteps      | 32112640   |
| train/                  |            |
|    approx_kl            | 0.01907757 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.1      |
|    explained_variance   | 0.978      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.208     |
|    n_updates            | 9790       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 2.31       |
|    value_loss           | 0.0133     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 521         |
|    ep_rew_mean          | 7.8e+03     |
| time/                   |             |
|    fps                  | 5553        |
|    iterations           | 990         |
|    time_elapsed         | 5841        |
|    total_timesteps      | 32440320    |
| train/                  |             |
|    approx_kl            | 0.020084277 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.4       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.208      |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.02       |
|    std                  | 2.34        |
|    value_loss           | 0.0181      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 520        |
|    ep_rew_mean          | 7.75e+03   |
| time/                   |            |
|    fps                  | 5553       |
|    iterations           | 1000       |
|    time_elapsed         | 5900       |
|    total_timesteps      | 32768000   |
| train/                  |            |
|    approx_kl            | 0.02046403 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -35.6      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.209     |
|    n_updates            | 9990       |
|    policy_gradient_loss | -0.0202    |
|    std                  | 2.39       |
|    value_loss           | 0.0225     |
----------------------------------------
Eval num_timesteps=33000000, episode_reward=9255.38 +/- 3764.73
Episode length: 625.80 +/- 229.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 626         |
|    mean_reward          | 9.26e+03    |
| time/                   |             |
|    total_timesteps      | 33000000    |
| train/                  |             |
|    approx_kl            | 0.020193664 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.209      |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 2.41        |
|    value_loss           | 0.0229      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 521         |
|    ep_rew_mean          | 7.87e+03    |
| time/                   |             |
|    fps                  | 5551        |
|    iterations           | 1010        |
|    time_elapsed         | 5961        |
|    total_timesteps      | 33095680    |
| train/                  |             |
|    approx_kl            | 0.019217106 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -35.8       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.21       |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 2.42        |
|    value_loss           | 0.0166      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 468        |
|    ep_rew_mean          | 7e+03      |
| time/                   |            |
|    fps                  | 5552       |
|    iterations           | 1020       |
|    time_elapsed         | 6019       |
|    total_timesteps      | 33423360   |
| train/                  |            |
|    approx_kl            | 0.02008798 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.21      |
|    n_updates            | 10190      |
|    policy_gradient_loss | -0.0202    |
|    std                  | 2.45       |
|    value_loss           | 0.0237     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 461         |
|    ep_rew_mean          | 6.96e+03    |
| time/                   |             |
|    fps                  | 5552        |
|    iterations           | 1030        |
|    time_elapsed         | 6078        |
|    total_timesteps      | 33751040    |
| train/                  |             |
|    approx_kl            | 0.019800123 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.2       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.212      |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.02       |
|    std                  | 2.49        |
|    value_loss           | 0.0211      |
-----------------------------------------
Eval num_timesteps=34000000, episode_reward=9210.16 +/- 2391.34
Episode length: 623.20 +/- 161.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 623         |
|    mean_reward          | 9.21e+03    |
| time/                   |             |
|    total_timesteps      | 34000000    |
| train/                  |             |
|    approx_kl            | 0.019914698 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.4       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.214      |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.51        |
|    value_loss           | 0.0162      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 529         |
|    ep_rew_mean          | 7.97e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1040        |
|    time_elapsed         | 6139        |
|    total_timesteps      | 34078720    |
| train/                  |             |
|    approx_kl            | 0.019786105 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.5       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.215      |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.0205     |
|    std                  | 2.53        |
|    value_loss           | 0.0176      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 535         |
|    ep_rew_mean          | 8.05e+03    |
| time/                   |             |
|    fps                  | 5551        |
|    iterations           | 1050        |
|    time_elapsed         | 6197        |
|    total_timesteps      | 34406400    |
| train/                  |             |
|    approx_kl            | 0.018889174 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -36.7       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.214      |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 2.55        |
|    value_loss           | 0.0172      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 550        |
|    ep_rew_mean          | 8.3e+03    |
| time/                   |            |
|    fps                  | 5552       |
|    iterations           | 1060       |
|    time_elapsed         | 6256       |
|    total_timesteps      | 34734080   |
| train/                  |            |
|    approx_kl            | 0.01796332 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -36.8      |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.214     |
|    n_updates            | 10590      |
|    policy_gradient_loss | -0.0194    |
|    std                  | 2.59       |
|    value_loss           | 0.0194     |
----------------------------------------
Eval num_timesteps=35000000, episode_reward=10070.34 +/- 1564.95
Episode length: 632.00 +/- 90.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 632         |
|    mean_reward          | 1.01e+04    |
| time/                   |             |
|    total_timesteps      | 35000000    |
| train/                  |             |
|    approx_kl            | 0.018601097 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37         |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.214      |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.61        |
|    value_loss           | 0.0203      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 527         |
|    ep_rew_mean          | 7.98e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1070        |
|    time_elapsed         | 6317        |
|    total_timesteps      | 35061760    |
| train/                  |             |
|    approx_kl            | 0.019511739 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37         |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.217      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 2.62        |
|    value_loss           | 0.0213      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 514        |
|    ep_rew_mean          | 7.71e+03   |
| time/                   |            |
|    fps                  | 5550       |
|    iterations           | 1080       |
|    time_elapsed         | 6375       |
|    total_timesteps      | 35389440   |
| train/                  |            |
|    approx_kl            | 0.01936699 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -37.2      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.216     |
|    n_updates            | 10790      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 2.65       |
|    value_loss           | 0.02       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 482         |
|    ep_rew_mean          | 7.22e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1090        |
|    time_elapsed         | 6434        |
|    total_timesteps      | 35717120    |
| train/                  |             |
|    approx_kl            | 0.020579383 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.4       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 2.69        |
|    value_loss           | 0.0206      |
-----------------------------------------
Eval num_timesteps=36000000, episode_reward=7264.52 +/- 1963.89
Episode length: 470.00 +/- 174.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 470         |
|    mean_reward          | 7.26e+03    |
| time/                   |             |
|    total_timesteps      | 36000000    |
| train/                  |             |
|    approx_kl            | 0.018556217 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.6       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 2.72        |
|    value_loss           | 0.0202      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 489         |
|    ep_rew_mean          | 7.34e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1100        |
|    time_elapsed         | 6494        |
|    total_timesteps      | 36044800    |
| train/                  |             |
|    approx_kl            | 0.019321434 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.6       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.218      |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 2.73        |
|    value_loss           | 0.0222      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 526         |
|    ep_rew_mean          | 7.96e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1110        |
|    time_elapsed         | 6552        |
|    total_timesteps      | 36372480    |
| train/                  |             |
|    approx_kl            | 0.019910108 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -37.8       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.222      |
|    n_updates            | 11090       |
|    policy_gradient_loss | -0.0207     |
|    std                  | 2.77        |
|    value_loss           | 0.0209      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 508         |
|    ep_rew_mean          | 7.74e+03    |
| time/                   |             |
|    fps                  | 5551        |
|    iterations           | 1120        |
|    time_elapsed         | 6611        |
|    total_timesteps      | 36700160    |
| train/                  |             |
|    approx_kl            | 0.017970726 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.22       |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 2.81        |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=37000000, episode_reward=8156.45 +/- 4203.14
Episode length: 533.80 +/- 250.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 534         |
|    mean_reward          | 8.16e+03    |
| time/                   |             |
|    total_timesteps      | 37000000    |
| train/                  |             |
|    approx_kl            | 0.018251853 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.3       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.221      |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 2.85        |
|    value_loss           | 0.0191      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 528      |
|    ep_rew_mean     | 8.14e+03 |
| time/              |          |
|    fps             | 5550     |
|    iterations      | 1130     |
|    time_elapsed    | 6671     |
|    total_timesteps | 37027840 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 524         |
|    ep_rew_mean          | 7.99e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1140        |
|    time_elapsed         | 6729        |
|    total_timesteps      | 37355520    |
| train/                  |             |
|    approx_kl            | 0.018598817 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.5       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.223      |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 2.9         |
|    value_loss           | 0.0207      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 560         |
|    ep_rew_mean          | 8.58e+03    |
| time/                   |             |
|    fps                  | 5551        |
|    iterations           | 1150        |
|    time_elapsed         | 6788        |
|    total_timesteps      | 37683200    |
| train/                  |             |
|    approx_kl            | 0.018431734 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -38.7       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.224      |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 2.94        |
|    value_loss           | 0.0205      |
-----------------------------------------
Eval num_timesteps=38000000, episode_reward=10077.87 +/- 1902.81
Episode length: 640.60 +/- 106.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 641        |
|    mean_reward          | 1.01e+04   |
| time/                   |            |
|    total_timesteps      | 38000000   |
| train/                  |            |
|    approx_kl            | 0.01843813 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -39        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.223     |
|    n_updates            | 11590      |
|    policy_gradient_loss | -0.0195    |
|    std                  | 3.01       |
|    value_loss           | 0.0227     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 7.02e+03 |
| time/              |          |
|    fps             | 5549     |
|    iterations      | 1160     |
|    time_elapsed    | 6849     |
|    total_timesteps | 38010880 |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 535        |
|    ep_rew_mean          | 8.22e+03   |
| time/                   |            |
|    fps                  | 5549       |
|    iterations           | 1170       |
|    time_elapsed         | 6908       |
|    total_timesteps      | 38338560   |
| train/                  |            |
|    approx_kl            | 0.01805542 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -39.2      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.227     |
|    n_updates            | 11690      |
|    policy_gradient_loss | -0.0203    |
|    std                  | 3.05       |
|    value_loss           | 0.0185     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 533         |
|    ep_rew_mean          | 8.04e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1180        |
|    time_elapsed         | 6966        |
|    total_timesteps      | 38666240    |
| train/                  |             |
|    approx_kl            | 0.019288324 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.4       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.228      |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 3.1         |
|    value_loss           | 0.0201      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 518         |
|    ep_rew_mean          | 7.91e+03    |
| time/                   |             |
|    fps                  | 5550        |
|    iterations           | 1190        |
|    time_elapsed         | 7025        |
|    total_timesteps      | 38993920    |
| train/                  |             |
|    approx_kl            | 0.018340258 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.6       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.228      |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 3.13        |
|    value_loss           | 0.0207      |
-----------------------------------------
Eval num_timesteps=39000000, episode_reward=11006.77 +/- 103.14
Episode length: 698.40 +/- 38.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 698         |
|    mean_reward          | 1.1e+04     |
| time/                   |             |
|    total_timesteps      | 39000000    |
| train/                  |             |
|    approx_kl            | 0.019632647 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -39.6       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.229      |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.14        |
|    value_loss           | 0.0209      |
-----------------------------------------
New best mean reward!
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 498       |
|    ep_rew_mean          | 7.55e+03  |
| time/                   |           |
|    fps                  | 5549      |
|    iterations           | 1200      |
|    time_elapsed         | 7086      |
|    total_timesteps      | 39321600  |
| train/                  |           |
|    approx_kl            | 0.0192401 |
|    clip_fraction        | 0.155     |
|    clip_range           | 0.2       |
|    entropy_loss         | -39.8     |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0003    |
|    loss                 | -0.227    |
|    n_updates            | 11990     |
|    policy_gradient_loss | -0.0198   |
|    std                  | 3.18      |
|    value_loss           | 0.0228    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 481         |
|    ep_rew_mean          | 7.28e+03    |
| time/                   |             |
|    fps                  | 5549        |
|    iterations           | 1210        |
|    time_elapsed         | 7144        |
|    total_timesteps      | 39649280    |
| train/                  |             |
|    approx_kl            | 0.020269949 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40         |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.232      |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.0209     |
|    std                  | 3.24        |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 498         |
|    ep_rew_mean          | 7.58e+03    |
| time/                   |             |
|    fps                  | 5549        |
|    iterations           | 1220        |
|    time_elapsed         | 7203        |
|    total_timesteps      | 39976960    |
| train/                  |             |
|    approx_kl            | 0.019025065 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.2       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.231      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 3.28        |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=40000000, episode_reward=10139.80 +/- 1764.84
Episode length: 624.40 +/- 114.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 624        |
|    mean_reward          | 1.01e+04   |
| time/                   |            |
|    total_timesteps      | 40000000   |
| train/                  |            |
|    approx_kl            | 0.02171565 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.2      |
|    explained_variance   | 0.971      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.233     |
|    n_updates            | 12200      |
|    policy_gradient_loss | -0.0199    |
|    std                  | 3.28       |
|    value_loss           | 0.018      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 553         |
|    ep_rew_mean          | 8.47e+03    |
| time/                   |             |
|    fps                  | 5548        |
|    iterations           | 1230        |
|    time_elapsed         | 7264        |
|    total_timesteps      | 40304640    |
| train/                  |             |
|    approx_kl            | 0.020193879 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.4       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.236      |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 3.33        |
|    value_loss           | 0.017       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 536        |
|    ep_rew_mean          | 8.29e+03   |
| time/                   |            |
|    fps                  | 5548       |
|    iterations           | 1240       |
|    time_elapsed         | 7322       |
|    total_timesteps      | 40632320   |
| train/                  |            |
|    approx_kl            | 0.01858515 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.7      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.233     |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.0196    |
|    std                  | 3.39       |
|    value_loss           | 0.0191     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 7.59e+03    |
| time/                   |             |
|    fps                  | 5549        |
|    iterations           | 1250        |
|    time_elapsed         | 7381        |
|    total_timesteps      | 40960000    |
| train/                  |             |
|    approx_kl            | 0.018270437 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -40.9       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.02       |
|    std                  | 3.44        |
|    value_loss           | 0.0184      |
-----------------------------------------
Eval num_timesteps=41000000, episode_reward=10212.99 +/- 1396.83
Episode length: 630.80 +/- 107.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 631        |
|    mean_reward          | 1.02e+04   |
| time/                   |            |
|    total_timesteps      | 41000000   |
| train/                  |            |
|    approx_kl            | 0.01887979 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -40.9      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.235     |
|    n_updates            | 12510      |
|    policy_gradient_loss | -0.0201    |
|    std                  | 3.45       |
|    value_loss           | 0.0188     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | 7.78e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1260        |
|    time_elapsed         | 7442        |
|    total_timesteps      | 41287680    |
| train/                  |             |
|    approx_kl            | 0.018379595 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.1       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.233      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 3.5         |
|    value_loss           | 0.022       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 511         |
|    ep_rew_mean          | 7.8e+03     |
| time/                   |             |
|    fps                  | 5548        |
|    iterations           | 1270        |
|    time_elapsed         | 7500        |
|    total_timesteps      | 41615360    |
| train/                  |             |
|    approx_kl            | 0.018666968 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.3       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.234      |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 3.57        |
|    value_loss           | 0.0246      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 514         |
|    ep_rew_mean          | 7.93e+03    |
| time/                   |             |
|    fps                  | 5549        |
|    iterations           | 1280        |
|    time_elapsed         | 7558        |
|    total_timesteps      | 41943040    |
| train/                  |             |
|    approx_kl            | 0.019812781 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.6       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.237      |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 3.64        |
|    value_loss           | 0.0234      |
-----------------------------------------
Eval num_timesteps=42000000, episode_reward=8155.76 +/- 4087.28
Episode length: 531.00 +/- 248.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 531         |
|    mean_reward          | 8.16e+03    |
| time/                   |             |
|    total_timesteps      | 42000000    |
| train/                  |             |
|    approx_kl            | 0.019031703 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.6       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.238      |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 3.65        |
|    value_loss           | 0.0212      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 522         |
|    ep_rew_mean          | 7.91e+03    |
| time/                   |             |
|    fps                  | 5548        |
|    iterations           | 1290        |
|    time_elapsed         | 7619        |
|    total_timesteps      | 42270720    |
| train/                  |             |
|    approx_kl            | 0.018839326 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -41.8       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.0202     |
|    std                  | 3.7         |
|    value_loss           | 0.0188      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 521         |
|    ep_rew_mean          | 8.01e+03    |
| time/                   |             |
|    fps                  | 5548        |
|    iterations           | 1300        |
|    time_elapsed         | 7677        |
|    total_timesteps      | 42598400    |
| train/                  |             |
|    approx_kl            | 0.018927407 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42         |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.241      |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 3.77        |
|    value_loss           | 0.0206      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 486        |
|    ep_rew_mean          | 7.45e+03   |
| time/                   |            |
|    fps                  | 5548       |
|    iterations           | 1310       |
|    time_elapsed         | 7736       |
|    total_timesteps      | 42926080   |
| train/                  |            |
|    approx_kl            | 0.01790436 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.3      |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.242     |
|    n_updates            | 13090      |
|    policy_gradient_loss | -0.019     |
|    std                  | 3.84       |
|    value_loss           | 0.0189     |
----------------------------------------
Eval num_timesteps=43000000, episode_reward=11004.33 +/- 149.06
Episode length: 686.40 +/- 18.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 686        |
|    mean_reward          | 1.1e+04    |
| time/                   |            |
|    total_timesteps      | 43000000   |
| train/                  |            |
|    approx_kl            | 0.01882206 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.4      |
|    explained_variance   | 0.964      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.24      |
|    n_updates            | 13120      |
|    policy_gradient_loss | -0.0194    |
|    std                  | 3.86       |
|    value_loss           | 0.0223     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 518         |
|    ep_rew_mean          | 7.77e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1320        |
|    time_elapsed         | 7797        |
|    total_timesteps      | 43253760    |
| train/                  |             |
|    approx_kl            | 0.018922128 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -42.5       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.243      |
|    n_updates            | 13190       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 3.9         |
|    value_loss           | 0.0199      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 525        |
|    ep_rew_mean          | 8.13e+03   |
| time/                   |            |
|    fps                  | 5547       |
|    iterations           | 1330       |
|    time_elapsed         | 7855       |
|    total_timesteps      | 43581440   |
| train/                  |            |
|    approx_kl            | 0.01862101 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -42.7      |
|    explained_variance   | 0.972      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.246     |
|    n_updates            | 13290      |
|    policy_gradient_loss | -0.0197    |
|    std                  | 3.95       |
|    value_loss           | 0.0161     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | 7.37e+03    |
| time/                   |             |
|    fps                  | 5548        |
|    iterations           | 1340        |
|    time_elapsed         | 7914        |
|    total_timesteps      | 43909120    |
| train/                  |             |
|    approx_kl            | 0.017989568 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.244      |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 4.03        |
|    value_loss           | 0.0207      |
-----------------------------------------
Eval num_timesteps=44000000, episode_reward=7855.46 +/- 3343.18
Episode length: 485.40 +/- 195.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 485         |
|    mean_reward          | 7.86e+03    |
| time/                   |             |
|    total_timesteps      | 44000000    |
| train/                  |             |
|    approx_kl            | 0.018849283 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43         |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 4.04        |
|    value_loss           | 0.0221      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 7.57e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1350        |
|    time_elapsed         | 7974        |
|    total_timesteps      | 44236800    |
| train/                  |             |
|    approx_kl            | 0.018397134 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.2       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.245      |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.019      |
|    std                  | 4.09        |
|    value_loss           | 0.0206      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 534         |
|    ep_rew_mean          | 8.28e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1360        |
|    time_elapsed         | 8033        |
|    total_timesteps      | 44564480    |
| train/                  |             |
|    approx_kl            | 0.018287221 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.4       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.246      |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 4.15        |
|    value_loss           | 0.018       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | 8e+03       |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1370        |
|    time_elapsed         | 8092        |
|    total_timesteps      | 44892160    |
| train/                  |             |
|    approx_kl            | 0.020488556 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.6       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.25       |
|    n_updates            | 13690       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 4.23        |
|    value_loss           | 0.0199      |
-----------------------------------------
Eval num_timesteps=45000000, episode_reward=11041.46 +/- 140.36
Episode length: 677.80 +/- 38.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 678         |
|    mean_reward          | 1.1e+04     |
| time/                   |             |
|    total_timesteps      | 45000000    |
| train/                  |             |
|    approx_kl            | 0.018886581 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.7       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.247      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 4.25        |
|    value_loss           | 0.0195      |
-----------------------------------------
New best mean reward!
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 475         |
|    ep_rew_mean          | 7.31e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1380        |
|    time_elapsed         | 8153        |
|    total_timesteps      | 45219840    |
| train/                  |             |
|    approx_kl            | 0.019199513 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -43.8       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.249      |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.3         |
|    value_loss           | 0.0213      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 528         |
|    ep_rew_mean          | 8.32e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1390        |
|    time_elapsed         | 8211        |
|    total_timesteps      | 45547520    |
| train/                  |             |
|    approx_kl            | 0.019213364 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44         |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.25       |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 4.38        |
|    value_loss           | 0.0207      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 532         |
|    ep_rew_mean          | 8.41e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1400        |
|    time_elapsed         | 8270        |
|    total_timesteps      | 45875200    |
| train/                  |             |
|    approx_kl            | 0.018051064 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.2       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.249      |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 4.44        |
|    value_loss           | 0.0209      |
-----------------------------------------
Eval num_timesteps=46000000, episode_reward=8763.98 +/- 4230.93
Episode length: 531.60 +/- 234.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 532         |
|    mean_reward          | 8.76e+03    |
| time/                   |             |
|    total_timesteps      | 46000000    |
| train/                  |             |
|    approx_kl            | 0.018715817 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.3       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.252      |
|    n_updates            | 14030       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 4.46        |
|    value_loss           | 0.018       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 556         |
|    ep_rew_mean          | 8.77e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1410        |
|    time_elapsed         | 8330        |
|    total_timesteps      | 46202880    |
| train/                  |             |
|    approx_kl            | 0.017860658 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.4       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.254      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 4.5         |
|    value_loss           | 0.0174      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 521         |
|    ep_rew_mean          | 8.18e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1420        |
|    time_elapsed         | 8388        |
|    total_timesteps      | 46530560    |
| train/                  |             |
|    approx_kl            | 0.019352788 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.6       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.253      |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.02       |
|    std                  | 4.57        |
|    value_loss           | 0.0217      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 517         |
|    ep_rew_mean          | 8.07e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1430        |
|    time_elapsed         | 8447        |
|    total_timesteps      | 46858240    |
| train/                  |             |
|    approx_kl            | 0.017759252 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.8       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 14290       |
|    policy_gradient_loss | -0.02       |
|    std                  | 4.63        |
|    value_loss           | 0.0197      |
-----------------------------------------
Eval num_timesteps=47000000, episode_reward=5928.38 +/- 4320.07
Episode length: 411.40 +/- 267.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 411         |
|    mean_reward          | 5.93e+03    |
| time/                   |             |
|    total_timesteps      | 47000000    |
| train/                  |             |
|    approx_kl            | 0.018265573 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -44.9       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.251      |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 4.65        |
|    value_loss           | 0.0232      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 517         |
|    ep_rew_mean          | 7.97e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1440        |
|    time_elapsed         | 8507        |
|    total_timesteps      | 47185920    |
| train/                  |             |
|    approx_kl            | 0.019798845 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45         |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.255      |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 4.7         |
|    value_loss           | 0.0236      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 532         |
|    ep_rew_mean          | 8.39e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1450        |
|    time_elapsed         | 8565        |
|    total_timesteps      | 47513600    |
| train/                  |             |
|    approx_kl            | 0.018150553 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.2       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.257      |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 4.77        |
|    value_loss           | 0.0189      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 497         |
|    ep_rew_mean          | 7.76e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1460        |
|    time_elapsed         | 8624        |
|    total_timesteps      | 47841280    |
| train/                  |             |
|    approx_kl            | 0.018277679 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.5       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.258      |
|    n_updates            | 14590       |
|    policy_gradient_loss | -0.0206     |
|    std                  | 4.84        |
|    value_loss           | 0.0255      |
-----------------------------------------
Eval num_timesteps=48000000, episode_reward=11004.82 +/- 98.12
Episode length: 682.20 +/- 26.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 682         |
|    mean_reward          | 1.1e+04     |
| time/                   |             |
|    total_timesteps      | 48000000    |
| train/                  |             |
|    approx_kl            | 0.018734809 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -45.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 14640       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 4.88        |
|    value_loss           | 0.0208      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 555        |
|    ep_rew_mean          | 8.75e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1470       |
|    time_elapsed         | 8685       |
|    total_timesteps      | 48168960   |
| train/                  |            |
|    approx_kl            | 0.01923821 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.6      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.259     |
|    n_updates            | 14690      |
|    policy_gradient_loss | -0.0204    |
|    std                  | 4.93       |
|    value_loss           | 0.0207     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 533        |
|    ep_rew_mean          | 8.37e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1480       |
|    time_elapsed         | 8743       |
|    total_timesteps      | 48496640   |
| train/                  |            |
|    approx_kl            | 0.01839709 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -45.8      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.26      |
|    n_updates            | 14790      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 5.01       |
|    value_loss           | 0.0182     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | 8.25e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1490        |
|    time_elapsed         | 8802        |
|    total_timesteps      | 48824320    |
| train/                  |             |
|    approx_kl            | 0.019647509 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46         |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 5.08        |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=49000000, episode_reward=10926.82 +/- 206.66
Episode length: 686.00 +/- 6.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 686         |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 49000000    |
| train/                  |             |
|    approx_kl            | 0.018633194 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.1       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.257      |
|    n_updates            | 14950       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 5.12        |
|    value_loss           | 0.0224      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 522         |
|    ep_rew_mean          | 8.23e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1500        |
|    time_elapsed         | 8862        |
|    total_timesteps      | 49152000    |
| train/                  |             |
|    approx_kl            | 0.017046094 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.2       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 14990       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 5.14        |
|    value_loss           | 0.0199      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 506         |
|    ep_rew_mean          | 7.91e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1510        |
|    time_elapsed         | 8920        |
|    total_timesteps      | 49479680    |
| train/                  |             |
|    approx_kl            | 0.018959632 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.4       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.259      |
|    n_updates            | 15090       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 5.2         |
|    value_loss           | 0.0233      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 517         |
|    ep_rew_mean          | 8.22e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1520        |
|    time_elapsed         | 8979        |
|    total_timesteps      | 49807360    |
| train/                  |             |
|    approx_kl            | 0.018334063 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.7       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.26       |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.019      |
|    std                  | 5.3         |
|    value_loss           | 0.0223      |
-----------------------------------------
Eval num_timesteps=50000000, episode_reward=9075.15 +/- 2147.93
Episode length: 579.20 +/- 132.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 579         |
|    mean_reward          | 9.08e+03    |
| time/                   |             |
|    total_timesteps      | 50000000    |
| train/                  |             |
|    approx_kl            | 0.018982423 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.8       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.262      |
|    n_updates            | 15250       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.35        |
|    value_loss           | 0.0278      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 530         |
|    ep_rew_mean          | 8.4e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1530        |
|    time_elapsed         | 9039        |
|    total_timesteps      | 50135040    |
| train/                  |             |
|    approx_kl            | 0.017760228 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -46.9       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.263      |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 5.38        |
|    value_loss           | 0.0169      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | 7.99e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1540        |
|    time_elapsed         | 9098        |
|    total_timesteps      | 50462720    |
| train/                  |             |
|    approx_kl            | 0.018602207 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.265      |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 5.49        |
|    value_loss           | 0.0202      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 518         |
|    ep_rew_mean          | 8.04e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1550        |
|    time_elapsed         | 9156        |
|    total_timesteps      | 50790400    |
| train/                  |             |
|    approx_kl            | 0.018236509 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.4       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.267      |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0204     |
|    std                  | 5.6         |
|    value_loss           | 0.0243      |
-----------------------------------------
Eval num_timesteps=51000000, episode_reward=10557.40 +/- 846.11
Episode length: 625.80 +/- 52.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 626         |
|    mean_reward          | 1.06e+04    |
| time/                   |             |
|    total_timesteps      | 51000000    |
| train/                  |             |
|    approx_kl            | 0.018768193 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.5       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.265      |
|    n_updates            | 15560       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 5.65        |
|    value_loss           | 0.022       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 492        |
|    ep_rew_mean          | 7.77e+03   |
| time/                   |            |
|    fps                  | 5545       |
|    iterations           | 1560       |
|    time_elapsed         | 9217       |
|    total_timesteps      | 51118080   |
| train/                  |            |
|    approx_kl            | 0.01836807 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -47.5      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.265     |
|    n_updates            | 15590      |
|    policy_gradient_loss | -0.0183    |
|    std                  | 5.66       |
|    value_loss           | 0.0178     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 539         |
|    ep_rew_mean          | 8.58e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1570        |
|    time_elapsed         | 9276        |
|    total_timesteps      | 51445760    |
| train/                  |             |
|    approx_kl            | 0.019093197 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -47.8       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.268      |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 5.77        |
|    value_loss           | 0.0178      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 7.7e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1580        |
|    time_elapsed         | 9334        |
|    total_timesteps      | 51773440    |
| train/                  |             |
|    approx_kl            | 0.023000002 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48         |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.268      |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 5.89        |
|    value_loss           | 0.024       |
-----------------------------------------
Eval num_timesteps=52000000, episode_reward=8928.35 +/- 2355.77
Episode length: 531.00 +/- 143.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 531         |
|    mean_reward          | 8.93e+03    |
| time/                   |             |
|    total_timesteps      | 52000000    |
| train/                  |             |
|    approx_kl            | 0.018654147 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.2       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 15860       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 5.97        |
|    value_loss           | 0.0183      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 552         |
|    ep_rew_mean          | 8.75e+03    |
| time/                   |             |
|    fps                  | 5545        |
|    iterations           | 1590        |
|    time_elapsed         | 9394        |
|    total_timesteps      | 52101120    |
| train/                  |             |
|    approx_kl            | 0.018871382 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.3       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.0198     |
|    std                  | 6           |
|    value_loss           | 0.0203      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 522         |
|    ep_rew_mean          | 8.23e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1600        |
|    time_elapsed         | 9453        |
|    total_timesteps      | 52428800    |
| train/                  |             |
|    approx_kl            | 0.017693419 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.4       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.271      |
|    n_updates            | 15990       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.08        |
|    value_loss           | 0.0239      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 457         |
|    ep_rew_mean          | 7.09e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1610        |
|    time_elapsed         | 9511        |
|    total_timesteps      | 52756480    |
| train/                  |             |
|    approx_kl            | 0.019745065 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.7       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 16090       |
|    policy_gradient_loss | -0.0201     |
|    std                  | 6.19        |
|    value_loss           | 0.0241      |
-----------------------------------------
Eval num_timesteps=53000000, episode_reward=6808.46 +/- 5151.50
Episode length: 430.80 +/- 272.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 431         |
|    mean_reward          | 6.81e+03    |
| time/                   |             |
|    total_timesteps      | 53000000    |
| train/                  |             |
|    approx_kl            | 0.017985111 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.9       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.272      |
|    n_updates            | 16170       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 6.31        |
|    value_loss           | 0.0184      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 503         |
|    ep_rew_mean          | 7.95e+03    |
| time/                   |             |
|    fps                  | 5545        |
|    iterations           | 1620        |
|    time_elapsed         | 9571        |
|    total_timesteps      | 53084160    |
| train/                  |             |
|    approx_kl            | 0.018089976 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -48.9       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.0179     |
|    std                  | 6.34        |
|    value_loss           | 0.0147      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 490         |
|    ep_rew_mean          | 7.76e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1630        |
|    time_elapsed         | 9630        |
|    total_timesteps      | 53411840    |
| train/                  |             |
|    approx_kl            | 0.018510932 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.1       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.273      |
|    n_updates            | 16290       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 6.43        |
|    value_loss           | 0.0208      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 524         |
|    ep_rew_mean          | 8.32e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1640        |
|    time_elapsed         | 9688        |
|    total_timesteps      | 53739520    |
| train/                  |             |
|    approx_kl            | 0.018363982 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.3       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.275      |
|    n_updates            | 16390       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.53        |
|    value_loss           | 0.0227      |
-----------------------------------------
Eval num_timesteps=54000000, episode_reward=7850.51 +/- 4219.01
Episode length: 463.80 +/- 221.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 464         |
|    mean_reward          | 7.85e+03    |
| time/                   |             |
|    total_timesteps      | 54000000    |
| train/                  |             |
|    approx_kl            | 0.018835975 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.276      |
|    n_updates            | 16470       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 6.59        |
|    value_loss           | 0.0209      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 520         |
|    ep_rew_mean          | 8.3e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1650        |
|    time_elapsed         | 9748        |
|    total_timesteps      | 54067200    |
| train/                  |             |
|    approx_kl            | 0.018559992 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.276      |
|    n_updates            | 16490       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 6.6         |
|    value_loss           | 0.019       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 488         |
|    ep_rew_mean          | 7.71e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1660        |
|    time_elapsed         | 9807        |
|    total_timesteps      | 54394880    |
| train/                  |             |
|    approx_kl            | 0.018512286 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -49.8       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.278      |
|    n_updates            | 16590       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 6.73        |
|    value_loss           | 0.0208      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 498         |
|    ep_rew_mean          | 7.96e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1670        |
|    time_elapsed         | 9865        |
|    total_timesteps      | 54722560    |
| train/                  |             |
|    approx_kl            | 0.017170735 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50         |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.278      |
|    n_updates            | 16690       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 6.85        |
|    value_loss           | 0.0217      |
-----------------------------------------
Eval num_timesteps=55000000, episode_reward=8600.25 +/- 4148.34
Episode length: 516.20 +/- 216.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | 8.6e+03     |
| time/                   |             |
|    total_timesteps      | 55000000    |
| train/                  |             |
|    approx_kl            | 0.018585928 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.2       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.279      |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 6.95        |
|    value_loss           | 0.026       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 488         |
|    ep_rew_mean          | 7.71e+03    |
| time/                   |             |
|    fps                  | 5545        |
|    iterations           | 1680        |
|    time_elapsed         | 9926        |
|    total_timesteps      | 55050240    |
| train/                  |             |
|    approx_kl            | 0.017262947 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.2       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.281      |
|    n_updates            | 16790       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 6.96        |
|    value_loss           | 0.0201      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 489        |
|    ep_rew_mean          | 7.78e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1690       |
|    time_elapsed         | 9984       |
|    total_timesteps      | 55377920   |
| train/                  |            |
|    approx_kl            | 0.01807657 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.4      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.279     |
|    n_updates            | 16890      |
|    policy_gradient_loss | -0.0183    |
|    std                  | 7.07       |
|    value_loss           | 0.0214     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 526        |
|    ep_rew_mean          | 8.32e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1700       |
|    time_elapsed         | 10042      |
|    total_timesteps      | 55705600   |
| train/                  |            |
|    approx_kl            | 0.01830658 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.6      |
|    explained_variance   | 0.957      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.281     |
|    n_updates            | 16990      |
|    policy_gradient_loss | -0.0191    |
|    std                  | 7.18       |
|    value_loss           | 0.0225     |
----------------------------------------
Eval num_timesteps=56000000, episode_reward=9845.08 +/- 2261.12
Episode length: 554.40 +/- 116.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 554        |
|    mean_reward          | 9.85e+03   |
| time/                   |            |
|    total_timesteps      | 56000000   |
| train/                  |            |
|    approx_kl            | 0.01826049 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -50.8      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.281     |
|    n_updates            | 17080      |
|    policy_gradient_loss | -0.018     |
|    std                  | 7.33       |
|    value_loss           | 0.0196     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 487         |
|    ep_rew_mean          | 7.77e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1710        |
|    time_elapsed         | 10103       |
|    total_timesteps      | 56033280    |
| train/                  |             |
|    approx_kl            | 0.018103223 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -50.9       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.283      |
|    n_updates            | 17090       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 7.35        |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | 7.61e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1720        |
|    time_elapsed         | 10161       |
|    total_timesteps      | 56360960    |
| train/                  |             |
|    approx_kl            | 0.018837545 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.1       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.283      |
|    n_updates            | 17190       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 7.45        |
|    value_loss           | 0.0199      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 7.81e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1730        |
|    time_elapsed         | 10219       |
|    total_timesteps      | 56688640    |
| train/                  |             |
|    approx_kl            | 0.018839635 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.3       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.285      |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 7.57        |
|    value_loss           | 0.0231      |
-----------------------------------------
Eval num_timesteps=57000000, episode_reward=8905.90 +/- 4299.81
Episode length: 532.60 +/- 236.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 533        |
|    mean_reward          | 8.91e+03   |
| time/                   |            |
|    total_timesteps      | 57000000   |
| train/                  |            |
|    approx_kl            | 0.01772596 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -51.6      |
|    explained_variance   | 0.95       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.284     |
|    n_updates            | 17390      |
|    policy_gradient_loss | -0.0193    |
|    std                  | 7.7        |
|    value_loss           | 0.0282     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | 7.56e+03 |
| time/              |          |
|    fps             | 5546     |
|    iterations      | 1740     |
|    time_elapsed    | 10280    |
|    total_timesteps | 57016320 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 487         |
|    ep_rew_mean          | 7.85e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1750        |
|    time_elapsed         | 10338       |
|    total_timesteps      | 57344000    |
| train/                  |             |
|    approx_kl            | 0.017055254 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -51.8       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.286      |
|    n_updates            | 17490       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 7.85        |
|    value_loss           | 0.0222      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 7.89e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1760        |
|    time_elapsed         | 10396       |
|    total_timesteps      | 57671680    |
| train/                  |             |
|    approx_kl            | 0.018528767 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52         |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.29       |
|    n_updates            | 17590       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 7.97        |
|    value_loss           | 0.02        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 503         |
|    ep_rew_mean          | 8.06e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1770        |
|    time_elapsed         | 10455       |
|    total_timesteps      | 57999360    |
| train/                  |             |
|    approx_kl            | 0.018034521 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.288      |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 8.14        |
|    value_loss           | 0.0235      |
-----------------------------------------
Eval num_timesteps=58000000, episode_reward=10008.46 +/- 2087.28
Episode length: 582.20 +/- 131.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 582         |
|    mean_reward          | 1e+04       |
| time/                   |             |
|    total_timesteps      | 58000000    |
| train/                  |             |
|    approx_kl            | 0.018469673 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.3       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.292      |
|    n_updates            | 17700       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 8.15        |
|    value_loss           | 0.0209      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 463         |
|    ep_rew_mean          | 7.41e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1780        |
|    time_elapsed         | 10515       |
|    total_timesteps      | 58327040    |
| train/                  |             |
|    approx_kl            | 0.017900797 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.5       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.29       |
|    n_updates            | 17790       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 8.29        |
|    value_loss           | 0.0208      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 552        |
|    ep_rew_mean          | 8.98e+03   |
| time/                   |            |
|    fps                  | 5547       |
|    iterations           | 1790       |
|    time_elapsed         | 10573      |
|    total_timesteps      | 58654720   |
| train/                  |            |
|    approx_kl            | 0.01873856 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -52.7      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.293     |
|    n_updates            | 17890      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 8.4        |
|    value_loss           | 0.0158     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 523         |
|    ep_rew_mean          | 8.31e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1800        |
|    time_elapsed         | 10632       |
|    total_timesteps      | 58982400    |
| train/                  |             |
|    approx_kl            | 0.018182695 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.9       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 17990       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 8.56        |
|    value_loss           | 0.02        |
-----------------------------------------
Eval num_timesteps=59000000, episode_reward=10881.28 +/- 116.67
Episode length: 656.80 +/- 19.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 657         |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 59000000    |
| train/                  |             |
|    approx_kl            | 0.018191226 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -52.9       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.295      |
|    n_updates            | 18000       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 8.57        |
|    value_loss           | 0.0199      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 538         |
|    ep_rew_mean          | 8.65e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1810        |
|    time_elapsed         | 10693       |
|    total_timesteps      | 59310080    |
| train/                  |             |
|    approx_kl            | 0.017595794 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.1       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.294      |
|    n_updates            | 18090       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 8.69        |
|    value_loss           | 0.0187      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 505         |
|    ep_rew_mean          | 8.1e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1820        |
|    time_elapsed         | 10751       |
|    total_timesteps      | 59637760    |
| train/                  |             |
|    approx_kl            | 0.017300759 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.3       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.293      |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 8.81        |
|    value_loss           | 0.026       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 482         |
|    ep_rew_mean          | 7.52e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1830        |
|    time_elapsed         | 10810       |
|    total_timesteps      | 59965440    |
| train/                  |             |
|    approx_kl            | 0.018613592 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.5       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.297      |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 8.95        |
|    value_loss           | 0.0194      |
-----------------------------------------
Eval num_timesteps=60000000, episode_reward=10714.09 +/- 668.71
Episode length: 639.00 +/- 40.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 639         |
|    mean_reward          | 1.07e+04    |
| time/                   |             |
|    total_timesteps      | 60000000    |
| train/                  |             |
|    approx_kl            | 0.018174984 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.6       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.296      |
|    n_updates            | 18310       |
|    policy_gradient_loss | -0.019      |
|    std                  | 8.97        |
|    value_loss           | 0.0235      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 485         |
|    ep_rew_mean          | 7.69e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1840        |
|    time_elapsed         | 10870       |
|    total_timesteps      | 60293120    |
| train/                  |             |
|    approx_kl            | 0.017749704 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.7       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18390       |
|    policy_gradient_loss | -0.019      |
|    std                  | 9.07        |
|    value_loss           | 0.0214      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 501         |
|    ep_rew_mean          | 7.94e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1850        |
|    time_elapsed         | 10929       |
|    total_timesteps      | 60620800    |
| train/                  |             |
|    approx_kl            | 0.016985333 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -53.9       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.297      |
|    n_updates            | 18490       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 9.21        |
|    value_loss           | 0.0192      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 519         |
|    ep_rew_mean          | 8.31e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1860        |
|    time_elapsed         | 10987       |
|    total_timesteps      | 60948480    |
| train/                  |             |
|    approx_kl            | 0.018492326 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.1       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.3        |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 9.34        |
|    value_loss           | 0.0188      |
-----------------------------------------
Eval num_timesteps=61000000, episode_reward=10897.14 +/- 156.32
Episode length: 633.20 +/- 23.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 633         |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 61000000    |
| train/                  |             |
|    approx_kl            | 0.017217405 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.1       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.299      |
|    n_updates            | 18610       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 9.37        |
|    value_loss           | 0.0197      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 454        |
|    ep_rew_mean          | 7.2e+03    |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1870       |
|    time_elapsed         | 11048      |
|    total_timesteps      | 61276160   |
| train/                  |            |
|    approx_kl            | 0.01751374 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -54.3      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.297     |
|    n_updates            | 18690      |
|    policy_gradient_loss | -0.0183    |
|    std                  | 9.53       |
|    value_loss           | 0.0224     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 465        |
|    ep_rew_mean          | 7.42e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1880       |
|    time_elapsed         | 11106      |
|    total_timesteps      | 61603840   |
| train/                  |            |
|    approx_kl            | 0.01740631 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -54.4      |
|    explained_variance   | 0.963      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.301     |
|    n_updates            | 18790      |
|    policy_gradient_loss | -0.0185    |
|    std                  | 9.65       |
|    value_loss           | 0.0215     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 498         |
|    ep_rew_mean          | 8e+03       |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1890        |
|    time_elapsed         | 11164       |
|    total_timesteps      | 61931520    |
| train/                  |             |
|    approx_kl            | 0.017926496 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.6       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.3        |
|    n_updates            | 18890       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 9.83        |
|    value_loss           | 0.0201      |
-----------------------------------------
Eval num_timesteps=62000000, episode_reward=9381.32 +/- 2079.25
Episode length: 559.60 +/- 111.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 560         |
|    mean_reward          | 9.38e+03    |
| time/                   |             |
|    total_timesteps      | 62000000    |
| train/                  |             |
|    approx_kl            | 0.018977389 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.7       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 18920       |
|    policy_gradient_loss | -0.0197     |
|    std                  | 9.89        |
|    value_loss           | 0.021       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 515         |
|    ep_rew_mean          | 8.16e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1900        |
|    time_elapsed         | 11225       |
|    total_timesteps      | 62259200    |
| train/                  |             |
|    approx_kl            | 0.017302992 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -54.8       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.305      |
|    n_updates            | 18990       |
|    policy_gradient_loss | -0.019      |
|    std                  | 10          |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 494         |
|    ep_rew_mean          | 7.88e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1910        |
|    time_elapsed         | 11283       |
|    total_timesteps      | 62586880    |
| train/                  |             |
|    approx_kl            | 0.017751604 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55         |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.303      |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 10.2        |
|    value_loss           | 0.0162      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 470         |
|    ep_rew_mean          | 7.51e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1920        |
|    time_elapsed         | 11341       |
|    total_timesteps      | 62914560    |
| train/                  |             |
|    approx_kl            | 0.018589554 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.2       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.304      |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 10.3        |
|    value_loss           | 0.0188      |
-----------------------------------------
Eval num_timesteps=63000000, episode_reward=10803.81 +/- 142.95
Episode length: 609.40 +/- 17.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 609         |
|    mean_reward          | 1.08e+04    |
| time/                   |             |
|    total_timesteps      | 63000000    |
| train/                  |             |
|    approx_kl            | 0.018643333 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.3       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19220       |
|    policy_gradient_loss | -0.0193     |
|    std                  | 10.4        |
|    value_loss           | 0.0188      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 487       |
|    ep_rew_mean          | 7.75e+03  |
| time/                   |           |
|    fps                  | 5546      |
|    iterations           | 1930      |
|    time_elapsed         | 11402     |
|    total_timesteps      | 63242240  |
| train/                  |           |
|    approx_kl            | 0.0175968 |
|    clip_fraction        | 0.137     |
|    clip_range           | 0.2       |
|    entropy_loss         | -55.4     |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.303    |
|    n_updates            | 19290     |
|    policy_gradient_loss | -0.0178   |
|    std                  | 10.5      |
|    value_loss           | 0.0224    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | 8.01e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1940        |
|    time_elapsed         | 11460       |
|    total_timesteps      | 63569920    |
| train/                  |             |
|    approx_kl            | 0.018660728 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.5       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19390       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 10.6        |
|    value_loss           | 0.0183      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 504         |
|    ep_rew_mean          | 8.12e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 1950        |
|    time_elapsed         | 11519       |
|    total_timesteps      | 63897600    |
| train/                  |             |
|    approx_kl            | 0.017069295 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.7       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19490       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 10.7        |
|    value_loss           | 0.0212      |
-----------------------------------------
Eval num_timesteps=64000000, episode_reward=11182.67 +/- 62.28
Episode length: 652.60 +/- 16.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 653         |
|    mean_reward          | 1.12e+04    |
| time/                   |             |
|    total_timesteps      | 64000000    |
| train/                  |             |
|    approx_kl            | 0.017952777 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -55.8       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19530       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 10.8        |
|    value_loss           | 0.0183      |
-----------------------------------------
New best mean reward!
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 526        |
|    ep_rew_mean          | 8.47e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 1960       |
|    time_elapsed         | 11579      |
|    total_timesteps      | 64225280   |
| train/                  |            |
|    approx_kl            | 0.01849908 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -55.8      |
|    explained_variance   | 0.973      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.307     |
|    n_updates            | 19590      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 10.9       |
|    value_loss           | 0.0149     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 505         |
|    ep_rew_mean          | 8.1e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1970        |
|    time_elapsed         | 11638       |
|    total_timesteps      | 64552960    |
| train/                  |             |
|    approx_kl            | 0.017370878 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56         |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.306      |
|    n_updates            | 19690       |
|    policy_gradient_loss | -0.0172     |
|    std                  | 11          |
|    value_loss           | 0.0188      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 7.86e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1980        |
|    time_elapsed         | 11696       |
|    total_timesteps      | 64880640    |
| train/                  |             |
|    approx_kl            | 0.016671933 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.2       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.307      |
|    n_updates            | 19790       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 11.3        |
|    value_loss           | 0.0255      |
-----------------------------------------
Eval num_timesteps=65000000, episode_reward=8986.77 +/- 4256.59
Episode length: 545.20 +/- 240.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 545         |
|    mean_reward          | 8.99e+03    |
| time/                   |             |
|    total_timesteps      | 65000000    |
| train/                  |             |
|    approx_kl            | 0.018049967 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.2       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.309      |
|    n_updates            | 19830       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 11.3        |
|    value_loss           | 0.0197      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 515         |
|    ep_rew_mean          | 8.42e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 1990        |
|    time_elapsed         | 11757       |
|    total_timesteps      | 65208320    |
| train/                  |             |
|    approx_kl            | 0.019499224 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.4       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.309      |
|    n_updates            | 19890       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 11.4        |
|    value_loss           | 0.0267      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 474         |
|    ep_rew_mean          | 7.58e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2000        |
|    time_elapsed         | 11815       |
|    total_timesteps      | 65536000    |
| train/                  |             |
|    approx_kl            | 0.018015848 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.6       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.311      |
|    n_updates            | 19990       |
|    policy_gradient_loss | -0.0185     |
|    std                  | 11.7        |
|    value_loss           | 0.0214      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 488         |
|    ep_rew_mean          | 7.73e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2010        |
|    time_elapsed         | 11874       |
|    total_timesteps      | 65863680    |
| train/                  |             |
|    approx_kl            | 0.018837456 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -56.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.313      |
|    n_updates            | 20090       |
|    policy_gradient_loss | -0.0192     |
|    std                  | 11.8        |
|    value_loss           | 0.0231      |
-----------------------------------------
Eval num_timesteps=66000000, episode_reward=11158.31 +/- 221.00
Episode length: 644.80 +/- 28.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 645         |
|    mean_reward          | 1.12e+04    |
| time/                   |             |
|    total_timesteps      | 66000000    |
| train/                  |             |
|    approx_kl            | 0.018540245 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57         |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.312      |
|    n_updates            | 20140       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 11.9        |
|    value_loss           | 0.0202      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 8.11e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2020       |
|    time_elapsed         | 11934      |
|    total_timesteps      | 66191360   |
| train/                  |            |
|    approx_kl            | 0.01692599 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57        |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.313     |
|    n_updates            | 20190      |
|    policy_gradient_loss | -0.0183    |
|    std                  | 12         |
|    value_loss           | 0.0212     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | 7.6e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2030        |
|    time_elapsed         | 11993       |
|    total_timesteps      | 66519040    |
| train/                  |             |
|    approx_kl            | 0.018012587 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.3       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.315      |
|    n_updates            | 20290       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 12.2        |
|    value_loss           | 0.019       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 537        |
|    ep_rew_mean          | 8.71e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2040       |
|    time_elapsed         | 12051      |
|    total_timesteps      | 66846720   |
| train/                  |            |
|    approx_kl            | 0.01812588 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -57.5      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.316     |
|    n_updates            | 20390      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 12.4       |
|    value_loss           | 0.0176     |
----------------------------------------
Eval num_timesteps=67000000, episode_reward=8894.94 +/- 4177.84
Episode length: 514.80 +/- 204.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | 8.89e+03    |
| time/                   |             |
|    total_timesteps      | 67000000    |
| train/                  |             |
|    approx_kl            | 0.018625226 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.6       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 20440       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 12.5        |
|    value_loss           | 0.0226      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | 8.53e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2050        |
|    time_elapsed         | 12112       |
|    total_timesteps      | 67174400    |
| train/                  |             |
|    approx_kl            | 0.016804762 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.7       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 20490       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 12.6        |
|    value_loss           | 0.0213      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 466         |
|    ep_rew_mean          | 7.56e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2060        |
|    time_elapsed         | 12170       |
|    total_timesteps      | 67502080    |
| train/                  |             |
|    approx_kl            | 0.017359879 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -57.9       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.316      |
|    n_updates            | 20590       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 12.8        |
|    value_loss           | 0.0234      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 506         |
|    ep_rew_mean          | 8.14e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2070        |
|    time_elapsed         | 12229       |
|    total_timesteps      | 67829760    |
| train/                  |             |
|    approx_kl            | 0.017737627 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.2       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20690       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 13.2        |
|    value_loss           | 0.0178      |
-----------------------------------------
Eval num_timesteps=68000000, episode_reward=10006.33 +/- 1840.67
Episode length: 585.20 +/- 106.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 585         |
|    mean_reward          | 1e+04       |
| time/                   |             |
|    total_timesteps      | 68000000    |
| train/                  |             |
|    approx_kl            | 0.017537892 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.3       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20750       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 13.3        |
|    value_loss           | 0.0188      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 523         |
|    ep_rew_mean          | 8.58e+03    |
| time/                   |             |
|    fps                  | 5545        |
|    iterations           | 2080        |
|    time_elapsed         | 12289       |
|    total_timesteps      | 68157440    |
| train/                  |             |
|    approx_kl            | 0.016929366 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.4       |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.319      |
|    n_updates            | 20790       |
|    policy_gradient_loss | -0.0174     |
|    std                  | 13.4        |
|    value_loss           | 0.0177      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 517        |
|    ep_rew_mean          | 8.24e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2090       |
|    time_elapsed         | 12347      |
|    total_timesteps      | 68485120   |
| train/                  |            |
|    approx_kl            | 0.01760288 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -58.6      |
|    explained_variance   | 0.961      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.318     |
|    n_updates            | 20890      |
|    policy_gradient_loss | -0.0172    |
|    std                  | 13.6       |
|    value_loss           | 0.0209     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 517         |
|    ep_rew_mean          | 8.41e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2100        |
|    time_elapsed         | 12406       |
|    total_timesteps      | 68812800    |
| train/                  |             |
|    approx_kl            | 0.017268624 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -58.7       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.32       |
|    n_updates            | 20990       |
|    policy_gradient_loss | -0.0179     |
|    std                  | 13.8        |
|    value_loss           | 0.0227      |
-----------------------------------------
Eval num_timesteps=69000000, episode_reward=6998.28 +/- 3883.70
Episode length: 426.00 +/- 204.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 426       |
|    mean_reward          | 7e+03     |
| time/                   |           |
|    total_timesteps      | 69000000  |
| train/                  |           |
|    approx_kl            | 0.0170035 |
|    clip_fraction        | 0.126     |
|    clip_range           | 0.2       |
|    entropy_loss         | -58.9     |
|    explained_variance   | 0.965     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.322    |
|    n_updates            | 21050     |
|    policy_gradient_loss | -0.0184   |
|    std                  | 14        |
|    value_loss           | 0.0167    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 504         |
|    ep_rew_mean          | 8.1e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2110        |
|    time_elapsed         | 12466       |
|    total_timesteps      | 69140480    |
| train/                  |             |
|    approx_kl            | 0.017392628 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59         |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.323      |
|    n_updates            | 21090       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 14.1        |
|    value_loss           | 0.0213      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 497        |
|    ep_rew_mean          | 8.08e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2120       |
|    time_elapsed         | 12524      |
|    total_timesteps      | 69468160   |
| train/                  |            |
|    approx_kl            | 0.01903981 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.2      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.324     |
|    n_updates            | 21190      |
|    policy_gradient_loss | -0.0194    |
|    std                  | 14.4       |
|    value_loss           | 0.0226     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 489         |
|    ep_rew_mean          | 7.96e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2130        |
|    time_elapsed         | 12582       |
|    total_timesteps      | 69795840    |
| train/                  |             |
|    approx_kl            | 0.017560486 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.4       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.325      |
|    n_updates            | 21290       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 14.6        |
|    value_loss           | 0.024       |
-----------------------------------------
Eval num_timesteps=70000000, episode_reward=10572.68 +/- 508.63
Episode length: 608.40 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 608       |
|    mean_reward          | 1.06e+04  |
| time/                   |           |
|    total_timesteps      | 70000000  |
| train/                  |           |
|    approx_kl            | 0.0167955 |
|    clip_fraction        | 0.135     |
|    clip_range           | 0.2       |
|    entropy_loss         | -59.5     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.323    |
|    n_updates            | 21360     |
|    policy_gradient_loss | -0.018    |
|    std                  | 14.7      |
|    value_loss           | 0.0244    |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 476         |
|    ep_rew_mean          | 7.66e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2140        |
|    time_elapsed         | 12643       |
|    total_timesteps      | 70123520    |
| train/                  |             |
|    approx_kl            | 0.017499581 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.5       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21390       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 14.8        |
|    value_loss           | 0.019       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | 8.23e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2150       |
|    time_elapsed         | 12702      |
|    total_timesteps      | 70451200   |
| train/                  |            |
|    approx_kl            | 0.01861206 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -59.7      |
|    explained_variance   | 0.951      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.324     |
|    n_updates            | 21490      |
|    policy_gradient_loss | -0.0181    |
|    std                  | 15         |
|    value_loss           | 0.0245     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 482         |
|    ep_rew_mean          | 7.83e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2160        |
|    time_elapsed         | 12760       |
|    total_timesteps      | 70778880    |
| train/                  |             |
|    approx_kl            | 0.018646903 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -59.9       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21590       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 15.2        |
|    value_loss           | 0.0193      |
-----------------------------------------
Eval num_timesteps=71000000, episode_reward=6670.80 +/- 5150.48
Episode length: 401.80 +/- 275.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 402         |
|    mean_reward          | 6.67e+03    |
| time/                   |             |
|    total_timesteps      | 71000000    |
| train/                  |             |
|    approx_kl            | 0.017346252 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60         |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.328      |
|    n_updates            | 21660       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 15.3        |
|    value_loss           | 0.022       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 454         |
|    ep_rew_mean          | 7.32e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2170        |
|    time_elapsed         | 12820       |
|    total_timesteps      | 71106560    |
| train/                  |             |
|    approx_kl            | 0.017578378 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60         |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21690       |
|    policy_gradient_loss | -0.0179     |
|    std                  | 15.4        |
|    value_loss           | 0.021       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 499         |
|    ep_rew_mean          | 8.14e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2180        |
|    time_elapsed         | 12878       |
|    total_timesteps      | 71434240    |
| train/                  |             |
|    approx_kl            | 0.017338008 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.3       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.327      |
|    n_updates            | 21790       |
|    policy_gradient_loss | -0.0178     |
|    std                  | 15.7        |
|    value_loss           | 0.0234      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 488         |
|    ep_rew_mean          | 7.91e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2190        |
|    time_elapsed         | 12936       |
|    total_timesteps      | 71761920    |
| train/                  |             |
|    approx_kl            | 0.017106649 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.4       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.332      |
|    n_updates            | 21890       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 16          |
|    value_loss           | 0.0156      |
-----------------------------------------
Eval num_timesteps=72000000, episode_reward=11094.20 +/- 150.02
Episode length: 659.60 +/- 33.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 660         |
|    mean_reward          | 1.11e+04    |
| time/                   |             |
|    total_timesteps      | 72000000    |
| train/                  |             |
|    approx_kl            | 0.018003957 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.5       |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.33       |
|    n_updates            | 21970       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 16.1        |
|    value_loss           | 0.0244      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 480        |
|    ep_rew_mean          | 7.72e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2200       |
|    time_elapsed         | 12997      |
|    total_timesteps      | 72089600   |
| train/                  |            |
|    approx_kl            | 0.01753062 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -60.6      |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.329     |
|    n_updates            | 21990      |
|    policy_gradient_loss | -0.0175    |
|    std                  | 16.2       |
|    value_loss           | 0.0235     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 463         |
|    ep_rew_mean          | 7.42e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2210        |
|    time_elapsed         | 13055       |
|    total_timesteps      | 72417280    |
| train/                  |             |
|    approx_kl            | 0.016159408 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -60.8       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.329      |
|    n_updates            | 22090       |
|    policy_gradient_loss | -0.0176     |
|    std                  | 16.5        |
|    value_loss           | 0.0239      |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 491       |
|    ep_rew_mean          | 7.76e+03  |
| time/                   |           |
|    fps                  | 5547      |
|    iterations           | 2220      |
|    time_elapsed         | 13113     |
|    total_timesteps      | 72744960  |
| train/                  |           |
|    approx_kl            | 0.0175456 |
|    clip_fraction        | 0.129     |
|    clip_range           | 0.2       |
|    entropy_loss         | -61       |
|    explained_variance   | 0.965     |
|    learning_rate        | 0.0003    |
|    loss                 | -0.334    |
|    n_updates            | 22190     |
|    policy_gradient_loss | -0.0183   |
|    std                  | 16.7      |
|    value_loss           | 0.0185    |
---------------------------------------
Eval num_timesteps=73000000, episode_reward=7924.76 +/- 4251.57
Episode length: 465.60 +/- 215.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 466         |
|    mean_reward          | 7.92e+03    |
| time/                   |             |
|    total_timesteps      | 73000000    |
| train/                  |             |
|    approx_kl            | 0.018123982 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.2       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22270       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 16.9        |
|    value_loss           | 0.0217      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 503         |
|    ep_rew_mean          | 8.12e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2230        |
|    time_elapsed         | 13174       |
|    total_timesteps      | 73072640    |
| train/                  |             |
|    approx_kl            | 0.018381227 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.2       |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.333      |
|    n_updates            | 22290       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 17          |
|    value_loss           | 0.0247      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 537         |
|    ep_rew_mean          | 8.79e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2240        |
|    time_elapsed         | 13232       |
|    total_timesteps      | 73400320    |
| train/                  |             |
|    approx_kl            | 0.017850425 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.3       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.337      |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 17.2        |
|    value_loss           | 0.0166      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 505         |
|    ep_rew_mean          | 8.22e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2250        |
|    time_elapsed         | 13290       |
|    total_timesteps      | 73728000    |
| train/                  |             |
|    approx_kl            | 0.017915014 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.5       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.335      |
|    n_updates            | 22490       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 17.5        |
|    value_loss           | 0.0217      |
-----------------------------------------
Eval num_timesteps=74000000, episode_reward=11070.24 +/- 169.56
Episode length: 641.40 +/- 33.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 641         |
|    mean_reward          | 1.11e+04    |
| time/                   |             |
|    total_timesteps      | 74000000    |
| train/                  |             |
|    approx_kl            | 0.016525302 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.6       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.335      |
|    n_updates            | 22580       |
|    policy_gradient_loss | -0.018      |
|    std                  | 17.6        |
|    value_loss           | 0.0267      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | 7.97e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2260        |
|    time_elapsed         | 13351       |
|    total_timesteps      | 74055680    |
| train/                  |             |
|    approx_kl            | 0.017296676 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -61.6       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.337      |
|    n_updates            | 22590       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 17.6        |
|    value_loss           | 0.0234      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | 7.9e+03    |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2270       |
|    time_elapsed         | 13410      |
|    total_timesteps      | 74383360   |
| train/                  |            |
|    approx_kl            | 0.01881849 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -61.9      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.338     |
|    n_updates            | 22690      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 18         |
|    value_loss           | 0.02       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 482         |
|    ep_rew_mean          | 7.72e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2280        |
|    time_elapsed         | 13468       |
|    total_timesteps      | 74711040    |
| train/                  |             |
|    approx_kl            | 0.018101215 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.1       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.338      |
|    n_updates            | 22790       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 18.4        |
|    value_loss           | 0.02        |
-----------------------------------------
Eval num_timesteps=75000000, episode_reward=11143.39 +/- 141.69
Episode length: 646.80 +/- 21.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 647        |
|    mean_reward          | 1.11e+04   |
| time/                   |            |
|    total_timesteps      | 75000000   |
| train/                  |            |
|    approx_kl            | 0.01782269 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -62.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.338     |
|    n_updates            | 22880      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 18.7       |
|    value_loss           | 0.0208     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 472         |
|    ep_rew_mean          | 7.65e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2290        |
|    time_elapsed         | 13529       |
|    total_timesteps      | 75038720    |
| train/                  |             |
|    approx_kl            | 0.016898582 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.3       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.339      |
|    n_updates            | 22890       |
|    policy_gradient_loss | -0.0187     |
|    std                  | 18.8        |
|    value_loss           | 0.0254      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 476        |
|    ep_rew_mean          | 7.72e+03   |
| time/                   |            |
|    fps                  | 5546       |
|    iterations           | 2300       |
|    time_elapsed         | 13587      |
|    total_timesteps      | 75366400   |
| train/                  |            |
|    approx_kl            | 0.01814966 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -62.5      |
|    explained_variance   | 0.962      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.34      |
|    n_updates            | 22990      |
|    policy_gradient_loss | -0.0187    |
|    std                  | 19.1       |
|    value_loss           | 0.0218     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 512         |
|    ep_rew_mean          | 8.34e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2310        |
|    time_elapsed         | 13645       |
|    total_timesteps      | 75694080    |
| train/                  |             |
|    approx_kl            | 0.018101363 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -62.8       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.34       |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0178     |
|    std                  | 19.4        |
|    value_loss           | 0.0216      |
-----------------------------------------
Eval num_timesteps=76000000, episode_reward=7689.27 +/- 2960.49
Episode length: 475.80 +/- 150.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 476         |
|    mean_reward          | 7.69e+03    |
| time/                   |             |
|    total_timesteps      | 76000000    |
| train/                  |             |
|    approx_kl            | 0.017417287 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63         |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.341      |
|    n_updates            | 23190       |
|    policy_gradient_loss | -0.0182     |
|    std                  | 19.8        |
|    value_loss           | 0.025       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | 8.28e+03 |
| time/              |          |
|    fps             | 5546     |
|    iterations      | 2320     |
|    time_elapsed    | 13705    |
|    total_timesteps | 76021760 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 525         |
|    ep_rew_mean          | 8.6e+03     |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2330        |
|    time_elapsed         | 13764       |
|    total_timesteps      | 76349440    |
| train/                  |             |
|    approx_kl            | 0.017959878 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.1       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.345      |
|    n_updates            | 23290       |
|    policy_gradient_loss | -0.0188     |
|    std                  | 20.1        |
|    value_loss           | 0.0234      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 447         |
|    ep_rew_mean          | 7.15e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2340        |
|    time_elapsed         | 13822       |
|    total_timesteps      | 76677120    |
| train/                  |             |
|    approx_kl            | 0.017282583 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.3       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.342      |
|    n_updates            | 23390       |
|    policy_gradient_loss | -0.018      |
|    std                  | 20.6        |
|    value_loss           | 0.0237      |
-----------------------------------------
Eval num_timesteps=77000000, episode_reward=10886.38 +/- 118.59
Episode length: 629.20 +/- 13.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 629         |
|    mean_reward          | 1.09e+04    |
| time/                   |             |
|    total_timesteps      | 77000000    |
| train/                  |             |
|    approx_kl            | 0.017369002 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.5       |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.344      |
|    n_updates            | 23490       |
|    policy_gradient_loss | -0.0175     |
|    std                  | 20.9        |
|    value_loss           | 0.0224      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 529      |
|    ep_rew_mean     | 8.5e+03  |
| time/              |          |
|    fps             | 5546     |
|    iterations      | 2350     |
|    time_elapsed    | 13883    |
|    total_timesteps | 77004800 |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 8.06e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2360        |
|    time_elapsed         | 13941       |
|    total_timesteps      | 77332480    |
| train/                  |             |
|    approx_kl            | 0.017825149 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.7       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.348      |
|    n_updates            | 23590       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 21.3        |
|    value_loss           | 0.0158      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | 8.04e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2370        |
|    time_elapsed         | 13999       |
|    total_timesteps      | 77660160    |
| train/                  |             |
|    approx_kl            | 0.016969131 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -63.9       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.347      |
|    n_updates            | 23690       |
|    policy_gradient_loss | -0.0181     |
|    std                  | 21.8        |
|    value_loss           | 0.0147      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 508        |
|    ep_rew_mean          | 8.15e+03   |
| time/                   |            |
|    fps                  | 5547       |
|    iterations           | 2380       |
|    time_elapsed         | 14058      |
|    total_timesteps      | 77987840   |
| train/                  |            |
|    approx_kl            | 0.01618514 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -64.1      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.35      |
|    n_updates            | 23790      |
|    policy_gradient_loss | -0.018     |
|    std                  | 22.2       |
|    value_loss           | 0.0162     |
----------------------------------------
Eval num_timesteps=78000000, episode_reward=10033.48 +/- 1703.66
Episode length: 589.80 +/- 108.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 590         |
|    mean_reward          | 1e+04       |
| time/                   |             |
|    total_timesteps      | 78000000    |
| train/                  |             |
|    approx_kl            | 0.017535985 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.1       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.349      |
|    n_updates            | 23800       |
|    policy_gradient_loss | -0.019      |
|    std                  | 22.2        |
|    value_loss           | 0.0222      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 477         |
|    ep_rew_mean          | 7.69e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2390        |
|    time_elapsed         | 14118       |
|    total_timesteps      | 78315520    |
| train/                  |             |
|    approx_kl            | 0.017294627 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.3       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.35       |
|    n_updates            | 23890       |
|    policy_gradient_loss | -0.0186     |
|    std                  | 22.6        |
|    value_loss           | 0.018       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 509         |
|    ep_rew_mean          | 8.29e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2400        |
|    time_elapsed         | 14176       |
|    total_timesteps      | 78643200    |
| train/                  |             |
|    approx_kl            | 0.017079778 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.5       |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.35       |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.0183     |
|    std                  | 23.2        |
|    value_loss           | 0.0219      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 531        |
|    ep_rew_mean          | 8.7e+03    |
| time/                   |            |
|    fps                  | 5547       |
|    iterations           | 2410       |
|    time_elapsed         | 14235      |
|    total_timesteps      | 78970880   |
| train/                  |            |
|    approx_kl            | 0.01644434 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -64.7      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.35      |
|    n_updates            | 24090      |
|    policy_gradient_loss | -0.0178    |
|    std                  | 23.6       |
|    value_loss           | 0.019      |
----------------------------------------
Eval num_timesteps=79000000, episode_reward=11015.41 +/- 137.76
Episode length: 632.60 +/- 32.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 633         |
|    mean_reward          | 1.1e+04     |
| time/                   |             |
|    total_timesteps      | 79000000    |
| train/                  |             |
|    approx_kl            | 0.016704585 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.7       |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.351      |
|    n_updates            | 24100       |
|    policy_gradient_loss | -0.0174     |
|    std                  | 23.6        |
|    value_loss           | 0.0173      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 482         |
|    ep_rew_mean          | 7.74e+03    |
| time/                   |             |
|    fps                  | 5546        |
|    iterations           | 2420        |
|    time_elapsed         | 14296       |
|    total_timesteps      | 79298560    |
| train/                  |             |
|    approx_kl            | 0.016751051 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -64.9       |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.351      |
|    n_updates            | 24190       |
|    policy_gradient_loss | -0.0177     |
|    std                  | 24.1        |
|    value_loss           | 0.0191      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 501         |
|    ep_rew_mean          | 8.25e+03    |
| time/                   |             |
|    fps                  | 5547        |
|    iterations           | 2430        |
|    time_elapsed         | 14354       |
|    total_timesteps      | 79626240    |
| train/                  |             |
|    approx_kl            | 0.017275125 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -65.1       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.351      |
|    n_updates            | 24290       |
|    policy_gradient_loss | -0.0177     |
|    std                  | 24.5        |
|    value_loss           | 0.0206      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 503        |
|    ep_rew_mean          | 8.14e+03   |
| time/                   |            |
|    fps                  | 5547       |
|    iterations           | 2440       |
|    time_elapsed         | 14412      |
|    total_timesteps      | 79953920   |
| train/                  |            |
|    approx_kl            | 0.01614509 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -65.2      |
|    explained_variance   | 0.965      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.355     |
|    n_updates            | 24390      |
|    policy_gradient_loss | -0.0181    |
|    std                  | 24.9       |
|    value_loss           | 0.0184     |
----------------------------------------
Eval num_timesteps=80000000, episode_reward=10880.93 +/- 104.82
Episode length: 627.00 +/- 18.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 627        |
|    mean_reward          | 1.09e+04   |
| time/                   |            |
|    total_timesteps      | 80000000   |
| train/                  |            |
|    approx_kl            | 0.01574908 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -65.3      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.352     |
|    n_updates            | 24410      |
|    policy_gradient_loss | -0.0176    |
|    std                  | 25         |
|    value_loss           | 0.021      |
----------------------------------------

============================================================
Training Complete!
============================================================
Total training time: 4:00:27
Total timesteps: 80,000,000
Timesteps per second: 5545.12
Run directory: /home/fspinto/projects/rl-humanoid/outputs/2025-12-24/16-29-37
============================================================

